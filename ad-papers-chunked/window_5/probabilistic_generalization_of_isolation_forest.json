[{"id_": "d3e67660-436d-4272-9e89-b17d4cc73009", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e. ", "original_text": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dcb3e00-60cf-4df0-880f-a28a420b662a", "node_type": "1", "metadata": {"window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them. ", "original_text": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis. "}, "hash": "0f9b295554b179ae6eabf8cf75ee5eadfd31a705e2acef038a960bfd82968fab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7dcb3e00-60cf-4df0-880f-a28a420b662a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them. ", "original_text": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3e67660-436d-4272-9e89-b17d4cc73009", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e. ", "original_text": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul. "}, "hash": "5e7a0a9e37af33981adb951a12df7219052a394b92d4c349d6c87df1bba7c9a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ef7220a-8e08-42fb-9b3b-3c48600ad879", "node_type": "1", "metadata": {"window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. ", "original_text": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method. "}, "hash": "0f091f406a571a6d470a349b4b07bcb10ffeef38f5dd6f25da1fee4f5e1a5bc8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis. ", "mimetype": "text/plain", "start_char_idx": 341, "end_char_idx": 824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ef7220a-8e08-42fb-9b3b-3c48600ad879", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. ", "original_text": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dcb3e00-60cf-4df0-880f-a28a420b662a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them. ", "original_text": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis. "}, "hash": "fc0e15f27f466eaed53cc1c57c50d8c792a3e7467c5b724ec90f794f612a8de5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "370f3937-7fff-451b-a466-fa35303fc46d", "node_type": "1", "metadata": {"window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. ", "original_text": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach. "}, "hash": "3ea328aa92da5480df60dede439be00c8fc1f3cb90b2012c2e70982cedfbafde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method. ", "mimetype": "text/plain", "start_char_idx": 824, "end_char_idx": 973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "370f3937-7fff-451b-a466-fa35303fc46d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. ", "original_text": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ef7220a-8e08-42fb-9b3b-3c48600ad879", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. ", "original_text": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method. "}, "hash": "135b9f1de44820ae07373793eee007c9c0df718b8030dc1d5b52574200ddf2c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8956d5ca-b7e9-4a5a-9a5b-13df4736163e", "node_type": "1", "metadata": {"window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n", "original_text": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment. "}, "hash": "db141db97c5f7f1821b504c52dff55b2a32d6caa2329b8b55faed77b80fa34f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach. ", "mimetype": "text/plain", "start_char_idx": 973, "end_char_idx": 1144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8956d5ca-b7e9-4a5a-9a5b-13df4736163e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n", "original_text": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "370f3937-7fff-451b-a466-fa35303fc46d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. ", "original_text": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach. "}, "hash": "c14fca8382b47c2268cee7ecf9fc8c3cc63770076769dfa6b25fbf6a63bb5bf0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19a29394-0b9a-4e8a-b257-ca571558a08a", "node_type": "1", "metadata": {"window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s). ", "original_text": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e. "}, "hash": "a29c0d3a74ea1662291fbd71c92233d6b5e5463104e2ef996a2b1c89b9d64652", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment. ", "mimetype": "text/plain", "start_char_idx": 1144, "end_char_idx": 1266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19a29394-0b9a-4e8a-b257-ca571558a08a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s). ", "original_text": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8956d5ca-b7e9-4a5a-9a5b-13df4736163e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n", "original_text": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment. "}, "hash": "e4f08ebb35aae865c597eaf1526479c87da6a6e6eee1e3d2e13957b8ee077359", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc972b08-e31c-4a7d-835d-7b64e5818b8a", "node_type": "1", "metadata": {"window": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc. ", "original_text": "regions where datapoints constitute dense formations and not through them. "}, "hash": "87209de4b933705ad554a75d72af26c01e8561a96e906ee2af08972146c4c892", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e. ", "mimetype": "text/plain", "start_char_idx": 1266, "end_char_idx": 1394, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc972b08-e31c-4a7d-835d-7b64e5818b8a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc. ", "original_text": "regions where datapoints constitute dense formations and not through them. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19a29394-0b9a-4e8a-b257-ca571558a08a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Information Sciences 584 (2022) 433\u2013449**\n\n**Contents lists available at ScienceDirect**\n\n**Information Sciences**\n\n**journal homepage: www.elsevier.com/locate/ins**\n\n---\n\n# A probabilistic generalization of isolation forest\n\n**Mikhail Tokovarov *, Pawe\u0142 Karczmarek**\n\n*Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s). ", "original_text": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e. "}, "hash": "9cf74844adbc95f156bd1edd65276baa824426bd9af18de20768e27666e369f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbaffae7-418f-44c0-854a-dd7bba7d26be", "node_type": "1", "metadata": {"window": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. "}, "hash": "6a12182609a78a6468685273b0190f275ea64de5c615bb3487dee4c801661af8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "regions where datapoints constitute dense formations and not through them. ", "mimetype": "text/plain", "start_char_idx": 1394, "end_char_idx": 1469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbaffae7-418f-44c0-854a-dd7bba7d26be", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc972b08-e31c-4a7d-835d-7b64e5818b8a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Nadbystrzycka 36B, 20-618 Lublin, Poland*\n\n---\n\n## ARTICLE INFO\n\n**Article history:**\nReceived 7 May 2021\nReceived in revised form 27 October 2021\nAccepted 30 October 2021\nAvailable online 7 November 2021\n\n**Keywords:**\nAnomaly detection\nIsolation forest\nProbabilistic generalization of isolation forest\nOptimal division\nSpatio-temporal datasets\n\n## ABSTRACT\n\nThe problem of finding anomalies and outliers in datasets is one of the most important challenges of modern data analysis.  Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc. ", "original_text": "regions where datapoints constitute dense formations and not through them. "}, "hash": "d831668e681a46bbd24bf7f0382cd5c4ce7cc6ff7f5ebe43ee3006793192b14e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e3db7e7-9a00-42b8-b064-0e3a793d4d14", "node_type": "1", "metadata": {"window": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1. ", "original_text": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. "}, "hash": "ec6767891ecaa3680d016d0da9718dae248349c1b8ed779965335b71b4c1b357", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. ", "mimetype": "text/plain", "start_char_idx": 1469, "end_char_idx": 1616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e3db7e7-9a00-42b8-b064-0e3a793d4d14", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1. ", "original_text": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbaffae7-418f-44c0-854a-dd7bba7d26be", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Among the commonly dedicated tools to solve this task one can find Isolation Forest (IF) that is an efficient, conceptually simple, and fast method.  In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively. "}, "hash": "dee2869594fccf9a49778454866e69f0cb6d291b47b8c727fb028ef93b701adf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceafeada-0671-430b-82fa-6fb385c2c162", "node_type": "1", "metadata": {"window": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. ", "original_text": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n"}, "hash": "fce55a82510c5757b7cea68611eb75627aeaf433697314c54987e52564439a3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. ", "mimetype": "text/plain", "start_char_idx": 1616, "end_char_idx": 1752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ceafeada-0671-430b-82fa-6fb385c2c162", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. ", "original_text": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e3db7e7-9a00-42b8-b064-0e3a793d4d14", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In this study, we propose the Probabilistic Generalization of Isolation Forest (PGIF) that is an intuitively appealing and efficient enhancement of the original approach.  The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1. ", "original_text": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets. "}, "hash": "69ad95af5f1d044568cc292c1765edfc1f73e65c583d80f1a9a570ee4f2a138c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc703926-04b0-4ba2-9d28-9d76e6e733dd", "node_type": "1", "metadata": {"window": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. ", "original_text": "\u00a9 2021 The Author(s). "}, "hash": "36445f51ae013bd85b952c6474794d098cf9466a9cd2655873d85e22fba2d19f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n", "mimetype": "text/plain", "start_char_idx": 1752, "end_char_idx": 1965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc703926-04b0-4ba2-9d28-9d76e6e733dd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. ", "original_text": "\u00a9 2021 The Author(s). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceafeada-0671-430b-82fa-6fb385c2c162", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization is based on nonlinear dependence of segment-cumulated probability from the length of segment.  Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. ", "original_text": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n"}, "hash": "4dbbf1206f5b51feb249289eb0eee62a8f27b624658ac02972c68d023e5eb13e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "385f74a2-e63b-47ef-9520-ad3f24259365", "node_type": "1", "metadata": {"window": "regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. ", "original_text": "Published by Elsevier Inc. "}, "hash": "1b23636dff7b3c8ec2cf645876f98b0bfc1eaee403df85a9497fdda288cbf021", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u00a9 2021 The Author(s). ", "mimetype": "text/plain", "start_char_idx": 1965, "end_char_idx": 1987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "385f74a2-e63b-47ef-9520-ad3f24259365", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. ", "original_text": "Published by Elsevier Inc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc703926-04b0-4ba2-9d28-9d76e6e733dd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Introduction of the generalization allows to achieve more effective splits that are rather performed between the clusters, i.e.  regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. ", "original_text": "\u00a9 2021 The Author(s). "}, "hash": "13d48720efb2eae58e75b2586d8d6450b321c8c535a9fb90afcf0a53790268eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9abfea2-e79d-4325-b8e7-541022605424", "node_type": "1", "metadata": {"window": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "9c8550240d9372e7a257fb8af53c7980a8adbba58dbb0e50fd8f91b27e6c6f1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published by Elsevier Inc. ", "mimetype": "text/plain", "start_char_idx": 1987, "end_char_idx": 2014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9abfea2-e79d-4325-b8e7-541022605424", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "385f74a2-e63b-47ef-9520-ad3f24259365", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "regions where datapoints constitute dense formations and not through them.  In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. ", "original_text": "Published by Elsevier Inc. "}, "hash": "37f2af49524b0935250ef7bf7ecffba1471cecc65e7dbddf406ae00b84bea6c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba5cf186-8e5c-4db1-834e-eb1339841396", "node_type": "1", "metadata": {"window": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data. ", "original_text": "---\n\n## 1. "}, "hash": "93e4a643c4b8dd2efdb431def02c164bb3e15867914b26efca961c8442adf838", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "mimetype": "text/plain", "start_char_idx": 2014, "end_char_idx": 2129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba5cf186-8e5c-4db1-834e-eb1339841396", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data. ", "original_text": "---\n\n## 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9abfea2-e79d-4325-b8e7-541022605424", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In a comprehensive series of experiments, we show that the proposed method allows us to detect anomalies hidden between clusters more effectively.  Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "20baa3f649d62e011a0c01b7cd9bfd6252624729fedba37a29f1f86020ac5a81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cb4d257-bb85-4311-84de-cec1a931a25e", "node_type": "1", "metadata": {"window": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. ", "original_text": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. "}, "hash": "7a6fa95b64118398ae06cf45c4a52d643ba5d6b12ec737fa36035a1ca83d992b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n## 1. ", "mimetype": "text/plain", "start_char_idx": 2129, "end_char_idx": 2140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3cb4d257-bb85-4311-84de-cec1a931a25e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. ", "original_text": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba5cf186-8e5c-4db1-834e-eb1339841396", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, it is demonstrated that our approach favorably affects the quality of anomaly detection in both artificial and real datasets.  In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data. ", "original_text": "---\n\n## 1. "}, "hash": "197f9dc545f63eea28223cb49a5627141b29df88ddc5466dc63fc4d1b6d3c92e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a17c7daf-77d5-4bfc-8545-95e6f2696c64", "node_type": "1", "metadata": {"window": "\u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n", "original_text": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. "}, "hash": "01481085230692ddab50e138a3ef6d9a9ad33c316e15f823016d803cd912d3c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. ", "mimetype": "text/plain", "start_char_idx": 2140, "end_char_idx": 2346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a17c7daf-77d5-4bfc-8545-95e6f2696c64", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n", "original_text": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cb4d257-bb85-4311-84de-cec1a931a25e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In terms of time complexity our method is close to the original one since the generalization is related only to the building of the trees while the scoring procedure (which takes the main time) is kept unchanged.\n \u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. ", "original_text": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades. "}, "hash": "79294f11c67cbca0e8c523bc6446f5fc0306ee5760b80cdfc4e73b346192f3cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b80dc06-5df0-40aa-aaac-35891d354816", "node_type": "1", "metadata": {"window": "Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. ", "original_text": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. "}, "hash": "be394facf6940b078d51ca163d3b34ea12cc27750a3ed7e6860434f3a278eef9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. ", "mimetype": "text/plain", "start_char_idx": 2346, "end_char_idx": 2654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b80dc06-5df0-40aa-aaac-35891d354816", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. ", "original_text": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a17c7daf-77d5-4bfc-8545-95e6f2696c64", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u00a9 2021 The Author(s).  Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n", "original_text": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems. "}, "hash": "542b2cc827523b6a15be41fbfc30932efc514443634b0961240d0a79537da54c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "333c7339-9066-44f9-abb0-5ade23cdf4bf", "node_type": "1", "metadata": {"window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here. ", "original_text": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. "}, "hash": "2352b5578ae9945c5d763b6ede24298886044a2baf1e66d58529df931f3debe6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. ", "mimetype": "text/plain", "start_char_idx": 2654, "end_char_idx": 2874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "333c7339-9066-44f9-abb0-5ade23cdf4bf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here. ", "original_text": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b80dc06-5df0-40aa-aaac-35891d354816", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Published by Elsevier Inc.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. ", "original_text": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration. "}, "hash": "02f3afc4ee681bfa2825ca770ae9ff7d95acd78f3eee5fe8bc99e89e832f5bb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20063182-8148-4869-82e8-8bdb4eba8923", "node_type": "1", "metadata": {"window": "---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. ", "original_text": "which are almost 100% similar to the actual data. "}, "hash": "ad9608c115fd127e4356d098e62c065f50937507595c7b529262b95540857bf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. ", "mimetype": "text/plain", "start_char_idx": 2874, "end_char_idx": 2976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20063182-8148-4869-82e8-8bdb4eba8923", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. ", "original_text": "which are almost 100% similar to the actual data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "333c7339-9066-44f9-abb0-5ade23cdf4bf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here. ", "original_text": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc. "}, "hash": "c1d129989021fdd04080fb86c6dac5dc708ea99c21c36c321a53dd3b71cd9ce1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00c43128-b2f5-4ebb-9105-456b6f8633f0", "node_type": "1", "metadata": {"window": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n", "original_text": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. "}, "hash": "b408fa80061523944a088a756e237e98dc044fc6f921a626556139f6c5d0b596", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "which are almost 100% similar to the actual data. ", "mimetype": "text/plain", "start_char_idx": 2976, "end_char_idx": 3026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "00c43128-b2f5-4ebb-9105-456b6f8633f0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n", "original_text": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20063182-8148-4869-82e8-8bdb4eba8923", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n## 1.  Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. ", "original_text": "which are almost 100% similar to the actual data. "}, "hash": "48d3b7d47275d1e6e37644eb56f5a232a6f597c137ee0bc61e3f762a4c1ab9f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c1dd8e8-d95d-4fad-9f3b-7a9a8552b09a", "node_type": "1", "metadata": {"window": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M. ", "original_text": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n"}, "hash": "b0a0c00967392f1e8e2b38e34a4ef1e8fc232316214b7cf54c10d639da643d31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. ", "mimetype": "text/plain", "start_char_idx": 3026, "end_char_idx": 3131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c1dd8e8-d95d-4fad-9f3b-7a9a8552b09a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M. ", "original_text": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00c43128-b2f5-4ebb-9105-456b6f8633f0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Introduction\n\nDetecting anomalies in data, often referred to as outliers, is a very important task that has been present in the scientific literature and everyday work of practitioners for several decades.  Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n", "original_text": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity. "}, "hash": "f95c88aed18d10b74e76a873a73b1131ec3f4ec80494dc799fcf2b061b582e2f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3878e37a-eae7-47a7-8bad-077fa3652772", "node_type": "1", "metadata": {"window": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov).", "original_text": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. "}, "hash": "d22abfc6f55a787f47c282be812c2c95e2663e0232e87c973863e6f179cb9c38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n", "mimetype": "text/plain", "start_char_idx": 3131, "end_char_idx": 3268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3878e37a-eae7-47a7-8bad-077fa3652772", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov).", "original_text": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c1dd8e8-d95d-4fad-9f3b-7a9a8552b09a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Particularly, in recent years, it is extremely hard task due to increasing volume of data as well as their frequent low quality related to measurement errors, human factor, errors in the operation of IT systems, but also attempts to break the security procedures of electronic and physical security systems.  Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M. ", "original_text": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n"}, "hash": "e5a9ab2060f2bea57339563463ff9d5d54752dac0efc08b27437aa69efd9a0a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f62d09b-fcac-4601-8232-8aa0c399435d", "node_type": "1", "metadata": {"window": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). ", "original_text": "We will recall some important results here. "}, "hash": "f3ccdf664b178764b3e6fa1822f7568795dc0c72677695401799986304ec0db9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. ", "mimetype": "text/plain", "start_char_idx": 3268, "end_char_idx": 3411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f62d09b-fcac-4601-8232-8aa0c399435d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). ", "original_text": "We will recall some important results here. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3878e37a-eae7-47a7-8bad-077fa3652772", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Some anomalies in the data such as their absence or unrealistic measurement values, for instance, negative water temperature, are not difficult to find using simple scripts and can be easily excluded from consideration.  However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov).", "original_text": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications. "}, "hash": "bd64c84c9f16f34e4a7bb284cbf4acc4071574bcc74be69ef57d87828996ad97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d91a05b-d81b-463a-9789-f575ec6c8b2f", "node_type": "1", "metadata": {"window": "which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n", "original_text": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. "}, "hash": "31a334ee150cc3a6ca61bc8aeb093928aadca77bca89fc4690aa494114e2a382", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will recall some important results here. ", "mimetype": "text/plain", "start_char_idx": 3411, "end_char_idx": 3455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d91a05b-d81b-463a-9789-f575ec6c8b2f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n", "original_text": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f62d09b-fcac-4601-8232-8aa0c399435d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, there are also frequent attempts of phishing identity, access to accounts or passwords, etc.  which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). ", "original_text": "We will recall some important results here. "}, "hash": "1224682c53f6369ec70b187ad6be7236ac3b5f6d7d2d751051dda6fbe2d59cf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46edb0ba-70b0-48c6-9bf7-8e29c11738eb", "node_type": "1", "metadata": {"window": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n"}, "hash": "4b2da49f8a044744b396db0c92c52c2aced9ba437f28c29c3c7a5f2a83203969", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. ", "mimetype": "text/plain", "start_char_idx": 3455, "end_char_idx": 3837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46edb0ba-70b0-48c6-9bf7-8e29c11738eb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d91a05b-d81b-463a-9789-f575ec6c8b2f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "which are almost 100% similar to the actual data.  An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n", "original_text": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15]. "}, "hash": "0732820fb1c69b86b016f641dd637932a16fdfc2e5a4d7cd651484139bdcd14b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b71fea2-c128-4c4d-a5c8-3d39d026c928", "node_type": "1", "metadata": {"window": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others. ", "original_text": "*E-mail address: p.karczmarek@pollub.pl (M. "}, "hash": "3b50b9e4ce3b4a89b0076a52b6a7c62e042181af7a2007c8628cfb7cfa535d23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n", "mimetype": "text/plain", "start_char_idx": 3837, "end_char_idx": 4031, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b71fea2-c128-4c4d-a5c8-3d39d026c928", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others. ", "original_text": "*E-mail address: p.karczmarek@pollub.pl (M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46edb0ba-70b0-48c6-9bf7-8e29c11738eb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "An additional hindering factor is the frequent multidimensionality of data and even their heterogeneity.  Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n"}, "hash": "13b9cbdd65c69395ae033bf1e78013bfc275ceeeafa7f1ebacbc2a4df5858220", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bf146b8-f852-4e76-9528-7af96b759675", "node_type": "1", "metadata": {"window": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. ", "original_text": "Tokovarov)."}, "hash": "d3ff8e4fc2d2b56a0fb68f451255f16ff9a738486d083c5021280282d3257c9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*E-mail address: p.karczmarek@pollub.pl (M. ", "mimetype": "text/plain", "start_char_idx": 4031, "end_char_idx": 4075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3bf146b8-f852-4e76-9528-7af96b759675", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. ", "original_text": "Tokovarov)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b71fea2-c128-4c4d-a5c8-3d39d026c928", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Data may take various forms such as typical datasets, images, videos, GPS positions, travel trajectories, time series, and many others.\n\n The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others. ", "original_text": "*E-mail address: p.karczmarek@pollub.pl (M. "}, "hash": "a4291f4059ec3534e82339098bd3fc13d39d8e35885f4ffbaf971c266ba3fba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77d66d2f-aafa-4883-a45f-f86d95aa405f", "node_type": "1", "metadata": {"window": "We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees. ", "original_text": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). "}, "hash": "bf167fa52c282afa9b1119b434aa3195327511eff58004c4670289467388b31d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tokovarov).", "mimetype": "text/plain", "start_char_idx": 4075, "end_char_idx": 4086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77d66d2f-aafa-4883-a45f-f86d95aa405f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees. ", "original_text": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf146b8-f852-4e76-9528-7af96b759675", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The above-mentioned factors make the literature on the subject rich not only due to the variety of methods but also due to their applications.  We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. ", "original_text": "Tokovarov)."}, "hash": "f3f2cd223a613bd7d2730dde56b6596ea2a5360da46eb0a2c8eb4d37108a0d23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c373eb7-1bdf-49f6-840f-74a82617313a", "node_type": "1", "metadata": {"window": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset. ", "original_text": "Published by Elsevier Inc.\n"}, "hash": "e808d2f73bd5cc4879c686b0dd433a37b7daa2d9836d0e22e73e5021c1f271f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). ", "mimetype": "text/plain", "start_char_idx": 4086, "end_char_idx": 4163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c373eb7-1bdf-49f6-840f-74a82617313a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset. ", "original_text": "Published by Elsevier Inc.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77d66d2f-aafa-4883-a45f-f86d95aa405f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We will recall some important results here.  There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees. ", "original_text": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s). "}, "hash": "ff82e867cb4a1b362df7898efc7d903d6c12c2db5eb4dec106083069ff9ce8a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ca652f5-c02b-43f3-8cf3-f8fd147d93ac", "node_type": "1", "metadata": {"window": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "2ccc9b1f4b7af8802cb53bbb9e7324637ece204bdcf77ed5351878199c98add3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published by Elsevier Inc.\n", "mimetype": "text/plain", "start_char_idx": 4163, "end_char_idx": 4190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ca652f5-c02b-43f3-8cf3-f8fd147d93ac", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c373eb7-1bdf-49f6-840f-74a82617313a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "There are many classes of algorithms used in practice such as support vector machines or core vector machine [1,2], deep neural networks [3\u20136] such as long-short term memory, self-organizing maps, autoencoders, clustering [7], various approaches to DBSCAN algorithm [8], and granular models or fuzzy set-based methods [9\u201313], in particular in an application to time series [14,15].  Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset. ", "original_text": "Published by Elsevier Inc.\n"}, "hash": "438da416aa5dcd4e3616d9d8237918afd45fd82d8fdc1b67c9567c1d0452f262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb4d4d6e-4c87-43c4-af50-fc12aea12154", "node_type": "1", "metadata": {"window": "*E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n", "original_text": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others. "}, "hash": "397a86aea12724e65aff11b23cc29844b52710129a5d53228aa35a5d5cdc305e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "mimetype": "text/plain", "start_char_idx": 4190, "end_char_idx": 4305, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb4d4d6e-4c87-43c4-af50-fc12aea12154", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "*E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n", "original_text": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ca652f5-c02b-43f3-8cf3-f8fd147d93ac", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Another methods, particularly used to time series analysis, are random weight networks [16], immune systems [17], weighted graph representation [18], Bayesian net-\n\n---\n\\* Corresponding author.\n *E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "063c6abaead72a9fdf28f7806d36ae89c4784e1ba183fd75b05e6f004322e7d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "151978b9-f815-4a9c-b5cf-bac8b99a00ed", "node_type": "1", "metadata": {"window": "Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. ", "original_text": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. "}, "hash": "05773e85f894470743670a081e1318beed64ef9f9e0386ad7ca9de9c8be30793", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others. ", "mimetype": "text/plain", "start_char_idx": 4305, "end_char_idx": 4386, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "151978b9-f815-4a9c-b5cf-bac8b99a00ed", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. ", "original_text": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb4d4d6e-4c87-43c4-af50-fc12aea12154", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "*E-mail address: p.karczmarek@pollub.pl (M.  Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n", "original_text": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others. "}, "hash": "1e228d4d00fccd65bb96f99f453f1ec1bf9a00d75ab5e728fe26a1ff1f3436e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "588390ac-0c67-45c8-a659-a8d5f8eff151", "node_type": "1", "metadata": {"window": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. ", "original_text": "The latter is based on training and traversing special forest of binary search trees. "}, "hash": "df0f2da87e60e69f32cb1140faf0a6ef496c774e0178731ee0a607dc6be2d9b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. ", "mimetype": "text/plain", "start_char_idx": 4386, "end_char_idx": 4552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "588390ac-0c67-45c8-a659-a8d5f8eff151", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. ", "original_text": "The latter is based on training and traversing special forest of binary search trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "151978b9-f815-4a9c-b5cf-bac8b99a00ed", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tokovarov). *\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. ", "original_text": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24]. "}, "hash": "ef88fdcdbe4349edd50ac7b6f00f0d66e67accd84215d2fbfa17791eeedce302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e92cbd3a-44e6-4e8a-b720-75de492a5659", "node_type": "1", "metadata": {"window": "Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach. ", "original_text": "The trees are trained on a subset of original dataset. "}, "hash": "fb0f44106797d9770bfb9d0a5d4e7812f7c009075f3ab2c7ae431a65f870d72f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The latter is based on training and traversing special forest of binary search trees. ", "mimetype": "text/plain", "start_char_idx": 4552, "end_char_idx": 4638, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e92cbd3a-44e6-4e8a-b720-75de492a5659", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach. ", "original_text": "The trees are trained on a subset of original dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "588390ac-0c67-45c8-a659-a8d5f8eff151", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "*\n\nhttps://doi.org/10.1016/j.ins.2021.10.075\n0020-0255/\u00a9 2021 The Author(s).  Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. ", "original_text": "The latter is based on training and traversing special forest of binary search trees. "}, "hash": "aac3abdee7f62a7039ed107fb1d6a02d492688cce15f82f64ca11b6c7cb65e51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7db1e84-e8e3-4cc6-b812-c5c0e93d4a53", "node_type": "1", "metadata": {"window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. ", "original_text": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records. "}, "hash": "708f43fe180ce467a1a417f6237995b4054da8b3162ecd29d33a3f007d64cd73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The trees are trained on a subset of original dataset. ", "mimetype": "text/plain", "start_char_idx": 4638, "end_char_idx": 4693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7db1e84-e8e3-4cc6-b812-c5c0e93d4a53", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. ", "original_text": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e92cbd3a-44e6-4e8a-b720-75de492a5659", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Published by Elsevier Inc.\n This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach. ", "original_text": "The trees are trained on a subset of original dataset. "}, "hash": "ad7bbbf5c97a0d0c477713fa7bca18701afa6ef277e4baea8a1264ebf6dd7948", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "211e9574-c573-4639-99c0-62c8f77101cf", "node_type": "1", "metadata": {"window": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described. ", "original_text": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n"}, "hash": "6532309bf044fe635ee1329be1b0e0e926380371dc0b2e598a7fffb8fc10a107", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records. ", "mimetype": "text/plain", "start_char_idx": 4693, "end_char_idx": 4785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "211e9574-c573-4639-99c0-62c8f77101cf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described. ", "original_text": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7db1e84-e8e3-4cc6-b812-c5c0e93d4a53", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. ", "original_text": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records. "}, "hash": "82e16fbbd5f19fbfd9e697e313aaa09e83f115e369317a996c57407f7e9166e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c7ee957-ae5b-444a-952d-2a87c7dbb70e", "node_type": "1", "metadata": {"window": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. ", "original_text": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. "}, "hash": "2bbf7ddb0c0a827b0e3e80dee72843b64574c693391962cb27f8dad3f79f5c09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n", "mimetype": "text/plain", "start_char_idx": 4785, "end_char_idx": 4887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c7ee957-ae5b-444a-952d-2a87c7dbb70e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. ", "original_text": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "211e9574-c573-4639-99c0-62c8f77101cf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nworks [19,20], scan statistics based on expectations [21], and many others.  Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described. ", "original_text": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n"}, "hash": "8309b5ab322ab1925fd11be9f13f900a4fe4e89901293374ca2f569ef1ea9a49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1761468-3d35-4fe7-9438-9c635d673014", "node_type": "1", "metadata": {"window": "The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set. ", "original_text": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. "}, "hash": "9df1d1564f2d5b38239e41109d37a1f5fd093225d4941a286de7b49343aae709", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. ", "mimetype": "text/plain", "start_char_idx": 4887, "end_char_idx": 5014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1761468-3d35-4fe7-9438-9c635d673014", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set. ", "original_text": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c7ee957-ae5b-444a-952d-2a87c7dbb70e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Finally, let us mention density-based methods that are very classic approaches such as k-nearest neighbor classifier-based results [22] and Isolation Forest [23,24].  The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. ", "original_text": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection. "}, "hash": "caa3aa9b8cdb12038a23ae21f9ae0c77912be7a8b25fb09a4a90a4ab91dcba7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "422c1b1e-1241-442f-9b30-eb7686b5a9a9", "node_type": "1", "metadata": {"window": "The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. ", "original_text": "There are also a few enhancements of the classic approach. "}, "hash": "8daf3fe3fa166e9290e66694bac23f5bc2b8a424233a8feeb362bf70dfaec8eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. ", "mimetype": "text/plain", "start_char_idx": 5014, "end_char_idx": 5202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "422c1b1e-1241-442f-9b30-eb7686b5a9a9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. ", "original_text": "There are also a few enhancements of the classic approach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1761468-3d35-4fe7-9438-9c635d673014", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The latter is based on training and traversing special forest of binary search trees.  The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set. ", "original_text": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27]. "}, "hash": "2ed6f6c2f4876132c7a05dff8ab2157e3d75a226a8161a4b7df2add4308a62cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "893269d3-04e4-442a-8a89-a8da7c856726", "node_type": "1", "metadata": {"window": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. ", "original_text": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. "}, "hash": "a94229aa46016f7df7047beba5bd9fc8c324cea2b44793013b1e6430d449ecb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are also a few enhancements of the classic approach. ", "mimetype": "text/plain", "start_char_idx": 5202, "end_char_idx": 5261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "893269d3-04e4-442a-8a89-a8da7c856726", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. ", "original_text": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "422c1b1e-1241-442f-9b30-eb7686b5a9a9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The trees are trained on a subset of original dataset.  Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. ", "original_text": "There are also a few enhancements of the classic approach. "}, "hash": "c0e82ad407f81c4665ad93ba6b333da97d15827a0673cb78f1aeb1559c2d7908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81b6b66d-c0f2-49a3-9faf-df4a09fbb473", "node_type": "1", "metadata": {"window": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. ", "original_text": "In [29] a so-called CBiForest was described. "}, "hash": "afc69879d7d94b00464ec06dcc206fd01c738e8511124a48e05ae52879ebc3e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. ", "mimetype": "text/plain", "start_char_idx": 5261, "end_char_idx": 5392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "81b6b66d-c0f2-49a3-9faf-df4a09fbb473", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. ", "original_text": "In [29] a so-called CBiForest was described. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "893269d3-04e4-442a-8a89-a8da7c856726", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Anomaly scores depend on the length of tree paths traversed by each of the dataset records.  It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. ", "original_text": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed. "}, "hash": "8d2160308505e79256a9a2ae1821db37276e41d51fd08a313da9ba602d74f117", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c27217d7-ef17-4d92-845a-aa78875bcd63", "node_type": "1", "metadata": {"window": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. ", "original_text": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. "}, "hash": "0051995d1421235a071af6f863a48727fa1772023e1f6af41710fe2d6234b4fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [29] a so-called CBiForest was described. ", "mimetype": "text/plain", "start_char_idx": 5392, "end_char_idx": 5437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c27217d7-ef17-4d92-845a-aa78875bcd63", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. ", "original_text": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81b6b66d-c0f2-49a3-9faf-df4a09fbb473", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is worth noting that Isolation Forest is often one of a few methods recommended by practitioners.\n\n Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. ", "original_text": "In [29] a so-called CBiForest was described. "}, "hash": "bca83f8a03fd56662bf083f8dcb7a0e5dda3a53cc4d08fec36928b6f7c5845ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3250fa0-5599-4417-9fad-18d296e625c7", "node_type": "1", "metadata": {"window": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher. ", "original_text": "The smaller of them is considered as anomaly set. "}, "hash": "d3d192b1f44086a38c1931515624d4adf6cf4408c45db3dfdab46f7292dfc025", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. ", "mimetype": "text/plain", "start_char_idx": 5437, "end_char_idx": 5549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3250fa0-5599-4417-9fad-18d296e625c7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher. ", "original_text": "The smaller of them is considered as anomaly set. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c27217d7-ef17-4d92-845a-aa78875bcd63", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Isolation Forest is still attracting a great interest of the researchers dealing with the issues related to outlier detection.  One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. ", "original_text": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately. "}, "hash": "ffc81a833a545a0139b03eafe25dd2ab3252e1acead0baefc2915f8844b8ad0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "001e4c55-e8ee-4935-a872-cb8450936387", "node_type": "1", "metadata": {"window": "There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32]. ", "original_text": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. "}, "hash": "9991409804ba3e2dc2e802e7ed788000f152253ecf776996fa871120b0e69728", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The smaller of them is considered as anomaly set. ", "mimetype": "text/plain", "start_char_idx": 5549, "end_char_idx": 5599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "001e4c55-e8ee-4935-a872-cb8450936387", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32]. ", "original_text": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3250fa0-5599-4417-9fad-18d296e625c7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "One can easily find up-to-date examples of applications of Isolation Forest for credit card fraud detection [25], analysis of anomaly detection algorithms [26], and malware analysis [27].  There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher. ", "original_text": "The smaller of them is considered as anomaly set. "}, "hash": "c61b1508f7253857edd4a4f0a167e77472a742bdc115a48d00b3183cebd3e268", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3592ffa2-3af6-43b6-839b-8c5228b4d3a9", "node_type": "1", "metadata": {"window": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees. ", "original_text": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. "}, "hash": "3f640e551bb0b01f4f877437da423180e97363e04cdf7076b7bd98ff5674752f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. ", "mimetype": "text/plain", "start_char_idx": 5599, "end_char_idx": 5702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3592ffa2-3af6-43b6-839b-8c5228b4d3a9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees. ", "original_text": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "001e4c55-e8ee-4935-a872-cb8450936387", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "There are also a few enhancements of the classic approach.  Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32]. ", "original_text": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies. "}, "hash": "8a85ee4615acee8e579af39609747ddcfa791740c157040c2c53d4cffee1a668", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d217b2f-a611-41d3-858b-955a0d06df2b", "node_type": "1", "metadata": {"window": "In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. ", "original_text": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. "}, "hash": "006c74babca125a1fd4ca52fe96982881b506aec2e1b945d4b5bde620db30b6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. ", "mimetype": "text/plain", "start_char_idx": 5702, "end_char_idx": 5910, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d217b2f-a611-41d3-858b-955a0d06df2b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. ", "original_text": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3592ffa2-3af6-43b6-839b-8c5228b4d3a9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Namely, in [28] the lines not necessarily parallel to the Cartesian axes and dividing the subsequent groups of data were proposed.  In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees. ", "original_text": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion. "}, "hash": "86e9843ef8d084b1f4633715bd30862ad1fe58d6471b243af5b24a85044a328c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60166156-96ca-4e97-a479-e0b8f0d1abe5", "node_type": "1", "metadata": {"window": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. ", "original_text": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. "}, "hash": "4bb9036de01a3bfbe7c2af9677ffbcca7f59f8ccc7f4007bda5d92b5c3dfc75c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. ", "mimetype": "text/plain", "start_char_idx": 5910, "end_char_idx": 6031, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60166156-96ca-4e97-a479-e0b8f0d1abe5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. ", "original_text": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d217b2f-a611-41d3-858b-955a0d06df2b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In [29] a so-called CBiForest was described.  It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. ", "original_text": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle. "}, "hash": "49cb5b429bab6f6843a62e9b2b67cf7de9ff39e497ad989ebf4b7d997ed13bf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef3b3019-92d3-461e-8452-28fd220ba5f9", "node_type": "1", "metadata": {"window": "The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. ", "original_text": "However, the cost of training time is higher. "}, "hash": "e7e14626abe7a4d7e9c53495230c48a2cf8cbce1729068b12e2c80873de9a510", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. ", "mimetype": "text/plain", "start_char_idx": 6031, "end_char_idx": 6148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef3b3019-92d3-461e-8452-28fd220ba5f9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. ", "original_text": "However, the cost of training time is higher. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60166156-96ca-4e97-a479-e0b8f0d1abe5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is based on a preprocessing of the data using K-Means [30] to find two input datasets considered separately.  The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. ", "original_text": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies. "}, "hash": "f8afedd303c26dee1851c1a2fac2d41c24f10b5c7094ec13276d831f1893d97c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57eaba55-7225-4a6e-8076-5e63eb1a3c0a", "node_type": "1", "metadata": {"window": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. ", "original_text": "Various novel anomaly scores were discussed in [32]. "}, "hash": "90b37eb1188d4ab5de3d3107b44d424ae0be361f14278e7d9755a96fd4d1c41e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the cost of training time is higher. ", "mimetype": "text/plain", "start_char_idx": 6148, "end_char_idx": 6194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57eaba55-7225-4a6e-8076-5e63eb1a3c0a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. ", "original_text": "Various novel anomaly scores were discussed in [32]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef3b3019-92d3-461e-8452-28fd220ba5f9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The smaller of them is considered as anomaly set.  In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. ", "original_text": "However, the cost of training time is higher. "}, "hash": "dc2fa2f0b34969109587c3c457e444b479a0bdb6803a3c4eeb86a18fbf5a68b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd50e65a-c940-4750-ba4e-32317f986b2f", "node_type": "1", "metadata": {"window": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. ", "original_text": "The authors marginalized randomness impact onto binary search trees. "}, "hash": "a155cdf0bc2d15196187455d692de89e3fcf40c9fb57d41c17b79279be80cc64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Various novel anomaly scores were discussed in [32]. ", "mimetype": "text/plain", "start_char_idx": 6194, "end_char_idx": 6247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd50e65a-c940-4750-ba4e-32317f986b2f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. ", "original_text": "The authors marginalized randomness impact onto binary search trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57eaba55-7225-4a6e-8076-5e63eb1a3c0a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In [31] the authors present a method well suited for detecting both scattered and clustered anomalies.  Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. ", "original_text": "Various novel anomaly scores were discussed in [32]. "}, "hash": "9133ce77741df4d590520fc9d1801f8b5dbd329b1d6e18cbbdf1a1cf0532a51f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54c2161e-313d-4aa6-9f11-e0caeaaaa3f6", "node_type": "1", "metadata": {"window": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n", "original_text": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. "}, "hash": "550fd0b0de142161cce359d594d52eb60cad58f9e68e2519b45f531e3f6f9ae3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The authors marginalized randomness impact onto binary search trees. ", "mimetype": "text/plain", "start_char_idx": 6247, "end_char_idx": 6316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54c2161e-313d-4aa6-9f11-e0caeaaaa3f6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n", "original_text": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd50e65a-c940-4750-ba4e-32317f986b2f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Its main idea is based upon generating split value with application of special technique of finding a hyperplane separating the data in the best way in the sense of specially constructed splitting criterion.  What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. ", "original_text": "The authors marginalized randomness impact onto binary search trees. "}, "hash": "054bc984614cc9c5adb55d2000f8605fe66069fa468afb2196af1be851cb5e4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02c129ec-b9a9-446b-80e2-d9b92036c476", "node_type": "1", "metadata": {"window": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. ", "original_text": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. "}, "hash": "21141e7ef0857f8f554d75bb4586422bbdba25075e0333108b65e1b0daecf507", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. ", "mimetype": "text/plain", "start_char_idx": 6316, "end_char_idx": 6414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "02c129ec-b9a9-446b-80e2-d9b92036c476", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. ", "original_text": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54c2161e-313d-4aa6-9f11-e0caeaaaa3f6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "What is interesting, the hyperplanes are not necessarily perpendicular to the axis, but can rather have arbitrary angle.  The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n", "original_text": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33]. "}, "hash": "94e6dc3de1b135240d138880d1cc1282a7e5be596c70521671d9850446e9916d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38d3a3c3-2eff-4a9f-b98b-04b04a9861e7", "node_type": "1", "metadata": {"window": "However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. ", "original_text": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. "}, "hash": "d5c3e151f23bc3d12b722803291105a4c0d7bd8ba7012463967578decdd5771b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. ", "mimetype": "text/plain", "start_char_idx": 6414, "end_char_idx": 6536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38d3a3c3-2eff-4a9f-b98b-04b04a9861e7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. ", "original_text": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02c129ec-b9a9-446b-80e2-d9b92036c476", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The method exhibits better performance, compared to original iForest, especially in the case of clustered anomalies.  However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. ", "original_text": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35]. "}, "hash": "b583b176b74b4f5bcc39cce632e9fc3209130bc60b51ddbb942cad1617709238", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b11602a1-305f-4589-a538-aa9d47128e1b", "node_type": "1", "metadata": {"window": "Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness. ", "original_text": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. "}, "hash": "a7f07da52f0c5a72ad2b30f8bdb0278d163b69e6d581d2753470e2559e602dbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. ", "mimetype": "text/plain", "start_char_idx": 6536, "end_char_idx": 6643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b11602a1-305f-4589-a538-aa9d47128e1b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness. ", "original_text": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38d3a3c3-2eff-4a9f-b98b-04b04a9861e7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, the cost of training time is higher.  Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. ", "original_text": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method. "}, "hash": "4ccd739988fadd32a2f19a0ebc1b0cf1ce581c35567947c8b0a68c349f029c4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69a1301b-1ae2-4fc1-a17d-060c59c3074c", "node_type": "1", "metadata": {"window": "The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n", "original_text": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. "}, "hash": "4a656809d44f494ad18055dac9a90606ed08701d2be0103ed38ad263f354d99b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. ", "mimetype": "text/plain", "start_char_idx": 6643, "end_char_idx": 6742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69a1301b-1ae2-4fc1-a17d-060c59c3074c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n", "original_text": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b11602a1-305f-4589-a538-aa9d47128e1b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Various novel anomaly scores were discussed in [32].  The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness. ", "original_text": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor. "}, "hash": "169e576ae5daaf73f31d749150913286db0cb73a6292725c3829ad315a088941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "915b28c5-0558-471c-954a-86f4edff2c7b", "node_type": "1", "metadata": {"window": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. ", "original_text": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n"}, "hash": "b0dbce2576c90c6674eb01f92a87cff4889565e32de735295e76e37b28d95a22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. ", "mimetype": "text/plain", "start_char_idx": 6742, "end_char_idx": 6916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "915b28c5-0558-471c-954a-86f4edff2c7b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. ", "original_text": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69a1301b-1ae2-4fc1-a17d-060c59c3074c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The authors marginalized randomness impact onto binary search trees.  The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n", "original_text": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36]. "}, "hash": "0330d2bf8cf85fc075fe674041594a6f751d38702626307c20036df79fa266db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6c6b514-bd52-4066-8aab-bc748c5d750c", "node_type": "1", "metadata": {"window": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF). ", "original_text": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. "}, "hash": "5d95b34533667873c74f20d63fd9ba883f8362b42f2a9237cc4ee7e87016df09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n", "mimetype": "text/plain", "start_char_idx": 6916, "end_char_idx": 6994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6c6b514-bd52-4066-8aab-bc748c5d750c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF). ", "original_text": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "915b28c5-0558-471c-954a-86f4edff2c7b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The binary trees were replaced by n-ary search trees with a new version of anomaly score in [33].  K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. ", "original_text": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n"}, "hash": "8ecd77cd250f4bf2055320fa5befaa600af0339982d10094ababc5cf32f6f805", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d7b9f6e-ce1c-4b6c-93db-5571db82488c", "node_type": "1", "metadata": {"window": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. ", "original_text": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. "}, "hash": "0f2fa86832a47961b755f777bbd2535aa36e6be7da9b38306279c7bb585aa63a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. ", "mimetype": "text/plain", "start_char_idx": 6994, "end_char_idx": 7227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d7b9f6e-ce1c-4b6c-93db-5571db82488c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. ", "original_text": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6c6b514-bd52-4066-8aab-bc748c5d750c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "K-means and well-known elbow rule [34] was also utilized to find optimal number of tree nodes at each tree split in [35].  Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF). ", "original_text": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples. "}, "hash": "552aa01c32ac83d6f92823026c1cb0d7f3f53173d7d6a075277314ebed4f6908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ccabd08-0046-430e-b015-caf1082c7c51", "node_type": "1", "metadata": {"window": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. ", "original_text": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness. "}, "hash": "80be63ccec000524a7b03c9776a29b3d9706247be78406ce9f82962416b023bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. ", "mimetype": "text/plain", "start_char_idx": 7227, "end_char_idx": 7549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ccabd08-0046-430e-b015-caf1082c7c51", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. ", "original_text": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d7b9f6e-ce1c-4b6c-93db-5571db82488c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such approach shortens the time of traversing the forest of trees and improves the accuracy of the method.  Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. ", "original_text": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries. "}, "hash": "c664c42000ec54f137e0e8b3af85700713e1a491ca0418d48fa12fb8c8937ef7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d5b9c34-967d-461c-8b0e-7b1694df9c0a", "node_type": "1", "metadata": {"window": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function. ", "original_text": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n"}, "hash": "1f44ee16b6d70825e43d6f284bce6e460b97b69188e546d808a10881ab03f58c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness. ", "mimetype": "text/plain", "start_char_idx": 7549, "end_char_idx": 7673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d5b9c34-967d-461c-8b0e-7b1694df9c0a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function. ", "original_text": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ccabd08-0046-430e-b015-caf1082c7c51", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, the number of divisions reflects the structure of data minimizing the randomness factor.  Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. ", "original_text": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness. "}, "hash": "eeca2dd6f94815b034ee87773cd7d0aa2d8f3bf3764569bce03a1237f6bbac04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf5d4656-780a-470e-a5af-cad02eb55a45", "node_type": "1", "metadata": {"window": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset. ", "original_text": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. "}, "hash": "3016174ce82048a52b26e5093c6516e870366e4de0b6971b7f46adb9d231ee58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n", "mimetype": "text/plain", "start_char_idx": 7673, "end_char_idx": 7814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf5d4656-780a-470e-a5af-cad02eb55a45", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset. ", "original_text": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d5b9c34-967d-461c-8b0e-7b1694df9c0a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Also fuzzy sets were applied to modify the Isolation Forest algorithm on a basis of membership of the records to the clusters obtained in the process of trees training [36].  A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function. ", "original_text": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n"}, "hash": "824d25951a7c240bc83ae9ba5c1b66b04571435c57abf8642108421f65310d2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbe82193-0bba-4424-b077-60c932e287ac", "node_type": "1", "metadata": {"window": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions. ", "original_text": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF). "}, "hash": "bee3eea4fcd111ecd791dfd533f2a7c070d8c44bb6d3ef5613102abe39a403cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. ", "mimetype": "text/plain", "start_char_idx": 7814, "end_char_idx": 8088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbe82193-0bba-4424-b077-60c932e287ac", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions. ", "original_text": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf5d4656-780a-470e-a5af-cad02eb55a45", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A survey of anomaly detection methods can be found, among others in [37\u201340].\n\n Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset. ", "original_text": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity. "}, "hash": "e61db20f14709f41c8d636bc365ab8362353402ea2e0aaa5b6919f1d15194ac9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e326beee-99b5-4c60-8dfe-d4e88035c766", "node_type": "1", "metadata": {"window": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. ", "original_text": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. "}, "hash": "6f6995c7f4e153bb6d7f617879a5b764e95af33bf1893aa6c339c9d1c08edd97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF). ", "mimetype": "text/plain", "start_char_idx": 8088, "end_char_idx": 8166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e326beee-99b5-4c60-8dfe-d4e88035c766", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. ", "original_text": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbe82193-0bba-4424-b077-60c932e287ac", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Despite many different attempts to improve the Isolation Forest algorithm, there is still no method that would unambiguously answer where to perform a random or pseudo-random division at the stage of building trees based on samples.  In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions. ", "original_text": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF). "}, "hash": "ff2363cf923c0649e62fcef880e9283936afc077d9cb96e6ccc1a65ca4e56307", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60b30f51-7b5e-482c-a801-e6a3f7cb2689", "node_type": "1", "metadata": {"window": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n", "original_text": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. "}, "hash": "79528fc8ba8a2fd3d80a0931a58c0be26cd1b2d6cf55bc01105d0258a8f7242b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. ", "mimetype": "text/plain", "start_char_idx": 8166, "end_char_idx": 8326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60b30f51-7b5e-482c-a801-e6a3f7cb2689", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n", "original_text": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e326beee-99b5-4c60-8dfe-d4e88035c766", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In particular, the authors of this manuscript are not aware of any published method that would effectively minimize the possibility of creating such divisions (splits) within a group of closely spaced points that are known to be definitely not anomalies or points that are in such clusters, but close to their boundaries.  At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. ", "original_text": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before. "}, "hash": "2dcd96c595a14b37862cac46d294544ae42f7179ef6296c2972b24068dccbb95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "715029af-8e11-4fc9-bbee-2d443ea9cba8", "node_type": "1", "metadata": {"window": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. ", "original_text": "We construct the function in the form of piecewise defined probability density function. "}, "hash": "49098ec26c0fc66651485293f533c9ae46413445e7af83b702c833d35b8e41bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. ", "mimetype": "text/plain", "start_char_idx": 8326, "end_char_idx": 8497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "715029af-8e11-4fc9-bbee-2d443ea9cba8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. ", "original_text": "We construct the function in the form of piecewise defined probability density function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60b30f51-7b5e-482c-a801-e6a3f7cb2689", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "At the same time, such a desired method should be able to find these intersection points with some influence of randomness.  It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n", "original_text": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data. "}, "hash": "d96c40dc235cd4c1b699236029d919408efe08e19eec6efc5a95a5dc7e1e0a32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ccaca2f-6d32-4785-8939-cdb0ec10ec1a", "node_type": "1", "metadata": {"window": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. ", "original_text": "The function is defined on the separate segments between neighboring points of the training dataset. "}, "hash": "f7128306ec5e1a414bd27a7a61c3ea9fb3c24634cd399cf2296de0457e7f54e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We construct the function in the form of piecewise defined probability density function. ", "mimetype": "text/plain", "start_char_idx": 8497, "end_char_idx": 8586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ccaca2f-6d32-4785-8939-cdb0ec10ec1a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. ", "original_text": "The function is defined on the separate segments between neighboring points of the training dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "715029af-8e11-4fc9-bbee-2d443ea9cba8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It seems that methods based on probability, in particular, inverse cumulative distribution function, may be a good solution of the problem.\n\n The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. ", "original_text": "We construct the function in the form of piecewise defined probability density function. "}, "hash": "f58a6e4d0499523667ea6cd60ef3fe17e82e750d7d16adf6d5b5de82b809a2ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50d680ab-c797-4154-a4ae-848b9c844e8b", "node_type": "1", "metadata": {"window": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. ", "original_text": "Due to useful analytical properties we use Kernel Density Estimation functions. "}, "hash": "a27ec8248ab6de41ad311e62650969feefa2761e69a681af5dd0ef44b7506c3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The function is defined on the separate segments between neighboring points of the training dataset. ", "mimetype": "text/plain", "start_char_idx": 8586, "end_char_idx": 8687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50d680ab-c797-4154-a4ae-848b9c844e8b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. ", "original_text": "Due to useful analytical properties we use Kernel Density Estimation functions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ccaca2f-6d32-4785-8939-cdb0ec10ec1a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The main goal of this study is to propose and verify a novel generalization of the Isolation Forest algorithm that is to enhance the effectiveness of outlier detection and preserve strong sides of the original approach, i.e., its stability and low computational complexity.  We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. ", "original_text": "The function is defined on the separate segments between neighboring points of the training dataset. "}, "hash": "55d4fc59ecca21a53fda4832820371313b32f3e39ce10d144fc979715d023706", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4153881-162a-436e-a899-ba1d01b9395f", "node_type": "1", "metadata": {"window": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. ", "original_text": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. "}, "hash": "5a2f7b21314fca609cdf1ec70e6a35acbe2436126bd44b644a6bf2c7a1a60624", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Due to useful analytical properties we use Kernel Density Estimation functions. ", "mimetype": "text/plain", "start_char_idx": 8687, "end_char_idx": 8767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4153881-162a-436e-a899-ba1d01b9395f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. ", "original_text": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50d680ab-c797-4154-a4ae-848b9c844e8b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We call our approach Probabilistic Generalization of Isolation Forest (PGIF).  The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. ", "original_text": "Due to useful analytical properties we use Kernel Density Estimation functions. "}, "hash": "3ad6aeec132c727bf0b32730b7d5501a0bfd5c01e7a8bd659318e75e16493d18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2c4c4ac-749a-4ec5-bd97-727cfc98e6a1", "node_type": "1", "metadata": {"window": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. ", "original_text": "We describe the idea in Section 3 in more detail.\n\n"}, "hash": "cd6a1a8848b18f0b5110145e0e21da18709e6a0ebd7ee151b32f32bc40c2292a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. ", "mimetype": "text/plain", "start_char_idx": 8767, "end_char_idx": 8890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2c4c4ac-749a-4ec5-bd97-727cfc98e6a1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. ", "original_text": "We describe the idea in Section 3 in more detail.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4153881-162a-436e-a899-ba1d01b9395f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization is based upon an innovative algorithm that, to the best of the authors\u2019 knowledge, has not been applied to Isolation Forest before.  We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. ", "original_text": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power. "}, "hash": "0f138877a1ce3250fea5ea13b2c7be22c6c7396acc28755a1343d5960260de68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fe8c804-d686-479e-a796-c18707caaa8d", "node_type": "1", "metadata": {"window": "We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n", "original_text": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. "}, "hash": "f6f1420a9eec7a3a7e598c890322bfbf35e8b8a4541c1be2efd71123d6758eae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We describe the idea in Section 3 in more detail.\n\n", "mimetype": "text/plain", "start_char_idx": 8890, "end_char_idx": 8941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fe8c804-d686-479e-a796-c18707caaa8d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n", "original_text": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2c4c4ac-749a-4ec5-bd97-727cfc98e6a1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We assign different probabilities to various regions of the explored space by building an empirical distribution of probability density on the basis of the training data.  We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. ", "original_text": "We describe the idea in Section 3 in more detail.\n\n"}, "hash": "a34c6dee2cfe318a0e95bc60a903b8c8e439e1b0e6e7cba34be5abb7c5d14bf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01dd60b2-0650-4d22-bcf8-cf274b13fd0a", "node_type": "1", "metadata": {"window": "The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows. ", "original_text": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. "}, "hash": "a2ee8f079e011697484299d445e336ca9205dda0e3f402c9f0a7a7eb2b9e5578", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. ", "mimetype": "text/plain", "start_char_idx": 8941, "end_char_idx": 9097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01dd60b2-0650-4d22-bcf8-cf274b13fd0a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows. ", "original_text": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fe8c804-d686-479e-a796-c18707caaa8d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We construct the function in the form of piecewise defined probability density function.  The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n", "original_text": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable. "}, "hash": "7d8c0d8a0898cfa94a930708564dc895b5355dc2c09f7e07679f7fd0e5af7919", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "236efabe-d408-471e-9b4b-82400ab2ee1f", "node_type": "1", "metadata": {"window": "Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm. ", "original_text": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. "}, "hash": "f094c8fc0cb74edf9f6e3bf386ae1fb04e81ad53fcfa772edeea579421fa1569", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. ", "mimetype": "text/plain", "start_char_idx": 9097, "end_char_idx": 9259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "236efabe-d408-471e-9b4b-82400ab2ee1f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm. ", "original_text": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01dd60b2-0650-4d22-bcf8-cf274b13fd0a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The function is defined on the separate segments between neighboring points of the training dataset.  Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows. ", "original_text": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter. "}, "hash": "cc552636fcf9566456e26f07069abfedddcdcff392566fe929a35ef0bda4c9b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffc38d8e-19a0-4b6d-a780-8ba6cb678088", "node_type": "1", "metadata": {"window": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. ", "original_text": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. "}, "hash": "d5148d2cb789ca511e79a2ef7ab6a831d03bc5ab5e15f85986ce0c7a41739446", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. ", "mimetype": "text/plain", "start_char_idx": 9259, "end_char_idx": 9366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ffc38d8e-19a0-4b6d-a780-8ba6cb678088", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. ", "original_text": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "236efabe-d408-471e-9b4b-82400ab2ee1f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Due to useful analytical properties we use Kernel Density Estimation functions.  The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm. ", "original_text": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees. "}, "hash": "8362b1fb765b6684634cb6614312cec2cc0047f525351fdceeeb61d7e0a82ba1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae7a7653-2291-4b3d-a597-512c51fab6d5", "node_type": "1", "metadata": {"window": "We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. ", "original_text": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. "}, "hash": "350e0af21e87568c6e84390b1776f775a412634e5d6857964431ad66d1b7b96e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. ", "mimetype": "text/plain", "start_char_idx": 9366, "end_char_idx": 9613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae7a7653-2291-4b3d-a597-512c51fab6d5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. ", "original_text": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffc38d8e-19a0-4b6d-a780-8ba6cb678088", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The function is built thus that the probability cumulated on a segment is proportional to its length raised to k-th power.  We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. ", "original_text": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located. "}, "hash": "a1dacfd309cfab64c51b4c660d65a5efaeecde015ac1488451b87b1bf61c8bbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4b54ece-e560-48b0-98af-a46a5d9133c4", "node_type": "1", "metadata": {"window": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n", "original_text": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n"}, "hash": "2fb7b4ce4d50133e8566eaf4156bf50a37a0076cb3df01c4330545bf1ae5725d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. ", "mimetype": "text/plain", "start_char_idx": 9613, "end_char_idx": 9721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4b54ece-e560-48b0-98af-a46a5d9133c4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n", "original_text": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae7a7653-2291-4b3d-a597-512c51fab6d5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We describe the idea in Section 3 in more detail.\n\n The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. ", "original_text": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers. "}, "hash": "36473ff2a3572f67b0b82e9abf56353cc30e2dc351406e53d3f818a0f72dfbce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c3c9e45-cf7e-4ac3-b2f6-5fdeb4c664cf", "node_type": "1", "metadata": {"window": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2. ", "original_text": "The paper is structured as follows. "}, "hash": "b27df4d50759c9a138b1ea890d66f862a5fba84cd0c84c71a0cac1eabb10175f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n", "mimetype": "text/plain", "start_char_idx": 9721, "end_char_idx": 9873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c3c9e45-cf7e-4ac3-b2f6-5fdeb4c664cf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2. ", "original_text": "The paper is structured as follows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4b54ece-e560-48b0-98af-a46a5d9133c4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization is based upon the assumption that not every split of the dataset in course of building an isolation tree is equally profitable.  Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n", "original_text": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n"}, "hash": "b0885c41e88071b5a0f3977ae27b280d8fd1d3919aee02175071d0756ab0fc49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22728236-34c9-499e-9683-0db013d86936", "node_type": "1", "metadata": {"window": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. ", "original_text": "In Section 2 we recall the Isolation Forest algorithm. "}, "hash": "af90b7712cb43def94c950e7206b1f3d94321c8d2f81f024d2ac546fb2c2ebfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The paper is structured as follows. ", "mimetype": "text/plain", "start_char_idx": 9873, "end_char_idx": 9909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22728236-34c9-499e-9683-0db013d86936", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. ", "original_text": "In Section 2 we recall the Isolation Forest algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c3c9e45-cf7e-4ac3-b2f6-5fdeb4c664cf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Namely, a split through a gap, separating clusters or outliers, is more profitable as it allows to isolate outliers earlier, making the path in the tree shorter.  On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2. ", "original_text": "The paper is structured as follows. "}, "hash": "ad72f1a107fc767c542df61692c9668a46108fe893a13cf4c1d901da73bc1544", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6e61dc2-b23d-4d95-8f2c-f13f8bf382f2", "node_type": "1", "metadata": {"window": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created. ", "original_text": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. "}, "hash": "144227996b21c02c75b4445a0b5546ccfaa33ce6125dddaad5b8f1baf7fce71d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 2 we recall the Isolation Forest algorithm. ", "mimetype": "text/plain", "start_char_idx": 9909, "end_char_idx": 9964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6e61dc2-b23d-4d95-8f2c-f13f8bf382f2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created. ", "original_text": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22728236-34c9-499e-9683-0db013d86936", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "On the other hand, the stability of Isolation Forest is based upon random building of the Isolation Trees.  Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. ", "original_text": "In Section 2 we recall the Isolation Forest algorithm. "}, "hash": "6d347ff1b419b37043697e44a26bf6dc85f6cafa1e638d05b89ea04d576bb2f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84804fb-d7bc-4cf1-8e54-b2174297a519", "node_type": "1", "metadata": {"window": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. ", "original_text": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. "}, "hash": "bab493b2ad66304b4c7a92a26957a94d6a8781b28c6024c7c5807a20356337ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. ", "mimetype": "text/plain", "start_char_idx": 9964, "end_char_idx": 10103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b84804fb-d7bc-4cf1-8e54-b2174297a519", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. ", "original_text": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6e61dc2-b23d-4d95-8f2c-f13f8bf382f2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, randomness of the process of building an Isolation tree should be preserved but the cluster regions have to be assigned with lower probability density compared to the out-of-cluster space where the anomalies are supposed to be located.  In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created. ", "original_text": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3. "}, "hash": "42ad77aaf12b2b144a0dca03d315ada0d991658046018a79005aa67eedd4731c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a71e7d6a-9bf9-42e0-8f22-6345f0798f9d", "node_type": "1", "metadata": {"window": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. ", "original_text": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n"}, "hash": "52b7e7c5a989dcb412558f16e82418b0d5c57fe4e8ed14e380dabf0da6a5e3ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. ", "mimetype": "text/plain", "start_char_idx": 10103, "end_char_idx": 10204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a71e7d6a-9bf9-42e0-8f22-6345f0798f9d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. ", "original_text": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84804fb-d7bc-4cf1-8e54-b2174297a519", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In such a manner we ensure more meaningful splitting of the data leading to earlier separation of outliers.  The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. ", "original_text": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion. "}, "hash": "3433f36cfbefac85d56ed7efa4ac135a1ea4291ecfaeeff4a8e4ca2eb3b9f175", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6185628d-4b45-44ed-a03f-2f7a76e55cf9", "node_type": "1", "metadata": {"window": "The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. ", "original_text": "## 2. "}, "hash": "bac22ee93bc7112a7ca989908978f0f10d97d86de80d0456d792c34e45cd62e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n", "mimetype": "text/plain", "start_char_idx": 10204, "end_char_idx": 10293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6185628d-4b45-44ed-a03f-2f7a76e55cf9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. ", "original_text": "## 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a71e7d6a-9bf9-42e0-8f22-6345f0798f9d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The results of the numeric experiments clearly show that the proposed modifications allow to achieve higher performance in the majority of test cases.\n\n The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. ", "original_text": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n"}, "hash": "bdb22d88cc8ed5cde0395e55683e80a8bff1bbdc7bdd8f25441a5eaec838e84d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9332d5e-ed37-45ba-9587-6f832e2c1cfc", "node_type": "1", "metadata": {"window": "In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n", "original_text": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. "}, "hash": "baf6db3c4a27693dce979d03c46032ec028e9bea60fa207883d6f9d075836730", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 2. ", "mimetype": "text/plain", "start_char_idx": 10293, "end_char_idx": 10299, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a9332d5e-ed37-45ba-9587-6f832e2c1cfc", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n", "original_text": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6185628d-4b45-44ed-a03f-2f7a76e55cf9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The paper is structured as follows.  In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. ", "original_text": "## 2. "}, "hash": "f474f1dac39277cbae31761e6a2681f448dfe421eae8e610002715c5d8ac7a10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2b7f598-00ce-4682-9b15-fb1840a417f9", "node_type": "1", "metadata": {"window": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n", "original_text": "In order to detect outliers in data, a forest of randomly built isolation trees is created. "}, "hash": "98e6f874e104624b6318b4916d732025a513f6904054ced07084f33219fa584f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. ", "mimetype": "text/plain", "start_char_idx": 10299, "end_char_idx": 10402, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c2b7f598-00ce-4682-9b15-fb1840a417f9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n", "original_text": "In order to detect outliers in data, a forest of randomly built isolation trees is created. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9332d5e-ed37-45ba-9587-6f832e2c1cfc", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In Section 2 we recall the Isolation Forest algorithm.  The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n", "original_text": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled. "}, "hash": "ca073c19942d12a357b33a05d54c0498b8707e6db5c771e261e044055690a10f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71f73aea-9319-491d-be62-76f49f930838", "node_type": "1", "metadata": {"window": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. ", "original_text": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. "}, "hash": "c94ad78493c92e31712421a1e7b7c2b39a2364673af17bc051f8b2860d271648", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to detect outliers in data, a forest of randomly built isolation trees is created. ", "mimetype": "text/plain", "start_char_idx": 10402, "end_char_idx": 10494, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71f73aea-9319-491d-be62-76f49f930838", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. ", "original_text": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2b7f598-00ce-4682-9b15-fb1840a417f9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The details of building an empirical probability density function as well as computational complexity analysis are presented in Section 3.  Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n", "original_text": "In order to detect outliers in data, a forest of randomly built isolation trees is created. "}, "hash": "3cb69d9a2985509ea88606185c5883f49abb385da452dce77f3091fb83ef6336", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32759ff1-51bd-459a-bbcb-ad014954ed62", "node_type": "1", "metadata": {"window": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. ", "original_text": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. "}, "hash": "373e4a1d1b8e0e89204ff0ccd5254e73d633a1e2bbbaf2b3c0ba810ff0acd9ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. ", "mimetype": "text/plain", "start_char_idx": 10494, "end_char_idx": 10599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32759ff1-51bd-459a-bbcb-ad014954ed62", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. ", "original_text": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71f73aea-9319-491d-be62-76f49f930838", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Section 4 is devoted to the presentation of the results of numerical experiments and the discussion.  Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. ", "original_text": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset. "}, "hash": "b498a07ff7a2774c5157ecf21a7197247139c431f5bf0f54f2ee9d8a16824496", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33b30b3b-6d35-4996-873e-a83a1c86bf98", "node_type": "1", "metadata": {"window": "## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n", "original_text": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. "}, "hash": "134a3a21af4b29f529cc36dc575eff6160136344c3b7ff39db9546cd4529f17f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. ", "mimetype": "text/plain", "start_char_idx": 10599, "end_char_idx": 10759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33b30b3b-6d35-4996-873e-a83a1c86bf98", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n", "original_text": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32759ff1-51bd-459a-bbcb-ad014954ed62", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Finally, conclusions and possible directions of future work are presented in Section 5.\n\n ## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. ", "original_text": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached. "}, "hash": "f13bf96674055f923a108bc6918f372b52ee778f48686fc783808524d8b39eae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91d6d357-e1f7-49fb-b0ea-5f95aeeb91ef", "node_type": "1", "metadata": {"window": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. ", "original_text": "The measure is obtained with the formula (1) [24].\n\n"}, "hash": "6e557fd4db6d425175bbc93719fe781f31404238fc84b8c478dc961149461ecf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. ", "mimetype": "text/plain", "start_char_idx": 10759, "end_char_idx": 10885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91d6d357-e1f7-49fb-b0ea-5f95aeeb91ef", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. ", "original_text": "The measure is obtained with the formula (1) [24].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33b30b3b-6d35-4996-873e-a83a1c86bf98", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "## 2.  Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n", "original_text": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point. "}, "hash": "981659143cb584e1c63d985eaf0a0c781e2b4c9d78bf41d05be547e438ac0bb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24398987-6668-4b6d-ac75-88ded682f416", "node_type": "1", "metadata": {"window": "In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2. ", "original_text": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n"}, "hash": "15e24e5cdceac6ebe1709a2586726ac0f534e8abc63136526044cae8dc495124", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The measure is obtained with the formula (1) [24].\n\n", "mimetype": "text/plain", "start_char_idx": 10885, "end_char_idx": 10937, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24398987-6668-4b6d-ac75-88ded682f416", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2. ", "original_text": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91d6d357-e1f7-49fb-b0ea-5f95aeeb91ef", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Isolation Forest\n\nIn this section, the main principles of the Isolation Forest algorithm are recalled.  In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. ", "original_text": "The measure is obtained with the formula (1) [24].\n\n"}, "hash": "ff5e5298ccfd350df0b2245365b7e0947829cb9648331464bd6e9b545222c442", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c796c20-c734-4cb1-85ff-213b3eb59b93", "node_type": "1", "metadata": {"window": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3. ", "original_text": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. "}, "hash": "007aec66a4ec6023137f2f794cb574494b96cc8a60d3960ba1ecb7a104ebb4ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n", "mimetype": "text/plain", "start_char_idx": 10937, "end_char_idx": 11323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c796c20-c734-4cb1-85ff-213b3eb59b93", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3. ", "original_text": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24398987-6668-4b6d-ac75-88ded682f416", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In order to detect outliers in data, a forest of randomly built isolation trees is created.  Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2. ", "original_text": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n"}, "hash": "b9b04021d16e7498242dfaab2b9b1fd6d94c96b1aad1a8400c6ab1daf7d6122f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fac0e4c-b78d-4812-a402-57a1ea3ec5e6", "node_type": "1", "metadata": {"window": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. ", "original_text": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. "}, "hash": "3f81183d03a992738343e8970376f11a9efe690a740e2732886f9558544d4562", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. ", "mimetype": "text/plain", "start_char_idx": 11323, "end_char_idx": 11514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2fac0e4c-b78d-4812-a402-57a1ea3ec5e6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. ", "original_text": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c796c20-c734-4cb1-85ff-213b3eb59b93", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each tree is grown on the basis of independently taken sample which is a subset of the analyzed dataset.  The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3. ", "original_text": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper. "}, "hash": "5822225aaafd8e549966c2a360313a5aed6d28f90c2c5029f8b9502561271de2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a40e8c16-40e0-4d45-a8cd-4df63c8fc8da", "node_type": "1", "metadata": {"window": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8. ", "original_text": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n"}, "hash": "ae6ff9aa4d3a050796d4f87e2b0b6b81149b42dd5bd7d92c0ffcb5c9de00367a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. ", "mimetype": "text/plain", "start_char_idx": 11514, "end_char_idx": 11628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a40e8c16-40e0-4d45-a8cd-4df63c8fc8da", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8. ", "original_text": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fac0e4c-b78d-4812-a402-57a1ea3ec5e6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The trees are called isolating as they recursively split the analyzed space until the separate points are isolated in its leaves or the depth limit is reached.  Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. ", "original_text": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper. "}, "hash": "719f4019ec5f5f20776e8f34dca8b3176cc0bc31df2a5965b8e64889bb1c635a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b33576f-e692-486f-938b-1047cd65d80c", "node_type": "1", "metadata": {"window": "The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9. ", "original_text": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. "}, "hash": "041a227c2d748671d4943d17737c53f625d24489fe051b322d85d88c00c405a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n", "mimetype": "text/plain", "start_char_idx": 11628, "end_char_idx": 11702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b33576f-e692-486f-938b-1047cd65d80c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9. ", "original_text": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a40e8c16-40e0-4d45-a8cd-4df63c8fc8da", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Outlier detection is performed by passing the data to the grown forest and producing an anomaly measure for every data point.  The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8. ", "original_text": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n"}, "hash": "6828154aab0368faf76a1b0ada31850520e191c74ffdaee8b69998b102d9c958", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2668736d-9636-4637-841b-644792a68275", "node_type": "1", "metadata": {"window": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. ", "original_text": "**if** e \u2265 l or |X| \u2264 1 **then**\n2. "}, "hash": "4c38aec990dcf4c1b0a164a8f5af775f7a9e1f449ac7b5c6986db88d703eb0cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. ", "mimetype": "text/plain", "start_char_idx": 11702, "end_char_idx": 11826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2668736d-9636-4637-841b-644792a68275", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. ", "original_text": "**if** e \u2265 l or |X| \u2264 1 **then**\n2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b33576f-e692-486f-938b-1047cd65d80c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The measure is obtained with the formula (1) [24].\n\n s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9. ", "original_text": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1. "}, "hash": "a613e4760a5cb9a555566e7fb88f777627459059d4247296ba771ba6c859c2d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a8942b0-2161-4d25-86fd-93d4ccdddafa", "node_type": "1", "metadata": {"window": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11. ", "original_text": "**return** exNode{Size \u2190 |X|}\n3. "}, "hash": "dbe6bc137f87a46744b066efe2af834c8c4ac22316fee4467364be603e71265e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**if** e \u2265 l or |X| \u2264 1 **then**\n2. ", "mimetype": "text/plain", "start_char_idx": 11826, "end_char_idx": 11862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a8942b0-2161-4d25-86fd-93d4ccdddafa", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11. ", "original_text": "**return** exNode{Size \u2190 |X|}\n3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2668736d-9636-4637-841b-644792a68275", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "s(x, n) = 2<sup>-E(h(x))/C(n)</sup> (1)\n\nwhere\n\nC(n) = 2H(n - 1) - 2(n-1)/n (2)\n\nH(n) = ln(n) + 0.57772156649 (3)\n\nIn the formulae (1), (2) and (3) the E(x) term stands for the average lengths of all the paths that have to be gone through in the isolating trees in order to isolate the point x. C(n) is theoretical average path length of unsuccessful search in the binary search tree.\n\n The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. ", "original_text": "**if** e \u2265 l or |X| \u2264 1 **then**\n2. "}, "hash": "887cf2fa47229119247af5bc03299bf62cb8efd98d687311a88e67580c04055a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6661ca5-c885-47b0-abfc-8a4d36d32bc1", "node_type": "1", "metadata": {"window": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12. ", "original_text": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. "}, "hash": "e80de28b4c7f52ce0a8c94948ab40d9665dca155568a7726ff0d23d67f4157ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**return** exNode{Size \u2190 |X|}\n3. ", "mimetype": "text/plain", "start_char_idx": 11862, "end_char_idx": 11895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6661ca5-c885-47b0-abfc-8a4d36d32bc1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12. ", "original_text": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a8942b0-2161-4d25-86fd-93d4ccdddafa", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The original algorithm of Isolation Forest building as well as the method of computing the anomaly score stays unchanged, so a particularly interested reader can refer to the original paper.  However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11. ", "original_text": "**return** exNode{Size \u2190 |X|}\n3. "}, "hash": "edd0b8141cfd895e4a11f3482e58599336d5861e64ab3081789ce73b90c75330", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7540bdf8-3cf5-443b-90e3-8cd8aeb0232f", "node_type": "1", "metadata": {"window": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3. ", "original_text": "Xl \u2190 filter(Xq \u2264 p)\n8. "}, "hash": "fc26b8f1f2e7392c543529a4e8181a4e8bf67562defa56a4fa3e7db1db25e444", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. ", "mimetype": "text/plain", "start_char_idx": 11895, "end_char_idx": 12037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7540bdf8-3cf5-443b-90e3-8cd8aeb0232f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3. ", "original_text": "Xl \u2190 filter(Xq \u2264 p)\n8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6661ca5-c885-47b0-abfc-8a4d36d32bc1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, it is the algorithm of building an isolation tree, what is the object of extension in the present paper.  Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12. ", "original_text": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7. "}, "hash": "03115f52361bf63f8f2ff19436f02fcf9769a27e8ccddcbfd75298904ae58797", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32fbab5e-4838-4313-a8f7-c7d2565d6e75", "node_type": "1", "metadata": {"window": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1. ", "original_text": "Xr \u2190 filter(Xq > p)\n9. "}, "hash": "9c9fa5091af7fb89e94bfd5ff3e496ee09ac0491f02e7778b4b7580cf2fa91dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Xl \u2190 filter(Xq \u2264 p)\n8. ", "mimetype": "text/plain", "start_char_idx": 12037, "end_char_idx": 12060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32fbab5e-4838-4313-a8f7-c7d2565d6e75", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1. ", "original_text": "Xr \u2190 filter(Xq > p)\n9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7540bdf8-3cf5-443b-90e3-8cd8aeb0232f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, we recall it in the form of pseudocode, see Algorithm 1 [24].\n\n **Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3. ", "original_text": "Xl \u2190 filter(Xq \u2264 p)\n8. "}, "hash": "0281cdaf0bbf06956f7a686b7d76acb2f34694032af260edcdfe2252ea83238b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "003e11fa-69d6-4f45-bc79-ca2bcaacb252", "node_type": "1", "metadata": {"window": "**if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. ", "original_text": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. "}, "hash": "302a98807e8997731d1744cc6c99127580bfa95783d1c9a94a5024c7ee0700c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Xr \u2190 filter(Xq > p)\n9. ", "mimetype": "text/plain", "start_char_idx": 12060, "end_char_idx": 12083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "003e11fa-69d6-4f45-bc79-ca2bcaacb252", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. ", "original_text": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32fbab5e-4838-4313-a8f7-c7d2565d6e75", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Algorithm 1: iTree(X, e, l)**\n\n**Inputs:** X \u2013 input data, e \u2013 current tree height, l \u2013 height limit\n**Output:** iTree\n1.  **if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1. ", "original_text": "Xr \u2190 filter(Xq > p)\n9. "}, "hash": "9f9b57146e334f5794d7bcef824d55cffe99837aa9a4739f5c394c4f0cbbcf33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2894ca58-39b9-46e7-b56b-8e5e35c89e20", "node_type": "1", "metadata": {"window": "**return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. ", "original_text": "Right \u2190 iTree(Xr, e + 1, l),\n11. "}, "hash": "a932bcfe1e3c34dbf0fbd98a43410bf7c3328776b2844129f0b022f0c5a47ed7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. ", "mimetype": "text/plain", "start_char_idx": 12083, "end_char_idx": 12133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2894ca58-39b9-46e7-b56b-8e5e35c89e20", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. ", "original_text": "Right \u2190 iTree(Xr, e + 1, l),\n11. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "003e11fa-69d6-4f45-bc79-ca2bcaacb252", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**if** e \u2265 l or |X| \u2264 1 **then**\n2.  **return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. ", "original_text": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10. "}, "hash": "4b6ff35d018f6c38475950af9c059c044cdf000471759375ac6d01bbbbb5e410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e322ba9-796e-44c5-a69a-c2c36f83276f", "node_type": "1", "metadata": {"window": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. ", "original_text": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12. "}, "hash": "0775daec426856c3bfdd2f463a79c76e5498ca753f42e4265d7cf597bee5b1b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Right \u2190 iTree(Xr, e + 1, l),\n11. ", "mimetype": "text/plain", "start_char_idx": 12133, "end_char_idx": 12166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e322ba9-796e-44c5-a69a-c2c36f83276f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. ", "original_text": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2894ca58-39b9-46e7-b56b-8e5e35c89e20", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**return** exNode{Size \u2190 |X|}\n3.  **else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. ", "original_text": "Right \u2190 iTree(Xr, e + 1, l),\n11. "}, "hash": "385ef6ca4d4b1a844f4428164fc9cbb49716bcdcb97ea43a62eac017eb8842c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bef07f8-4ac2-4af8-855f-14b65668b008", "node_type": "1", "metadata": {"window": "Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. ", "original_text": "**end if**\n\n## 3. "}, "hash": "0ee07dae52d8c9d04a650d88a7dd4f646489fe915db7bd346c8416090a508d95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12. ", "mimetype": "text/plain", "start_char_idx": 12166, "end_char_idx": 12200, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3bef07f8-4ac2-4af8-855f-14b65668b008", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. ", "original_text": "**end if**\n\n## 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e322ba9-796e-44c5-a69a-c2c36f83276f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**else**\n4. let Q be a list of attributes in X\n5. randomly select an attribute q \u2208 Q\n6. p \u2190 uniformRandomReal(from \u2190 min(Xq), to \u2190 max(Xq)\n7.  Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. ", "original_text": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12. "}, "hash": "e75201ff5fc8c120ee6ec30f7031559057f75000b82817a4751779fbed5860b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15567119-2146-40e7-a8c9-cfa85123cd42", "node_type": "1", "metadata": {"window": "Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n", "original_text": "Proposed generalization\n\n### 3.1. "}, "hash": "bfe66c043d9ddeac76159eb548ffe8768b567d39709ed9f1f36f87a2b34d64b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**end if**\n\n## 3. ", "mimetype": "text/plain", "start_char_idx": 12200, "end_char_idx": 12218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "15567119-2146-40e7-a8c9-cfa85123cd42", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n", "original_text": "Proposed generalization\n\n### 3.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bef07f8-4ac2-4af8-855f-14b65668b008", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Xl \u2190 filter(Xq \u2264 p)\n8.  Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. ", "original_text": "**end if**\n\n## 3. "}, "hash": "fac3722497a3bf8253111c16fae9ba6dbb7084d09af10206f784df9e47701059", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "372307d2-3949-4b65-8d96-9ad37f649a96", "node_type": "1", "metadata": {"window": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig. ", "original_text": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. "}, "hash": "63ae3e445d28126f861cdf53cadb42171ddc184dc7545361a1af74f4fc69fa80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Proposed generalization\n\n### 3.1. ", "mimetype": "text/plain", "start_char_idx": 12218, "end_char_idx": 12252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "372307d2-3949-4b65-8d96-9ad37f649a96", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig. ", "original_text": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15567119-2146-40e7-a8c9-cfa85123cd42", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Xr \u2190 filter(Xq > p)\n9.  **return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n", "original_text": "Proposed generalization\n\n### 3.1. "}, "hash": "ad2f8d63d991cfe835d5af624e733a72a6c33faa82b42a319abd25feaba407ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "422ba1ec-5c03-48ed-8d4e-a8911605e3de", "node_type": "1", "metadata": {"window": "Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. ", "original_text": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. "}, "hash": "3ed9d845665d352811170c937660630d851602f0b64aba1b520bcf967bb90278", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. ", "mimetype": "text/plain", "start_char_idx": 12252, "end_char_idx": 12460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "422ba1ec-5c03-48ed-8d4e-a8911605e3de", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. ", "original_text": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "372307d2-3949-4b65-8d96-9ad37f649a96", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**return** inNode{Left \u2190 iTree(Xl, e + 1, l),\n10.  Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig. ", "original_text": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6. "}, "hash": "f9c09ae99dba1222c87c28c135e5167350cce8039c196ebde283fb6a1fd51d2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e327b8ee-12f1-4a17-80be-8b543b66e962", "node_type": "1", "metadata": {"window": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. ", "original_text": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. "}, "hash": "3b3f69faa12690d39113b7d43b63bd2ade929e802e055f22eb391edf2c649f07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. ", "mimetype": "text/plain", "start_char_idx": 12460, "end_char_idx": 12695, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e327b8ee-12f1-4a17-80be-8b543b66e962", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. ", "original_text": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "422ba1ec-5c03-48ed-8d4e-a8911605e3de", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Right \u2190 iTree(Xr, e + 1, l),\n11.  SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. ", "original_text": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space. "}, "hash": "2e637edcd35bfef62b8cf4725a6cf90f79ee62d48dddd90d483c30e86f0cf38c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23f01a6a-0d0b-4d81-8116-649762f5527b", "node_type": "1", "metadata": {"window": "**end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. ", "original_text": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. "}, "hash": "cb50ea99bd4a2d8d4e4004a21ce4bc52ef97b14ebc2d27c03944010dddeb192e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. ", "mimetype": "text/plain", "start_char_idx": 12695, "end_char_idx": 12789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23f01a6a-0d0b-4d81-8116-649762f5527b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. ", "original_text": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e327b8ee-12f1-4a17-80be-8b543b66e962", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "SplitAtt \u2190 q, SplitValue \u2190 p}\n12.  **end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. ", "original_text": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density. "}, "hash": "6f2104c93dde8a0eab6037556ee4e2134c0d3b33a9fa01b990f59b4432fe918e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77db1d7d-a073-479d-b35c-f69081c556bb", "node_type": "1", "metadata": {"window": "Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. ", "original_text": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n"}, "hash": "cc5b4cdfeffd00c4d37f4ed8a0c172553738127eee15f6cc4934084ad6dbb3c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. ", "mimetype": "text/plain", "start_char_idx": 12789, "end_char_idx": 12951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77db1d7d-a073-479d-b35c-f69081c556bb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. ", "original_text": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23f01a6a-0d0b-4d81-8116-649762f5527b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**end if**\n\n## 3.  Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. ", "original_text": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region. "}, "hash": "c588abafb33d2b81beefd30e0aa2461fe464358c08f3479a807fcb54995d1748", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b555722-6774-4678-8f5d-ed43a2b30a12", "node_type": "1", "metadata": {"window": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. ", "original_text": "Fig. "}, "hash": "f3fd5226134db7b2e399e284d3222141777c1d8de09e821f80a532208589b14a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n", "mimetype": "text/plain", "start_char_idx": 12951, "end_char_idx": 13092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b555722-6774-4678-8f5d-ed43a2b30a12", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77db1d7d-a073-479d-b35c-f69081c556bb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Proposed generalization\n\n### 3.1.  General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. ", "original_text": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n"}, "hash": "91aec5b6ef35b704ac93978340fbc7cdb2079b49893d43182d2335679a070ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04496436-51c5-41fa-a556-1d1e0217cdc7", "node_type": "1", "metadata": {"window": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n", "original_text": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. "}, "hash": "9db0df8cd2fa37f2a8814f6bf66b4b80226387c960510cfc1a24d000364696c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 13092, "end_char_idx": 13097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04496436-51c5-41fa-a556-1d1e0217cdc7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n", "original_text": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b555722-6774-4678-8f5d-ed43a2b30a12", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "General idea of probabilistic generalization\n\nThe algorithm (see, Algorithm 1) of building the isolation tree employs uniform distribution for generating a split point p in every node as shown in the line 6.  We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. ", "original_text": "Fig. "}, "hash": "e5c1344d8b4bfa3dcbfaaaafaab2d132fc050a3533ed4e56125c9ea0a97d0097", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c2dfeca-a631-451d-a764-75a0b507824b", "node_type": "1", "metadata": {"window": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig. ", "original_text": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. "}, "hash": "39b49cf96361333bbfcc76ffe2097c1fbcbda24247ed2f806cdcb04d967420c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. ", "mimetype": "text/plain", "start_char_idx": 13097, "end_char_idx": 13224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c2dfeca-a631-451d-a764-75a0b507824b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig. ", "original_text": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04496436-51c5-41fa-a556-1d1e0217cdc7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We are interested in ensuring the data splitting in more meaningful way by assigning lower probability density to the densely populated regions, i.e., clusters understood as the groups of records or points in a multidimensional space.  Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n", "original_text": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree. "}, "hash": "8065a3641e38f9f4eec844da6c989f9d178a9fa7b9b33ff01ed851915be58612", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbcb0510-5be2-4e66-bb22-f2029d4abb2a", "node_type": "1", "metadata": {"window": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function. ", "original_text": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. "}, "hash": "9ca561232106be70b17d44a1843bd0ab7ac8204a1389d6617166a8b6838f39e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. ", "mimetype": "text/plain", "start_char_idx": 13224, "end_char_idx": 13351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbcb0510-5be2-4e66-bb22-f2029d4abb2a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function. ", "original_text": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c2dfeca-a631-451d-a764-75a0b507824b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Out-of-cluster regions, on the contrary, have to be assigned with higher probability density.  Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig. ", "original_text": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively. "}, "hash": "7c6328b3ca25b849892c3c867a9f0230a513e9f2331f9e8a12312f1590afec8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2593738b-ecbc-4813-9448-a5ffa5623892", "node_type": "1", "metadata": {"window": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths. ", "original_text": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. "}, "hash": "2088cb116e5d90169122d46fcbd2a6960427e31de0501019606b236d034cc690", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. ", "mimetype": "text/plain", "start_char_idx": 13351, "end_char_idx": 13456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2593738b-ecbc-4813-9448-a5ffa5623892", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths. ", "original_text": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbcb0510-5be2-4e66-bb22-f2029d4abb2a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such approach is justified by an intuitively appealing statement that an outlier is a point that is separated from the majority of the data by some empty region.  Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function. ", "original_text": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis. "}, "hash": "e07422f14708e9b552d2d3b7e4874bbc45ab60614ffba57de132f7a941fb8d60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a95fca6-2146-4fbb-a03a-73e128ddb2ac", "node_type": "1", "metadata": {"window": "Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. ", "original_text": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. "}, "hash": "5e426e768908e32db35493897d60f9d993a57b3d3680cd03ebc4749fca62fa92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. ", "mimetype": "text/plain", "start_char_idx": 13456, "end_char_idx": 13606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1a95fca6-2146-4fbb-a03a-73e128ddb2ac", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. ", "original_text": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2593738b-ecbc-4813-9448-a5ffa5623892", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Obviously, one may assume that the same mechanism is also applicable on very lower levels of splits when only a few records are considered.\n\n Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths. ", "original_text": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data. "}, "hash": "381e6728a450f85b8fc4224599fa9dccb16d8620a11c6af2a411825dc142bd90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbfd3198-7e85-497c-97a1-6086a45ce322", "node_type": "1", "metadata": {"window": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5. ", "original_text": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n"}, "hash": "8a37afa6ded1dc4943637e85c4c0d4c39c8ac58acb0db0c4af681487c351ba0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. ", "mimetype": "text/plain", "start_char_idx": 13606, "end_char_idx": 13770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbfd3198-7e85-497c-97a1-6086a45ce322", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5. ", "original_text": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a95fca6-2146-4fbb-a03a-73e128ddb2ac", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. ", "original_text": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8. "}, "hash": "a7d4b92000b35e06a6cf59dbe68430569e0df4b3c9d980ff6c6fef3e6f6ae801", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5aff8e3-fe3b-41d4-92cd-ba6aed4981de", "node_type": "1", "metadata": {"window": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e. ", "original_text": "Fig. "}, "hash": "633ba42598f07956dd5e96fe1da58e5c04046434364aa1b6f7e6bf51def57750", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n", "mimetype": "text/plain", "start_char_idx": 13770, "end_char_idx": 13894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5aff8e3-fe3b-41d4-92cd-ba6aed4981de", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbfd3198-7e85-497c-97a1-6086a45ce322", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1 illustrates the approach, the part (A) presents a 256-point artificial training set sampled for building the isolation tree.  Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5. ", "original_text": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n"}, "hash": "c1e438c76785bdcb3b1dd8d83db8daa60e4ca92ad41c11f6c7a59ed35501ffb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "011600c8-e729-4750-887f-4de6bac8da53", "node_type": "1", "metadata": {"window": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n", "original_text": "1 (C) shows the probability density distribution built as piecewise defined function. "}, "hash": "5e54f482397da582ef988962118eadd265053296e47b48d15700651e4249af9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 13894, "end_char_idx": 13899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "011600c8-e729-4750-887f-4de6bac8da53", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n", "original_text": "1 (C) shows the probability density distribution built as piecewise defined function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5aff8e3-fe3b-41d4-92cd-ba6aed4981de", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Parts (B) and (C) present original approach (with uniform distribution), and the idea of the modified algorithm, respectively.  The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e. ", "original_text": "Fig. "}, "hash": "93ed0709262a93d67eb570c794b63a87106531f4605423208a60ac2dcd2e29d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cfdcfdf-e447-4941-8da2-751f78c5ff55", "node_type": "1", "metadata": {"window": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. ", "original_text": "The value of the function on segments is proportional to their lengths. "}, "hash": "9c116d17dee058bb89617589ef0ba6d791042c184b237c3e0411cffe11c6fef9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 (C) shows the probability density distribution built as piecewise defined function. ", "mimetype": "text/plain", "start_char_idx": 13899, "end_char_idx": 13985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1cfdcfdf-e447-4941-8da2-751f78c5ff55", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. ", "original_text": "The value of the function on segments is proportional to their lengths. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "011600c8-e729-4750-887f-4de6bac8da53", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The black dots at figures (B) and (C) show the distribution of the sample data along the Feature 1 axis.  Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n", "original_text": "1 (C) shows the probability density distribution built as piecewise defined function. "}, "hash": "f417de9f61a3c41fbd7c5e158c64403fb8cb25f172f9c0b94900aa36ff6f1ac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0179d15-c9a5-4e70-9ebc-9b7a164ed7be", "node_type": "1", "metadata": {"window": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n", "original_text": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. "}, "hash": "40cb3965ce3d7002bf686a487fff99e95105ddb96a1dbebd1369de8584386d6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The value of the function on segments is proportional to their lengths. ", "mimetype": "text/plain", "start_char_idx": 13985, "end_char_idx": 14057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a0179d15-c9a5-4e70-9ebc-9b7a164ed7be", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n", "original_text": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cfdcfdf-e447-4941-8da2-751f78c5ff55", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Note that the Feature 1 corresponds to the x-axis while Feature 2 represents the y-axis values, i.e., the first and the second dimension of the data.  Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. ", "original_text": "The value of the function on segments is proportional to their lengths. "}, "hash": "6bee0d4afbc441693239ceff5ec6d0337c56045db7c8e05f5fb240297d4b9948", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5c610b2-e440-4f15-8588-23cd6770b086", "node_type": "1", "metadata": {"window": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure. ", "original_text": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5. "}, "hash": "ce607063eae93223ceb0b35f9a926355fc4ed863239c2193dcf1c2b22b22d8ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. ", "mimetype": "text/plain", "start_char_idx": 14057, "end_char_idx": 14265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f5c610b2-e440-4f15-8588-23cd6770b086", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure. ", "original_text": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0179d15-c9a5-4e70-9ebc-9b7a164ed7be", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Casting the data onto the dimension of the Feature 1, we can identify a gap between the clusters at approximately 0.5 and a clearly distinguishable outlier at 0.8.  In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n", "original_text": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8. "}, "hash": "a55c9d5ede49443e73480d4e256623f40b5820b756f22644358fb921de0f623d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "caad1a37-7934-422d-8c1a-cf442e91a012", "node_type": "1", "metadata": {"window": "Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs. ", "original_text": "Even the longest segment alone, i.e. "}, "hash": "6ee634b0e61e5977d9dbfeb6523d0c077d958cf89ad85cfaceae173fb2378669", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5. ", "mimetype": "text/plain", "start_char_idx": 14265, "end_char_idx": 14362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "caad1a37-7934-422d-8c1a-cf442e91a012", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs. ", "original_text": "Even the longest segment alone, i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5c610b2-e440-4f15-8588-23cd6770b086", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of uniform distribution, shown at the left figure, it is equally probable that a split is performed at any region.\n\n Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure. ", "original_text": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5. "}, "hash": "4b235736065ff9d131bf792cdb5ea022f8082520a7d26c500d136e81eab5efb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c45ed7d-a842-4301-aa35-81b7d56c4a27", "node_type": "1", "metadata": {"window": "1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers. ", "original_text": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n"}, "hash": "99335e9839a30da0ef3cbf0f1147ed69cbe8f2fc90ce0ba3b9a7e3227d58bbf4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Even the longest segment alone, i.e. ", "mimetype": "text/plain", "start_char_idx": 14362, "end_char_idx": 14399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c45ed7d-a842-4301-aa35-81b7d56c4a27", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers. ", "original_text": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "caad1a37-7934-422d-8c1a-cf442e91a012", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs. ", "original_text": "Even the longest segment alone, i.e. "}, "hash": "3727060dcf3998b57e50f667dc962bec414fe2fe3043ea783942e227d3cbc783", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0de17fe0-3c61-4726-8ea7-3b46a6701a99", "node_type": "1", "metadata": {"window": "The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. ", "original_text": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. "}, "hash": "5b69d476d60ed39e5ba0f94d9dfa848bda28ca1dc629cdc5da50d688460d09ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n", "mimetype": "text/plain", "start_char_idx": 14399, "end_char_idx": 14510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0de17fe0-3c61-4726-8ea7-3b46a6701a99", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. ", "original_text": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c45ed7d-a842-4301-aa35-81b7d56c4a27", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1 (C) shows the probability density distribution built as piecewise defined function.  The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers. ", "original_text": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n"}, "hash": "702675f65967fc90c5f46aca6e263ace0ec464e4ef072fb71d9b97d7806bb9b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8b11a6f-ecc8-4a73-b791-e4a42d55f73b", "node_type": "1", "metadata": {"window": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n", "original_text": "Therefore, its anomaly score would be higher.\n\n"}, "hash": "f3456614c9f0927a886a4323321f378f5e5a0512fdd529bfef1e205921f64308", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. ", "mimetype": "text/plain", "start_char_idx": 14510, "end_char_idx": 14654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a8b11a6f-ecc8-4a73-b791-e4a42d55f73b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n", "original_text": "Therefore, its anomaly score would be higher.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0de17fe0-3c61-4726-8ea7-3b46a6701a99", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The value of the function on segments is proportional to their lengths.  Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. ", "original_text": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building. "}, "hash": "e94425bb208ec677b37f0dcc543c9a03d1abb5891d02a3da8a807cc9e4622ac6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80c40e74-a9eb-4d3e-ac0c-70577f374b83", "node_type": "1", "metadata": {"window": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig. ", "original_text": "---\n[Description of Figure 1: A three-panel figure. "}, "hash": "8fe487caada659dd775a74b997064e48561808ac660eb0ef8a6ca90b28213a6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, its anomaly score would be higher.\n\n", "mimetype": "text/plain", "start_char_idx": 14654, "end_char_idx": 14701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80c40e74-a9eb-4d3e-ac0c-70577f374b83", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig. ", "original_text": "---\n[Description of Figure 1: A three-panel figure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8b11a6f-ecc8-4a73-b791-e4a42d55f73b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Here, we can see that the probability density at the clusters being densely populated regions is notably lower compared to the gap in-between the clusters at 0.5 and between a cluster and the outlier at 0.8.  Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n", "original_text": "Therefore, its anomaly score would be higher.\n\n"}, "hash": "2d930730ec5b4ff0bfa9375f6c22f407402596a5f4c223b1fb2c81e7177a8a37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de6875da-9d5c-44c3-8d74-18d65c4e8ba7", "node_type": "1", "metadata": {"window": "Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1.", "original_text": "Panel (A) is a scatter plot labeled \"Feature 1\" vs. "}, "hash": "18f23bb0b10f09924ed63ca822da702088922fb1589932d054a4c064cdd6cde4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Description of Figure 1: A three-panel figure. ", "mimetype": "text/plain", "start_char_idx": 14701, "end_char_idx": 14753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de6875da-9d5c-44c3-8d74-18d65c4e8ba7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1.", "original_text": "Panel (A) is a scatter plot labeled \"Feature 1\" vs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80c40e74-a9eb-4d3e-ac0c-70577f374b83", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Integrated value of the density probability function for top-5 longest segments is equal to 0.5.  Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig. ", "original_text": "---\n[Description of Figure 1: A three-panel figure. "}, "hash": "1b2d06c5da04004b704ca34850dbd76feb0304ca8ed5b53151b694ef9087f6da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59b8ef76-6995-4d77-86b2-aff804924366", "node_type": "1", "metadata": {"window": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method. ", "original_text": "\"Feature 2\" showing several clusters of points and some outliers. "}, "hash": "609cc2f3317ff453324626ae59abe1cd53ae98247796eb311363f51cc608f72f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Panel (A) is a scatter plot labeled \"Feature 1\" vs. ", "mimetype": "text/plain", "start_char_idx": 14753, "end_char_idx": 14805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59b8ef76-6995-4d77-86b2-aff804924366", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method. ", "original_text": "\"Feature 2\" showing several clusters of points and some outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de6875da-9d5c-44c3-8d74-18d65c4e8ba7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Even the longest segment alone, i.e.  the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1.", "original_text": "Panel (A) is a scatter plot labeled \"Feature 1\" vs. "}, "hash": "dd3a02361e619ffc93a77a564e7ddac82867abd3a0a3cfc763f45b8d017c7cc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "152d9c84-a888-45b8-b09a-6b967d7cf371", "node_type": "1", "metadata": {"window": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n", "original_text": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. "}, "hash": "dfa2bd39d7a5e145ef65df5a07c58a32ceb5d90b12fde81d93707273db4024e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Feature 2\" showing several clusters of points and some outliers. ", "mimetype": "text/plain", "start_char_idx": 14805, "end_char_idx": 14871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "152d9c84-a888-45b8-b09a-6b967d7cf371", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n", "original_text": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59b8ef76-6995-4d77-86b2-aff804924366", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "the gap separating the outlier at 0.8 from the rest of the data obtains 0.25 of the total probability weight.\n\n Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method. ", "original_text": "\"Feature 2\" showing several clusters of points and some outliers. "}, "hash": "fae22c2790ac57a94834336d937a5aaf071f0732805f2c07b0db0ecd5031b9eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "623f9303-3172-497a-9099-69b7375a4a7a", "node_type": "1", "metadata": {"window": "Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2. ", "original_text": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n"}, "hash": "158f1ee5260213d967bbfa8b907d134c8408e9782d488d104fe8e37a7365b0a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. ", "mimetype": "text/plain", "start_char_idx": 14871, "end_char_idx": 14988, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "623f9303-3172-497a-9099-69b7375a4a7a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2. ", "original_text": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "152d9c84-a888-45b8-b09a-6b967d7cf371", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Thanks to the higher probability density it is more likely that an outlier is isolated at the earlier stages of the process of a tree building.  Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n", "original_text": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely. "}, "hash": "08addc14ed0c5d1d158c9b0e9e29fe313f9e9251d911b887f75d91305cebb5cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c282620-85b7-4c39-b35c-a9919bdef4d3", "node_type": "1", "metadata": {"window": "---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. ", "original_text": "**Fig. "}, "hash": "80b9e15c6838311b47e25dd1190b089b5c93f4f8d7ebb6a175996b12fea15fb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n", "mimetype": "text/plain", "start_char_idx": 14988, "end_char_idx": 15167, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c282620-85b7-4c39-b35c-a9919bdef4d3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "623f9303-3172-497a-9099-69b7375a4a7a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, its anomaly score would be higher.\n\n ---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2. ", "original_text": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n"}, "hash": "c537fee8861c1695fb3231d03f382c2b5fa49bec548825c37058a7cc91efd859", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40f02b70-ae47-4979-b00a-60c435b68956", "node_type": "1", "metadata": {"window": "Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. ", "original_text": "1."}, "hash": "fac4fdd5282cd8924f762e696cdecd489126e086b8a720c6b05aae9d94fa8b30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 15167, "end_char_idx": 15174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40f02b70-ae47-4979-b00a-60c435b68956", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. ", "original_text": "1."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c282620-85b7-4c39-b35c-a9919bdef4d3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 1: A three-panel figure.  Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. ", "original_text": "**Fig. "}, "hash": "68e0cbd21e668a8c10959758c131e61f26ef8ca481e451f30cf78598a628496f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cad26cf-69d6-446c-809b-3d755d5c9039", "node_type": "1", "metadata": {"window": "\"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n", "original_text": "** Illustration of the proposed method. "}, "hash": "de000d529ae7480a51a8793324a3467ccd29d7b35026df5b9154aac8f3305cd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.", "mimetype": "text/plain", "start_char_idx": 15174, "end_char_idx": 15176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cad26cf-69d6-446c-809b-3d755d5c9039", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n", "original_text": "** Illustration of the proposed method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40f02b70-ae47-4979-b00a-60c435b68956", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Panel (A) is a scatter plot labeled \"Feature 1\" vs.  \"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. ", "original_text": "1."}, "hash": "126b253faa847eca0aa6b07f30f3d77751d1b2d26ef84ca54de67565f97b922d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b5a5da6-8ab7-4cfc-944e-afddd71fee1f", "node_type": "1", "metadata": {"window": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n", "original_text": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n"}, "hash": "7698b5543a0a9696de39e60434f1b13ac7691e726d95b621c30fd3ca9b683f1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Illustration of the proposed method. ", "mimetype": "text/plain", "start_char_idx": 15176, "end_char_idx": 15216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b5a5da6-8ab7-4cfc-944e-afddd71fee1f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n", "original_text": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cad26cf-69d6-446c-809b-3d755d5c9039", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\"Feature 2\" showing several clusters of points and some outliers.  Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n", "original_text": "** Illustration of the proposed method. "}, "hash": "d0f4c973bd0d6826c67097f90b224e7b17848774223688290b0cdf726af5c160", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bec7370-8c3b-4583-ba5d-19018b9aa19f", "node_type": "1", "metadata": {"window": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. ", "original_text": "---\n\n### 3.2. "}, "hash": "61c97dd0c85fa3990703dd9abe6c9b764204af6fdabfb9b80c1e1eceaaa82ad8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n", "mimetype": "text/plain", "start_char_idx": 15216, "end_char_idx": 15337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1bec7370-8c3b-4583-ba5d-19018b9aa19f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. ", "original_text": "---\n\n### 3.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b5a5da6-8ab7-4cfc-944e-afddd71fee1f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Panel (B) shows a uniform probability density plot for Feature 1, indicating that any split value is equally likely.  Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n", "original_text": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n"}, "hash": "85b64239113a34a089d2f67b1c959b726e2ba2f9850e622eb622969384ff98f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85339bbb-3a83-4ee7-a8b5-512001e0777b", "node_type": "1", "metadata": {"window": "**Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n", "original_text": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. "}, "hash": "fd6daf33ff0ce7495cdc13ad5434683674c6cc69d9c8c53d96104cea56135624", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n### 3.2. ", "mimetype": "text/plain", "start_char_idx": 15337, "end_char_idx": 15351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85339bbb-3a83-4ee7-a8b5-512001e0777b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n", "original_text": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bec7370-8c3b-4583-ba5d-19018b9aa19f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Panel (C) shows a modified probability density plot for Feature 1, with low density over the clusters and high density (peaks) in the gaps between clusters and around outliers.]\n\n **Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. ", "original_text": "---\n\n### 3.2. "}, "hash": "1d55889616179eef8ae6845b8a9acd63c804ea26ef5e34dbdf74c45c65ee70d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20329786-0cbf-4db2-bc66-e041e08cc267", "node_type": "1", "metadata": {"window": "1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. ", "original_text": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. "}, "hash": "bcc82fd9bc4d012d1bf3fa7d8fcad22979c392d7a6dcafffe65c4f9c215f2e3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. ", "mimetype": "text/plain", "start_char_idx": 15351, "end_char_idx": 15509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20329786-0cbf-4db2-bc66-e041e08cc267", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. ", "original_text": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85339bbb-3a83-4ee7-a8b5-512001e0777b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n", "original_text": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter. "}, "hash": "e0c2bdcb8176a27568e8329dec034b26d8ebb6ec7e2003df7336278681482c2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2afd4cc2-6a57-427b-98ff-bc22f6e80e11", "node_type": "1", "metadata": {"window": "** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n", "original_text": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n"}, "hash": "73f8b86707fa070bd0d41efd193f6931c1aa4c907bfb3e4558aee5b05e51ae49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. ", "mimetype": "text/plain", "start_char_idx": 15509, "end_char_idx": 15620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2afd4cc2-6a57-427b-98ff-bc22f6e80e11", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n", "original_text": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20329786-0cbf-4db2-bc66-e041e08cc267", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1. ** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. ", "original_text": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm. "}, "hash": "ecbcb12c86723bff67469edb3c42f27dd4a304254077b155d7910b49724b1ba6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a78deee-cced-46bb-ad7d-26db003bde31", "node_type": "1", "metadata": {"window": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n", "original_text": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n"}, "hash": "d3a1368f77f945a0dc5e6d1c161ad62feac1dcdeecc074fd7410e5693e5064fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n", "mimetype": "text/plain", "start_char_idx": 15620, "end_char_idx": 16005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a78deee-cced-46bb-ad7d-26db003bde31", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n", "original_text": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2afd4cc2-6a57-427b-98ff-bc22f6e80e11", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Illustration of the proposed method.  (A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n", "original_text": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n"}, "hash": "eadf9d3c5e1183b3b92021e35e6441add9e93b853541ab058be13cd3d3210d7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09314a2c-43b9-4208-82bf-c6d95f423b07", "node_type": "1", "metadata": {"window": "---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n", "original_text": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. "}, "hash": "07e0807b9f4e2550264681b5f2aead630b40c4b7da7ea2f7781eefa4711f3021", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n", "mimetype": "text/plain", "start_char_idx": 16005, "end_char_idx": 16191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09314a2c-43b9-4208-82bf-c6d95f423b07", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n", "original_text": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a78deee-cced-46bb-ad7d-26db003bde31", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(A) 256-point sample, (B) original approach (uniform distribution of split value generation), and (C) modified approach.\n ---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n", "original_text": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n"}, "hash": "fa23eba651fd6faf48195189fe325399b7ff6e4cac68cda3063949b851681197", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1980b9b-5f04-4f2f-85a4-f13d1a2bf655", "node_type": "1", "metadata": {"window": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n", "original_text": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n"}, "hash": "283cdcac055e607bad14fe2cdc6f91d18e22424371c2c5ebf9f57e106845dbc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. ", "mimetype": "text/plain", "start_char_idx": 16191, "end_char_idx": 16377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1980b9b-5f04-4f2f-85a4-f13d1a2bf655", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n", "original_text": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09314a2c-43b9-4208-82bf-c6d95f423b07", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n### 3.2.  Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n", "original_text": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}. "}, "hash": "c0cc82041aac7bebb185a5cdadc5f59df05667d88e61d264563350023502b079", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3922c01f-fcc9-49e4-935b-368745cf33b1", "node_type": "1", "metadata": {"window": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n", "original_text": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. "}, "hash": "b7916933d99f426c4129b16e7cfee380cfbb735ce19c494e6fffa2e9a97a6ae2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n", "mimetype": "text/plain", "start_char_idx": 16377, "end_char_idx": 16463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3922c01f-fcc9-49e4-935b-368745cf33b1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n", "original_text": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1980b9b-5f04-4f2f-85a4-f13d1a2bf655", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Piecewise defined cumulated probability function\n\nHere, let us describe the enhancement of Isolation Forest at the step of the choice of splitting parameter.  We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n", "original_text": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n"}, "hash": "6daf6f1e3e3f2f67b6c30d6255f38761924fd17e51e9111ab8d0e0c4cba7159b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "851b8e4e-3637-4237-a5d5-4ce2ebb75bd8", "node_type": "1", "metadata": {"window": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n", "original_text": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n"}, "hash": "a13333a80038f8ad46226417d09f5639e7b5e84d0d23126cdfb472bf9f63cf85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. ", "mimetype": "text/plain", "start_char_idx": 16463, "end_char_idx": 16674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "851b8e4e-3637-4237-a5d5-4ce2ebb75bd8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n", "original_text": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3922c01f-fcc9-49e4-935b-368745cf33b1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We start from defining a piecewise cumulated probability function for the original Isolation Forest algorithm.  Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n", "original_text": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements. "}, "hash": "b3b98b223135cf538ce301c8121a0e531dd4f0382f79111b2aab5330d2b95528", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2404bb55-6b67-4b16-97de-4905742e3d8b", "node_type": "1", "metadata": {"window": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3. ", "original_text": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n"}, "hash": "0b6d48d6c2b6e3ff7e88a734cc1842e5cf7cbb98c53fd911be06fd93d62973ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n", "mimetype": "text/plain", "start_char_idx": 16674, "end_char_idx": 16844, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2404bb55-6b67-4b16-97de-4905742e3d8b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3. ", "original_text": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "851b8e4e-3637-4237-a5d5-4ce2ebb75bd8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Let f(x) be a nonnegative function such that f(x) = 0, \u2200x \u2209 [a, b), {a, b} < R. Due to useful analytical properties, let it be expressed by scaled and shifted Kernel Density Function K(x) with limited support:\n\nf(x) = K( (x - (b+a)/2) / ((b-a)/2) ) (4)\n\nIt is worth noting that an unshifted and unscaled Kernel Density Function with limited support fulfills the requirements (5)\u2013(8).\n\n K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n", "original_text": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n"}, "hash": "9e81ef820c655f09abbffc4dc800d188281e20a0024e40d633c30f9912a93d05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cfa0077-f7a5-44f4-b1c9-e5a9fb42de46", "node_type": "1", "metadata": {"window": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n", "original_text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n"}, "hash": "05cd7a43e7576966d4b64085e8fcb0427f2e5e06a89fcac5e997b9bf4b11d9d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n", "mimetype": "text/plain", "start_char_idx": 16844, "end_char_idx": 17181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8cfa0077-f7a5-44f4-b1c9-e5a9fb42de46", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n", "original_text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2404bb55-6b67-4b16-97de-4905742e3d8b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "K(u) \u2265 0 \u2200u \u2208 R (5)\n\nK(u) = K(-u) (6)\n\n\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = 1 (7)\n\nK(u) = 0, \u2200u \u2209 (-1, 1) (8)\n\nSince the support is limited then (7) can be expressed by the formula (9).\n\n \u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3. ", "original_text": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n"}, "hash": "7951c7af39ea00611ef84eb83b8a49f77bf01c03fb1adce75dff83075a822738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc2b58c1-c056-4a8d-ac47-37d7d0b80065", "node_type": "1", "metadata": {"window": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. ", "original_text": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n"}, "hash": "b3c36732869c7d8816e2f9bb6812d1314c11f0deabad49bcd501246b98bffcb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n", "mimetype": "text/plain", "start_char_idx": 17181, "end_char_idx": 17430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc2b58c1-c056-4a8d-ac47-37d7d0b80065", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. ", "original_text": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cfa0077-f7a5-44f4-b1c9-e5a9fb42de46", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u222b<sup>\u221e</sup><sub>-\u221e</sub> K(u)du = \u222b<sup>1</sup><sub>-1</sub> K(u)du = 1 (9)\n\nIt is easy to see that by changing the integration interval we can move from {-1, 1} to any limits {a, b}.  Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n", "original_text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n"}, "hash": "dce91f415ca6ae35932d091528e330a15e2b518d360a2578e43499265984a0ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75a75f1e-4d98-4743-93f7-cfacebf2838f", "node_type": "1", "metadata": {"window": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n", "original_text": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n"}, "hash": "a6ba3d9599702bde44dbd9cdb1fc915bdac2e64e2559cba57c773840f4f27742", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n", "mimetype": "text/plain", "start_char_idx": 17430, "end_char_idx": 17609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "75a75f1e-4d98-4743-93f7-cfacebf2838f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n", "original_text": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc2b58c1-c056-4a8d-ac47-37d7d0b80065", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Rewriting (9) we obtain the integral of f(x) - F(x) in the form of the formula (10).\n\n F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. ", "original_text": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n"}, "hash": "97996292e115fe00ccceadf07fca51465fe6954731a6b6713ffdda56bbd37416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a6d6434-b868-460f-a78b-d6f6a0b3cbe6", "node_type": "1", "metadata": {"window": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. ", "original_text": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n"}, "hash": "f6fc61a385e7ff506db3e0880701b98489721471a6d28a70c14e000eda23cb50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n", "mimetype": "text/plain", "start_char_idx": 17609, "end_char_idx": 17900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a6d6434-b868-460f-a78b-d6f6a0b3cbe6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. ", "original_text": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75a75f1e-4d98-4743-93f7-cfacebf2838f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "F<sub>(a,b)</sub>(x) = \u222b<sup>b</sup><sub>a</sub> K( (x - (b+a)/2) / ((b-a)/2) ) dx = (b-a)/2 \u222b<sup>1</sup><sub>-1</sub> K(u)du = (b-a)/2 (10)\n\nLet X be increasing sequence of real numbers containing n elements.  For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n", "original_text": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n"}, "hash": "133e7df9496e24cd1a0d5723e443209d78fcc6914911a119cafe17a8554d4410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b71c0fa-614b-4d5d-9405-aa0ed669fb95", "node_type": "1", "metadata": {"window": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. ", "original_text": "### 3.3. "}, "hash": "acb7cb7a5220d0e2eadc5277ace92841f8157bf2ab28ceeb315e4ba00adb6af8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n", "mimetype": "text/plain", "start_char_idx": 17900, "end_char_idx": 18102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b71c0fa-614b-4d5d-9405-aa0ed669fb95", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. ", "original_text": "### 3.3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a6d6434-b868-460f-a78b-d6f6a0b3cbe6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "For any pair of its elements x<sub>i</sub> and x<sub>i+1</sub>, i \u2208 [1, n \u2013 1], we can write the formulae (9) and (10) in the form shown in (11) and (12), respectively.\n\n f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. ", "original_text": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n"}, "hash": "891efbe65ec924989261ba6ab93a54168333e2cb4c07c348a9664bc8c04e975b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a74f0865-3d7e-48a5-8d57-69e6bd8e8c54", "node_type": "1", "metadata": {"window": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n", "original_text": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n"}, "hash": "7dbc8713ade4e3bed1a85b2ad77000691615d8cc0b579a09f5e8eca3fe2b7e64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.3. ", "mimetype": "text/plain", "start_char_idx": 18102, "end_char_idx": 18111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a74f0865-3d7e-48a5-8d57-69e6bd8e8c54", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n", "original_text": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b71c0fa-614b-4d5d-9405-aa0ed669fb95", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "f<sub>i</sub>(x) = K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (11)\n\nF<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)/2 (12)\n\nOn the basis of (12), the probability of randomly choosing a split value xg belonging to the i-th segment reads as the formula (13) shows.\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. ", "original_text": "### 3.3. "}, "hash": "2554dba7334dc5ecbeb22c77e9dc4d7eafb1b52de5d276843e50a79a4ad7e96e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1816da83-9695-452d-8c02-ac0046bf70e1", "node_type": "1", "metadata": {"window": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n", "original_text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. "}, "hash": "37a1b201f23392a52a7b10dee0225efa43474e36edd7be5df66c68f6a7edf792", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n", "mimetype": "text/plain", "start_char_idx": 18111, "end_char_idx": 18438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1816da83-9695-452d-8c02-ac0046bf70e1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n", "original_text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a74f0865-3d7e-48a5-8d57-69e6bd8e8c54", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [x<sub>i</sub>, x<sub>i+1</sub>)) = F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) / s (13)\n\nWhere s is the sum of values of F for all the segments, essential for ensuring that P is bounded above by 1 as shows (14).\n\n s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n", "original_text": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n"}, "hash": "18049c3eb723a86e75c0de4fda4ea3fb935c65a571517765288f2c86b1d81179", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bf6bdb0-95ef-4fa1-9f4e-ec9823b5b301", "node_type": "1", "metadata": {"window": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value. ", "original_text": "1).\n\n"}, "hash": "78b2a099ff72aba3a63b5f17cfac49453347765fbf41af5225d92bd93945239d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. ", "mimetype": "text/plain", "start_char_idx": 18438, "end_char_idx": 18776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8bf6bdb0-95ef-4fa1-9f4e-ec9823b5b301", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value. ", "original_text": "1).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1816da83-9695-452d-8c02-ac0046bf70e1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "s = \u2211<sup>n-1</sup><sub>i=1</sub> F<sub>x<sub>i</sub>,x<sub>i+1</sub></sub>(x) (14)\n\nThus, the probability density function at the i-th segment can be found by the formula (15).\n\n p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n", "original_text": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig. "}, "hash": "a2965ccf81da1cdb02684fe46929ccee93d23d1abd214621ee4c6d3b8157ff3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "766c7306-33fc-4af1-abe4-bdab06e5db37", "node_type": "1", "metadata": {"window": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. ", "original_text": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. "}, "hash": "eb992576787b02794a087da2e54333dc3aca81ee25cccd2f176349f9377e6923", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1).\n\n", "mimetype": "text/plain", "start_char_idx": 18776, "end_char_idx": 18781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "766c7306-33fc-4af1-abe4-bdab06e5db37", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. ", "original_text": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bf6bdb0-95ef-4fa1-9f4e-ec9823b5b301", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "p<sub>i</sub>(x) = (1/s) * f<sub>i</sub>(x) (15)\n\nDue to piecewise definition of the probability density function and because of the fact that we use Kernel functions for defining it, the cumulative probability function for generation of x<sub>g</sub> can be written with the formula (16).\n\n P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value. ", "original_text": "1).\n\n"}, "hash": "0d95cd06b798ec35701a9c587af62f4d3311ec011501fd027e880b684a34e330", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1628cd13-c7ae-4267-85d2-2999d0979e39", "node_type": "1", "metadata": {"window": "### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n", "original_text": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. "}, "hash": "809a5f418fce35332b3f7e9b2f4b1d82c4b442422229030ec8b09ecfce319ecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. ", "mimetype": "text/plain", "start_char_idx": 18781, "end_char_idx": 18909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1628cd13-c7ae-4267-85d2-2999d0979e39", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n", "original_text": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "766c7306-33fc-4af1-abe4-bdab06e5db37", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P(x \u2264 x<sub>g</sub>) = \u2211<sup>m-1</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m</sub></sub> p<sub>m</sub>(x)dx (16)\n\nwhere m = max (j), j \u2208 N, x<sub>j</sub> \u2264 x<sub>g</sub>.\n\n ### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. ", "original_text": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other. "}, "hash": "33f330b4914efec675918366689280efbcffd24d0f22c5691bd2e31694797e77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "664afc97-0787-4a4e-ba87-95cbbaa5b3fb", "node_type": "1", "metadata": {"window": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n", "original_text": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n"}, "hash": "d5e82bbbfc49f7760b7bb262cf73f362a9abf332c58fe37744e41b0615295655", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. ", "mimetype": "text/plain", "start_char_idx": 18909, "end_char_idx": 19019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "664afc97-0787-4a4e-ba87-95cbbaa5b3fb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n", "original_text": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1628cd13-c7ae-4267-85d2-2999d0979e39", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 3.3.  Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n", "original_text": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps. "}, "hash": "1dba3b18aebcfe92367bbcb892987489ca3299af8efc74acd661db881a44f051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7bc4f25-db19-4449-94d9-6d3186363205", "node_type": "1", "metadata": {"window": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n", "original_text": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n"}, "hash": "4bf8fd00f9ebd2b0079e4d04fa481d47a311784d5e2a76f7124c1e096f419d81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n", "mimetype": "text/plain", "start_char_idx": 19019, "end_char_idx": 19186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7bc4f25-db19-4449-94d9-6d3186363205", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n", "original_text": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "664afc97-0787-4a4e-ba87-95cbbaa5b3fb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mathematical formulation of Probabilistic generalization of Isolation Forest\n\nBecause of continuity of the segments, in case of uniform K(\u00b7), the probability of the x<sub>g</sub> belonging to any segment {a, b} within [x<sub>1</sub>, x<sub>n</sub>] depends solely on the distance between a and b as shown in the formula (17).\n\n P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n", "original_text": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n"}, "hash": "3a06f53342c026c60c4f99d6d35bc8b666383f60af3684f0c2ccaee07eee1c33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e16a4a99-c5c1-4ac7-9153-2be7dcb98d69", "node_type": "1", "metadata": {"window": "1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n", "original_text": "The introduced generalization requires the modification of procedure of generating the split value. "}, "hash": "b9cb930af25b23c166486d794ccbb532216eceb4b5a91c15805ecb2da518ec7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n", "mimetype": "text/plain", "start_char_idx": 19186, "end_char_idx": 19488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e16a4a99-c5c1-4ac7-9153-2be7dcb98d69", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n", "original_text": "The introduced generalization requires the modification of procedure of generating the split value. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7bc4f25-db19-4449-94d9-6d3186363205", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P<sub>i</sub> = P(x<sub>g</sub> \u2208 [a,b)) = (b-a) / (x<sub>n</sub> - x<sub>1</sub>) (17)\n\nIt means that in the original Isolation Forest the probability of generating x<sub>g</sub> such that the split is performed across a cluster is much higher in comparison to splitting across an inter-cluster gap that is relatively narrower (See Fig.  1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n", "original_text": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n"}, "hash": "6c27a9ccffbe5fd347d489eef77a345f64585b3330c9c99893f3968ffa0031dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8b28723-01d8-4719-9695-8604c7fa0766", "node_type": "1", "metadata": {"window": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). ", "original_text": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. "}, "hash": "cf66907d4fba6fee23e4d01c1a0d4481fd810c4e9dcaaaeb2a5a0fa804faee53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The introduced generalization requires the modification of procedure of generating the split value. ", "mimetype": "text/plain", "start_char_idx": 19488, "end_char_idx": 19588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8b28723-01d8-4719-9695-8604c7fa0766", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). ", "original_text": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e16a4a99-c5c1-4ac7-9153-2be7dcb98d69", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1).\n\n The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n", "original_text": "The introduced generalization requires the modification of procedure of generating the split value. "}, "hash": "3b1b6ead42edbf76dff569b87a21b634d2ad97f1f6497c4d66c275d8ae742f2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aaf2de72-d77e-4698-abdd-0c416b12dfde", "node_type": "1", "metadata": {"window": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g. ", "original_text": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n"}, "hash": "ff018097db402b4af948c388dfdb928848a683d20b8f81f8f85307d1f1dd7a07", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. ", "mimetype": "text/plain", "start_char_idx": 19588, "end_char_idx": 19671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aaf2de72-d77e-4698-abdd-0c416b12dfde", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g. ", "original_text": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8b28723-01d8-4719-9695-8604c7fa0766", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The basic idea of the proposed method is the assumption that the normal datapoints in clusters are located close to each other.  The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). ", "original_text": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution. "}, "hash": "8103995ee2d1449018145ee2eb7e99cef17922c31a8319178c3e5f98652315d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4135b6bb-0075-4182-a104-91fc5dc48b59", "node_type": "1", "metadata": {"window": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. ", "original_text": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n"}, "hash": "1559936eef2e67c69c42a1a93737c783144047c630693890174081f93e9e17f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n", "mimetype": "text/plain", "start_char_idx": 19671, "end_char_idx": 19795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4135b6bb-0075-4182-a104-91fc5dc48b59", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. ", "original_text": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aaf2de72-d77e-4698-abdd-0c416b12dfde", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The outliers, on the contrary, are located outside of the clusters and are separated by relatively wide gaps.  Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g. ", "original_text": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n"}, "hash": "d009e759da9c1e9404b662885b0c2423e137f4697b7dbb1fdee33d9e8e74189a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d38a2b1-4b65-491e-82b7-3930b2b75803", "node_type": "1", "metadata": {"window": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n", "original_text": "Using (10) we can express (20) by the formula (21).\n\n"}, "hash": "d9db9572369fb4bfc21e067a9ef9601fef742bb95f09db8ec8e8ea93b676f35f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n", "mimetype": "text/plain", "start_char_idx": 19795, "end_char_idx": 20185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d38a2b1-4b65-491e-82b7-3930b2b75803", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n", "original_text": "Using (10) we can express (20) by the formula (21).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4135b6bb-0075-4182-a104-91fc5dc48b59", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, we introduce the generalization expressed by the formula (18), by adding to the formula (11) the dependency yielded from k-th power of a segment\u2019s length.\n\n f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. ", "original_text": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n"}, "hash": "45e398e88f04c6976923b83c63ddd8b0493fe62875945ab086c7a2e47d667699", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d1e50d7-4769-44fb-a652-41172126d8b1", "node_type": "1", "metadata": {"window": "The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n", "original_text": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n"}, "hash": "f396876e69e96ecb7f9c68f3c74168759a8337ddb007345c05413bc1368cd0ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using (10) we can express (20) by the formula (21).\n\n", "mimetype": "text/plain", "start_char_idx": 20185, "end_char_idx": 20238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d1e50d7-4769-44fb-a652-41172126d8b1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n", "original_text": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d38a2b1-4b65-491e-82b7-3930b2b75803", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "f<sub>i</sub>(x) = (x<sub>i+1</sub> - x<sub>i</sub>)<sup>k</sup> K( (x - (x<sub>i+1</sub>+x<sub>i</sub>)/2) / ((x<sub>i+1</sub>-x<sub>i</sub>)/2) ) (18)\n\nNote that within the light of the proposed generalization the original Isolation Forest method becomes a special case with k = 0 and uniform K(\u00b7).\n\n The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n", "original_text": "Using (10) we can express (20) by the formula (21).\n\n"}, "hash": "b8a82e83f2a1b31d351be75a84c2ca36004d1148234fa42293d4e5bc655a48c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99fdbf7b-53d0-4ae2-a4c2-b339a8007dbd", "node_type": "1", "metadata": {"window": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. ", "original_text": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). "}, "hash": "ce4179705714807bff5a5a4033e0f5498dbe5cca287d57448fc7a14b275c09c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n", "mimetype": "text/plain", "start_char_idx": 20238, "end_char_idx": 20676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99fdbf7b-53d0-4ae2-a4c2-b339a8007dbd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. ", "original_text": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d1e50d7-4769-44fb-a652-41172126d8b1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The introduced generalization requires the modification of procedure of generating the split value.  The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n", "original_text": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n"}, "hash": "13664053cb1693f389b9156939045ae405e353e0f6bde1628af4aa0001fa2509", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7a5f2db-d1a2-4580-b0b4-7cd70b0044d1", "node_type": "1", "metadata": {"window": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n", "original_text": "In general, some numerical methods, e.g. "}, "hash": "7389de6323e157ec2f928c91c6c1d13939495b2eab475b2930327a9e16ff1f3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). ", "mimetype": "text/plain", "start_char_idx": 20676, "end_char_idx": 21028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7a5f2db-d1a2-4580-b0b4-7cd70b0044d1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n", "original_text": "In general, some numerical methods, e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99fdbf7b-53d0-4ae2-a4c2-b339a8007dbd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The first step includes generation of c \u2208 [0, 1), drawn from uniform distribution.  x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. ", "original_text": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7). "}, "hash": "20d61ade5c7001b39d142b1e87a1952652011b417d32c70c0502c03f608936ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c7b8fad-2d0e-4914-a4cc-ce3846c0c34f", "node_type": "1", "metadata": {"window": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. ", "original_text": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. "}, "hash": "9ec0765fc56c95ab8d6fe1c3ef9292f7349f26bc3d28f0fa880b03a504e5ff4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In general, some numerical methods, e.g. ", "mimetype": "text/plain", "start_char_idx": 21028, "end_char_idx": 21069, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c7b8fad-2d0e-4914-a4cc-ce3846c0c34f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. ", "original_text": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7a5f2db-d1a2-4580-b0b4-7cd70b0044d1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "x<sub>g</sub> has to be generated on the basis of c with the use of inverse cumulative probability function shown by (19).\n\n x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n", "original_text": "In general, some numerical methods, e.g. "}, "hash": "d9720287ea291361f5c70ed0d04e13ea0c2b4152355553fa5b4642bf8074dab5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ad917e6-ca30-407c-8333-013e313027ee", "node_type": "1", "metadata": {"window": "Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. ", "original_text": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n"}, "hash": "02f75e91f1dfcb4af9336568e1ace26c63c6d23d0b48a8f909f22a8d01564a89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. ", "mimetype": "text/plain", "start_char_idx": 21069, "end_char_idx": 21139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8ad917e6-ca30-407c-8333-013e313027ee", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. ", "original_text": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c7b8fad-2d0e-4914-a4cc-ce3846c0c34f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "x<sub>g</sub> = P<sup>-1</sup>(c), x<sub>g</sub>: P(x \u2264 x<sub>g</sub>) = c (19)\n\nIn the proposed method the value of x<sub>g</sub> can be obtained on the basis of the following formula:\n\nc = \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> + \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> p<sub>m+1</sub>(x)dx (20)\n\nwhere m = min(j) \u2013 1, j \u2208 N, c \u2264 \u2211<sup>j</sup><sub>i=1</sub> P<sub>i</sub>.\n Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. ", "original_text": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>. "}, "hash": "9dd0fb1f02108af48ceead362e069eb699d4368129f588d86ad2d8f1fecc79a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad9b98e0-772b-4aee-bd5f-297e9c90ea23", "node_type": "1", "metadata": {"window": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. ", "original_text": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n"}, "hash": "60f6db1af38e49fe4c3b02cf15d4659ddd050762d65b035c01226978dd8f558f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n", "mimetype": "text/plain", "start_char_idx": 21139, "end_char_idx": 21259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad9b98e0-772b-4aee-bd5f-297e9c90ea23", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. ", "original_text": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ad917e6-ca30-407c-8333-013e313027ee", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Using (10) we can express (20) by the formula (21).\n\n (1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. ", "original_text": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n"}, "hash": "afe219e7326a1b062e0d5623df47b1585eb22b9ac29c0a0239b5f8da00bfbf8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d11e2f1-a25e-4ac0-9ef0-82099c0166a4", "node_type": "1", "metadata": {"window": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. ", "original_text": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. "}, "hash": "1664cec8a77b48d70653409246e0f8b64ec011f38dff4a5aef0ced82251dcb6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n", "mimetype": "text/plain", "start_char_idx": 21259, "end_char_idx": 21423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d11e2f1-a25e-4ac0-9ef0-82099c0166a4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. ", "original_text": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad9b98e0-772b-4aee-bd5f-297e9c90ea23", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(1/s) \u222b<sup>x<sub>g</sub></sup><sub>x<sub>m+1</sub></sub> (x<sub>m+2</sub> - x<sub>m+1</sub>)<sup>k</sup> K( (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) ) dx = c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub> (21)\n\nOn the basis of the variable change u = (x - (x<sub>m+2</sub>+x<sub>m+1</sub>)/2) / ((x<sub>m+2</sub>-x<sub>m+1</sub>)/2) and the formula (10) one can transform (21) to the form of (22).\n\n \u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. ", "original_text": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n"}, "hash": "67b74e93d11669aae32f3fb68aced4d0e29dd7fd78432ae484a56d595b823292", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7a14f92-9002-4f69-9e88-e8a3c2f451f7", "node_type": "1", "metadata": {"window": "In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. ", "original_text": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n"}, "hash": "9b9a09552ff3a82ebf311896eb491eeae36ac07e576e61c28adc25eb84f45f86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. ", "mimetype": "text/plain", "start_char_idx": 21423, "end_char_idx": 21548, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7a14f92-9002-4f69-9e88-e8a3c2f451f7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. ", "original_text": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d11e2f1-a25e-4ac0-9ef0-82099c0166a4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u222b<sup>u<sub>g</sub></sup><sub>-1</sub> K(u)du = (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> (22)\n\nNext, u<sub>g</sub> can be obtained as the solution of the equation (23):\n\n\u03ba(u<sub>g</sub>) + (1/2) * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>) / p<sub>m+1</sub> = 0 (23)\n\nwhere \u03ba(u) is the antiderivative of the applied K(\u00b7).  In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. ", "original_text": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4. "}, "hash": "2b15e1707c017d25f43945210d9b23824653fc484b3059c38be025dfbd75e38f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d640e31-da1b-48ed-9a56-0616f87fc1e6", "node_type": "1", "metadata": {"window": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. ", "original_text": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. "}, "hash": "af5bbdac95957e6aff6a53ec1c647341922f036e7e93424177d8daf004ed95fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n", "mimetype": "text/plain", "start_char_idx": 21548, "end_char_idx": 21741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d640e31-da1b-48ed-9a56-0616f87fc1e6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. ", "original_text": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7a14f92-9002-4f69-9e88-e8a3c2f451f7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In general, some numerical methods, e.g.  Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. ", "original_text": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n"}, "hash": "7d4fdb5815092eef5fab91efbd6c8baadefa8d6a6b31ab6d0a23c80ea3c0ddc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69265357-00d2-4e7f-a9a7-e669d82d825c", "node_type": "1", "metadata": {"window": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. ", "original_text": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. "}, "hash": "fd044700d214723bbfaa9099534c2ae2fceaad0d1bb62353f0a27bf7ed87fe99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. ", "mimetype": "text/plain", "start_char_idx": 21741, "end_char_idx": 22039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69265357-00d2-4e7f-a9a7-e669d82d825c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. ", "original_text": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d640e31-da1b-48ed-9a56-0616f87fc1e6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Newton-Raphson or Bisection, can be used for obtaining u<sub>g</sub>.  Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. ", "original_text": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1. "}, "hash": "048b7469fbf9d9f5ab88303c0f1b13f32d1f9716368f7c2d41a118b1d49a4bbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7641ca9-90ed-47bd-b957-fb56c9086bfd", "node_type": "1", "metadata": {"window": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. ", "original_text": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. "}, "hash": "c3b610a97b3cdf76cbecab8181a251826cc37bd7ba61461e612c9d27f65cf90b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. ", "mimetype": "text/plain", "start_char_idx": 22039, "end_char_idx": 22094, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7641ca9-90ed-47bd-b957-fb56c9086bfd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. ", "original_text": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69265357-00d2-4e7f-a9a7-e669d82d825c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Uniform K(\u00b7) is in particular convenient as for this kind of kernel u<sub>g</sub> can be found directly as (24) shows.\n\n u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. ", "original_text": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2. "}, "hash": "f069f24de87a167af999e0574204c7aa47cfb0f3b4cd9b80cef2f51a3b7c23c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3729052c-1b95-4477-bd68-f2dbd8587a92", "node_type": "1", "metadata": {"window": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n", "original_text": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. "}, "hash": "9eb0fa40fa5e271b1f7b912e0b456b1e67e33c30063650f0bb402f58199c7b43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. ", "mimetype": "text/plain", "start_char_idx": 22094, "end_char_idx": 22138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3729052c-1b95-4477-bd68-f2dbd8587a92", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n", "original_text": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7641ca9-90ed-47bd-b957-fb56c9086bfd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "u<sub>g</sub> = (2 * (c - \u2211<sup>m</sup><sub>i=1</sub> P<sub>i</sub>)) / p<sub>m+1</sub> - 1 (24)\n\nFinally, x<sub>g</sub> is obtained with the use of formula (25).\n\n x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. ", "original_text": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3. "}, "hash": "46347735bc75ff7e390d38a4a778dd55dea20060ae7f7a2b1f698c6d95618c7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ecdc226-e1db-4015-a402-bc25b02f2ee9", "node_type": "1", "metadata": {"window": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2. ", "original_text": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. "}, "hash": "3020cae888df1dca6bb2f3d8ff41867053c1b65d7a5dc78b28a364495fd2dee6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. ", "mimetype": "text/plain", "start_char_idx": 22138, "end_char_idx": 22217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ecdc226-e1db-4015-a402-bc25b02f2ee9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2. ", "original_text": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3729052c-1b95-4477-bd68-f2dbd8587a92", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "x<sub>g</sub> = u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2 (25)\n\n### 3.4.  Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n", "original_text": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5. "}, "hash": "26e69ded185ba171b4c7229f0c98deb01b2d818d2555f53a8cbc3d996d7698ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d47dc4eb-7cc5-4166-b5fa-5f38043600f2", "node_type": "1", "metadata": {"window": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n", "original_text": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. "}, "hash": "6afd9080bfa99b8f2515942cf5c5b69e564c3363169678b1bc9fa98bfdbf9f5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. ", "mimetype": "text/plain", "start_char_idx": 22217, "end_char_idx": 22322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d47dc4eb-7cc5-4166-b5fa-5f38043600f2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n", "original_text": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ecdc226-e1db-4015-a402-bc25b02f2ee9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Algorithm of Probabilistic generalization of Isolation Forest\n\nOur approach extends the Algorithm 1 by modifying the operation described in the line 6 in the way presented in the Algorithm 2.\n\n **Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2. ", "original_text": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8. "}, "hash": "a10225985d8ab1a5503003723f8f191a6ecf40f9d50f38059e2296f09e35f130", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfc52546-9e8e-4198-a18b-c3b45d3f5c3c", "node_type": "1", "metadata": {"window": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3. ", "original_text": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. "}, "hash": "9d1b6a91dc9a9e168e1182f648553605343ee6efb113e045e874fd4615373bed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. ", "mimetype": "text/plain", "start_char_idx": 22322, "end_char_idx": 22393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfc52546-9e8e-4198-a18b-c3b45d3f5c3c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3. ", "original_text": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d47dc4eb-7cc5-4166-b5fa-5f38043600f2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Algorithm 2: generateSplitNonUniformly(X<sub>q</sub>, K, k)**\n\n**Inputs:** X<sub>q</sub> \u2013 input data cast to randomly selected axis q\nK \u2013 selected kernel\nk \u2013 power to which the length of segments is raised\n**Output:** x<sub>g</sub> \u2013 split value x \u2208 [min(X<sub>q</sub>), max(X<sub>q</sub>)]\n\n1.  X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n", "original_text": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11. "}, "hash": "a2838ffc6b2849d582d058a45790b65419ce0eee3a187098b87530b306d18d89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9447b161-d9ec-4db7-b8e3-33afde635305", "node_type": "1", "metadata": {"window": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n", "original_text": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. "}, "hash": "1c8dfd0e611ea1b3509df5fa442bb1357adf020caf4ede7922f4f282181c499f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. ", "mimetype": "text/plain", "start_char_idx": 22393, "end_char_idx": 22489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9447b161-d9ec-4db7-b8e3-33afde635305", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n", "original_text": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc52546-9e8e-4198-a18b-c3b45d3f5c3c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "X<sub>unq</sub> \u2190 sortedUniqueValues(X<sub>q</sub>)\n2.  D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3. ", "original_text": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13. "}, "hash": "e9810d36cc83956cd799cf9d8d9a0cee570861e89bfdb3f5b6be2a26257c58cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed2bfc82-d1c6-4a5c-91ac-c471635b98d4", "node_type": "1", "metadata": {"window": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4. ", "original_text": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n"}, "hash": "ddd007b5abac52a3d9dd9af3ec95e477e56c9f8f6f5ae1e0daa18235235c72fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. ", "mimetype": "text/plain", "start_char_idx": 22489, "end_char_idx": 22754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed2bfc82-d1c6-4a5c-91ac-c471635b98d4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4. ", "original_text": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9447b161-d9ec-4db7-b8e3-33afde635305", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "D \u2190 neighborDifferences(X<sub>unq</sub>)\n3.  P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n", "original_text": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1. "}, "hash": "91e2e95dbc0d0f66dacd7ee981883fd25a48091674d821ccfaef8f124363ca85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cfee974-3f2d-483e-8dec-855b282a499b", "node_type": "1", "metadata": {"window": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n", "original_text": "2. "}, "hash": "09a87c825979881e52dca68ed49fd0ef8ead70b9170ce1361c7bc550bc36bc38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n", "mimetype": "text/plain", "start_char_idx": 22754, "end_char_idx": 22877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6cfee974-3f2d-483e-8dec-855b282a499b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed2bfc82-d1c6-4a5c-91ac-c471635b98d4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P \u2190 {D<sub>i</sub><sup>k+1</sup> for each D<sub>i</sub> in D}\n4. s \u2190 sum(P)\n5.  P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4. ", "original_text": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n"}, "hash": "271923a93b26a119f7f5892c92ff7674ca86db075735bb82ffc340a70d5860e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b80205e-e0ca-4964-87a6-61e4518853c3", "node_type": "1", "metadata": {"window": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5. ", "original_text": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n"}, "hash": "50a5409f5979325975e543b18432a585b3351ec829115cd3053adf31dfd5420a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 22877, "end_char_idx": 22880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b80205e-e0ca-4964-87a6-61e4518853c3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5. ", "original_text": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cfee974-3f2d-483e-8dec-855b282a499b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "P \u2190 {P<sub>i</sub>/s for each P<sub>i</sub> in P}\n6. i \u2190 1\n7. c \u2190 uniformRandomReal(from \u2190 0, to \u2190 1)\n8.  **while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n", "original_text": "2. "}, "hash": "84394c51750107a0ef5b4920af6579c96344a8282e074a0efba5f601b9dfca0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a4f9c15-72a7-4283-9453-bf94dcb98b0f", "node_type": "1", "metadata": {"window": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n", "original_text": "3. "}, "hash": "f56c9da74b4354a4b523b118efb521bb501229cba9a6bc5d9080b8d9d9da83f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n", "mimetype": "text/plain", "start_char_idx": 22880, "end_char_idx": 23017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a4f9c15-72a7-4283-9453-bf94dcb98b0f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b80205e-e0ca-4964-87a6-61e4518853c3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**while** P<sub>i</sub> < c\n9. c \u2190 c \u2013 P<sub>i</sub>\n10. i \u2190 i + 1\n11.  **end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5. ", "original_text": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n"}, "hash": "56648d1b0ed58c920389f34f2d04439f26afee67cb117ac7efac3927869c37c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46001bee-1971-44c5-80f6-ca3d62c8b532", "node_type": "1", "metadata": {"window": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6. ", "original_text": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n"}, "hash": "6b24dd8359f09cc4e3c9031dac95428e5b781f0409d53e5f57b2846d4f7a850e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 23017, "end_char_idx": 23020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46001bee-1971-44c5-80f6-ca3d62c8b532", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6. ", "original_text": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a4f9c15-72a7-4283-9453-bf94dcb98b0f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**end while**\n12. u<sub>g</sub> \u2190 invertedCumulativeProbabilityFunction(K, c/P<sub>i</sub>)\n13.  **return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n", "original_text": "3. "}, "hash": "bbdaabd6222110047fea2678bd3bd89ecc852759c553c602285af1ab7841af20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc15dffe-adee-471d-8788-0cb55cb749f9", "node_type": "1", "metadata": {"window": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n", "original_text": "4. "}, "hash": "06eeccefc2bd67383810d5cf59b430048dccca7cf1762d2ed6daf04787e13ce2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n", "mimetype": "text/plain", "start_char_idx": 23020, "end_char_idx": 23114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc15dffe-adee-471d-8788-0cb55cb749f9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46001bee-1971-44c5-80f6-ca3d62c8b532", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**return** u<sub>g</sub> * (x<sub>m+2</sub> - x<sub>m+1</sub>)/2 + (x<sub>m+2</sub> + x<sub>m+1</sub>)/2\n\nThe Algorithm 2 allows to obtain a split value generated with probability density function built on the basis of the training sample in the following way:\n\n1.  The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6. ", "original_text": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n"}, "hash": "5bc3a2aed92b7fd997fc12559296956184487a497a50a514527f5f4f1170eca4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72004059-5a73-4aa2-aee0-709fbe16d6bd", "node_type": "1", "metadata": {"window": "2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7. ", "original_text": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n"}, "hash": "fc830484811680b5161d531a20ea98b86c313ea572a192382b9b29fb410a7214", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 23114, "end_char_idx": 23117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72004059-5a73-4aa2-aee0-709fbe16d6bd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7. ", "original_text": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc15dffe-adee-471d-8788-0cb55cb749f9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The input data X<sub>q</sub> are sorted and unique values are taken, the result is stored in the variable X<sub>unq</sub>.\n 2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n", "original_text": "4. "}, "hash": "1f642fc282f45aa7c4f8f1caa4a10e46979784019e42ab11ccb2724d8446c7d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "000d912f-879b-402e-b360-90552cd73b60", "node_type": "1", "metadata": {"window": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n", "original_text": "5. "}, "hash": "beb0903dcbb14a04702776ce269b3ee3e8cbe4ca5c6421f22afd1dc90c4a9eb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n", "mimetype": "text/plain", "start_char_idx": 23117, "end_char_idx": 23209, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "000d912f-879b-402e-b360-90552cd73b60", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n", "original_text": "5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72004059-5a73-4aa2-aee0-709fbe16d6bd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2.  The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7. ", "original_text": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n"}, "hash": "c5298abf7e1f045eecddbaa75061e6e098a031ce96656773a6f81e99f8fed37f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7324a9aa-722c-4566-8a82-05d6e5ff20ad", "node_type": "1", "metadata": {"window": "3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8. ", "original_text": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n"}, "hash": "96acc3c497e4040de18eb206a7409f35eb60592c90e6c9d429dfcc2a3d0c657b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. ", "mimetype": "text/plain", "start_char_idx": 23209, "end_char_idx": 23212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7324a9aa-722c-4566-8a82-05d6e5ff20ad", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8. ", "original_text": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "000d912f-879b-402e-b360-90552cd73b60", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The lengths of sequent sections are found as the differences between the neighboring datapoints, the result is stored in the variable D.\n 3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n", "original_text": "5. "}, "hash": "63dea550bb359f461680791c4d6815dc0624ed54a4e8e0d0e79e976b161fd6b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99359256-94c2-4ea8-b807-0189b5d0e03b", "node_type": "1", "metadata": {"window": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n", "original_text": "6. "}, "hash": "440d59efa5ae117496401b2ab0c55d1a4240f39486b783040341208ef5e01578", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n", "mimetype": "text/plain", "start_char_idx": 23212, "end_char_idx": 23301, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99359256-94c2-4ea8-b807-0189b5d0e03b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n", "original_text": "6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7324a9aa-722c-4566-8a82-05d6e5ff20ad", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.  Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8. ", "original_text": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n"}, "hash": "f89d128cf7ae11acb836ce2bb19c9d9b1169ef5f9adb582e58d14c6a928fb11e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83d8a291-6dce-4532-9140-6f1a07a01633", "node_type": "1", "metadata": {"window": "4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9. ", "original_text": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n"}, "hash": "5d6852ae908b9e38aeee6117a0e665548dbc0c2d295bdd1b8162abe875930615", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. ", "mimetype": "text/plain", "start_char_idx": 23301, "end_char_idx": 23304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83d8a291-6dce-4532-9140-6f1a07a01633", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9. ", "original_text": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99359256-94c2-4ea8-b807-0189b5d0e03b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each distance is raised to (k + 1)-th power, the resulting array is stored in the variable P.\n 4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n", "original_text": "6. "}, "hash": "017a07de4d5d9c7d37bde709393424d0ef41c5f529ad9cdbefeac26f5875d1a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e18e0d3-ddce-4897-89da-ab6103a52aaf", "node_type": "1", "metadata": {"window": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n", "original_text": "7. "}, "hash": "106d4d63ce252dac977a70c127c41598a053c60f172c108fd88f3a1731c51cbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n", "mimetype": "text/plain", "start_char_idx": 23304, "end_char_idx": 23410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4e18e0d3-ddce-4897-89da-ab6103a52aaf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n", "original_text": "7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83d8a291-6dce-4532-9140-6f1a07a01633", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4.  The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9. ", "original_text": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n"}, "hash": "d02d589cc318da133c0b4084d5b5ca338de9eded2a0532724c61fe82bbc3fae9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4810ea23-1280-4c3a-9754-ebfbbf322c34", "node_type": "1", "metadata": {"window": "5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10. ", "original_text": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n"}, "hash": "3013f3a3a8509690b89ae41ebd70551ccb0fba801a3f45889e34565dd1478fb1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7. ", "mimetype": "text/plain", "start_char_idx": 23410, "end_char_idx": 23413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4810ea23-1280-4c3a-9754-ebfbbf322c34", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10. ", "original_text": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e18e0d3-ddce-4897-89da-ab6103a52aaf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The sum of the elements of the array P is computed, the result is stored in the variable s.\n 5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n", "original_text": "7. "}, "hash": "1e3bb02b3853f1cb66d26be74baa6da9d8581f7b919378bd4a027c8db1c586ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad277346-4bbf-40b6-9752-c0fb71cc70e4", "node_type": "1", "metadata": {"window": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n", "original_text": "8. "}, "hash": "17a7ea62f23ca1fa2d42632b05579eabaf2bfb1a982b6dd93823787981a946d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n", "mimetype": "text/plain", "start_char_idx": 23413, "end_char_idx": 23559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad277346-4bbf-40b6-9752-c0fb71cc70e4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n", "original_text": "8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4810ea23-1280-4c3a-9754-ebfbbf322c34", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5.  Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10. ", "original_text": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n"}, "hash": "f0b44b1f5abd783be859980d02afcb0086f535b3dea512c0bb15384401b69419", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbd67ab4-3d04-4962-9e62-5f3e87d1ebc6", "node_type": "1", "metadata": {"window": "6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11. ", "original_text": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n"}, "hash": "16d73db419c2df07b74f8f67888b1cf36864ca6d41d254c486aefb6ff0ab92f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8. ", "mimetype": "text/plain", "start_char_idx": 23559, "end_char_idx": 23562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fbd67ab4-3d04-4962-9e62-5f3e87d1ebc6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11. ", "original_text": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad277346-4bbf-40b6-9752-c0fb71cc70e4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each element of P is divided by s in order to ensure the sum of P\u2019s elements equal to 1.\n 6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n", "original_text": "8. "}, "hash": "49206357b32f4dd4c6bd6111540a25f2a296954bb39909ab1a27a233e24ed26c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9086e548-e382-4afe-aa69-746a273b5785", "node_type": "1", "metadata": {"window": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n", "original_text": "**The loop body:**\n9. "}, "hash": "346e9ac56161c6fd1f1b09573e1f345a224b2d3fcebeb98382584d60bf96377a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n", "mimetype": "text/plain", "start_char_idx": 23562, "end_char_idx": 23810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9086e548-e382-4afe-aa69-746a273b5785", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n", "original_text": "**The loop body:**\n9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbd67ab4-3d04-4962-9e62-5f3e87d1ebc6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6.  A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11. ", "original_text": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n"}, "hash": "c48bd3401dc3b8688911d7aff24ba5e310c040e488f1cb724ff3c5da1a3bb3ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af7408c8-d439-4c9d-94d6-e90d846c18cb", "node_type": "1", "metadata": {"window": "7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12. ", "original_text": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n"}, "hash": "d457b688bbe34d76e654024f801a11480ab0ebb51c34ab9fb50c9b84a72be84f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**The loop body:**\n9. ", "mimetype": "text/plain", "start_char_idx": 23810, "end_char_idx": 23832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af7408c8-d439-4c9d-94d6-e90d846c18cb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12. ", "original_text": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9086e548-e382-4afe-aa69-746a273b5785", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A random number is drawn from uniform distribution from the interval [0, 1) and stored in the variable c.\n 7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n", "original_text": "**The loop body:**\n9. "}, "hash": "4dec795abf710b4f9c9174cf90ec60558ec7c0aa4b9d0adc7c8cfe081412c671", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca30f56b-3b11-4ee9-b135-71e02563ffdb", "node_type": "1", "metadata": {"window": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n", "original_text": "10. "}, "hash": "b71d45ce136f1d219426653979945a5cd764e3014c917c053c92850812c73289", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n", "mimetype": "text/plain", "start_char_idx": 23832, "end_char_idx": 23947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca30f56b-3b11-4ee9-b135-71e02563ffdb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n", "original_text": "10. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af7408c8-d439-4c9d-94d6-e90d846c18cb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "7.  The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12. ", "original_text": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n"}, "hash": "6c04b1b13305249815214602725bff8a9c6cd76418af9c4f563b971adea788f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c21fd822-9ae4-4d6f-943a-e847aa628bca", "node_type": "1", "metadata": {"window": "8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5. ", "original_text": "The counter i is decreased.\n\n"}, "hash": "2186a8924e90980ab71cc9aab31dc85d3b36e17e72abe6c4112dde4e03cb13b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10. ", "mimetype": "text/plain", "start_char_idx": 23947, "end_char_idx": 23951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c21fd822-9ae4-4d6f-943a-e847aa628bca", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5. ", "original_text": "The counter i is decreased.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca30f56b-3b11-4ee9-b135-71e02563ffdb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The counter variable i, indicating the order number of currently considered segment and the cumulated probability assigned to it (P<sub>i</sub>).\n 8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n", "original_text": "10. "}, "hash": "823faec56a89c354207e9401c07726995a86037b2f17a7a0fe49f182275914a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb2244db-56d7-4a38-8e17-548bc471e69e", "node_type": "1", "metadata": {"window": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. ", "original_text": "**End of the loop body**\n11. "}, "hash": "243451692d9b9909d7acf029d79c8121b0d44e255cca7fa581caf0fa6f8c180f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The counter i is decreased.\n\n", "mimetype": "text/plain", "start_char_idx": 23951, "end_char_idx": 23980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb2244db-56d7-4a38-8e17-548bc471e69e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. ", "original_text": "**End of the loop body**\n11. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c21fd822-9ae4-4d6f-943a-e847aa628bca", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "8.  Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5. ", "original_text": "The counter i is decreased.\n\n"}, "hash": "ba0527a5454737690565988a43db97c819791bf0742dd801cf268f6f3008d0a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e6f315b-e268-425c-9e0f-2c593b34a952", "node_type": "1", "metadata": {"window": "**The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. ", "original_text": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n"}, "hash": "03b66c864de9736d7bb4f4940e064937d64da3178eecc68750434ed07308a442", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**End of the loop body**\n11. ", "mimetype": "text/plain", "start_char_idx": 23980, "end_char_idx": 24009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e6f315b-e268-425c-9e0f-2c593b34a952", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. ", "original_text": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb2244db-56d7-4a38-8e17-548bc471e69e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Condition of the loop: the loop body is performed while the value of cumulated probability of the currently considered segment (P<sub>i</sub>) is lower than the current value of the variable c containing the random number generated in the line 7.\n\n **The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. ", "original_text": "**End of the loop body**\n11. "}, "hash": "9db67ea053587b48825eb353589cb0f9fcc7f69098b50ff8508f852be9f79521", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c8a69e5-230f-425f-927c-38b77b2af1d0", "node_type": "1", "metadata": {"window": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n", "original_text": "12. "}, "hash": "5d77e9d2043806d1e0627d0a9744189494afde68c0f267a97d5ea03c25af9808", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n", "mimetype": "text/plain", "start_char_idx": 24009, "end_char_idx": 24227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c8a69e5-230f-425f-927c-38b77b2af1d0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n", "original_text": "12. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e6f315b-e268-425c-9e0f-2c593b34a952", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**The loop body:**\n9.  The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. ", "original_text": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n"}, "hash": "120fe923e41b65baf4bea6a4690fae22aac7f7f9caf901ba6336cc41417478a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1100f46-87b6-4444-9a1e-1c89f4950b6f", "node_type": "1", "metadata": {"window": "10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n", "original_text": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n"}, "hash": "8a8d86997d1981d3314a8f17058b5c69458f08bc18bce20ee9ee196fb9553fdb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12. ", "mimetype": "text/plain", "start_char_idx": 24227, "end_char_idx": 24231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1100f46-87b6-4444-9a1e-1c89f4950b6f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n", "original_text": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c8a69e5-230f-425f-927c-38b77b2af1d0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The value of c is decreased by the value of cumulated probability of currently considered segment (P<sub>i</sub>).\n 10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n", "original_text": "12. "}, "hash": "bba2d2609717bff9a8ecef5285c2db5cb57950d4ca8da91a79a1ebe4176de9f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b9529b5-d988-4689-bc39-f768822ccce5", "node_type": "1", "metadata": {"window": "The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n", "original_text": "### 3.5. "}, "hash": "3c7203ff95c210174a5defc5870deb3817a3ed5d63d9c9f1f18d6d7780550515", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n", "mimetype": "text/plain", "start_char_idx": 24231, "end_char_idx": 24628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b9529b5-d988-4689-bc39-f768822ccce5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n", "original_text": "### 3.5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1100f46-87b6-4444-9a1e-1c89f4950b6f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "10.  The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n", "original_text": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n"}, "hash": "9bf817ffb98f50069ddacf19a5184febe5bfe73a4b27178b05ae8ca94eed0bb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fe2da6f-fb6c-4ae3-9800-c9cdf179745c", "node_type": "1", "metadata": {"window": "**End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF. ", "original_text": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. "}, "hash": "be1f206682529efbaaaea10f9699610100aa280f99b1f7261875db92d370a2f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.5. ", "mimetype": "text/plain", "start_char_idx": 24628, "end_char_idx": 24637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fe2da6f-fb6c-4ae3-9800-c9cdf179745c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF. ", "original_text": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b9529b5-d988-4689-bc39-f768822ccce5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The counter i is decreased.\n\n **End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n", "original_text": "### 3.5. "}, "hash": "aae87a88795207b128c947a95f6da6a5a9a5289eeb5bf74fb42c8444881aac11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d0d6d66-838d-43bd-9817-ab4553f0d942", "node_type": "1", "metadata": {"window": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). ", "original_text": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. "}, "hash": "df13b0f50597049f79b1214c108746d030ef1a6a7e2fe79c8e5ef3675e372cbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. ", "mimetype": "text/plain", "start_char_idx": 24637, "end_char_idx": 24899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d0d6d66-838d-43bd-9817-ab4553f0d942", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). ", "original_text": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fe2da6f-fb6c-4ae3-9800-c9cdf179745c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**End of the loop body**\n11.  The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF. ", "original_text": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2. "}, "hash": "6d1a7b5701158246e9bec3d16455db6d7af4abc7b9aab4bd8554ba8021b5f2f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87d69834-5222-4b97-bb06-0a3ff508fef1", "node_type": "1", "metadata": {"window": "12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps. ", "original_text": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n"}, "hash": "6f97000679adfef70cbafb5a12ffdf1f2043dbaed945b0fc54a4ddb6147031d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. ", "mimetype": "text/plain", "start_char_idx": 24899, "end_char_idx": 25038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "87d69834-5222-4b97-bb06-0a3ff508fef1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps. ", "original_text": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d0d6d66-838d-43bd-9817-ab4553f0d942", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The value from the interval [\u22121, 1] corresponding to the value of c is calculated with the use of inverted cumulative probability function found on the basis of applied kernel K, the result is stored in u<sub>g</sub>.\n 12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). ", "original_text": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset. "}, "hash": "40ae8d5dc8342ea259b9ce6935c2ebb4642e268e5d71cce99ad5810a1eb2985c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da07df07-a2e6-47b4-8482-b3c87b2ad32b", "node_type": "1", "metadata": {"window": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. ", "original_text": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n"}, "hash": "0354481419edd75fc114450ef9233009a9e87562a62324f7924de1f453c17d22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n", "mimetype": "text/plain", "start_char_idx": 25038, "end_char_idx": 25149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da07df07-a2e6-47b4-8482-b3c87b2ad32b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. ", "original_text": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87d69834-5222-4b97-bb06-0a3ff508fef1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "12.  Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps. ", "original_text": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n"}, "hash": "18a818f6021de321050210e0bb4830247fbec2238afe31e641e15b990d441a51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5610d03-f0e6-411c-98c5-0de3e74155e3", "node_type": "1", "metadata": {"window": "### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). ", "original_text": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n"}, "hash": "90571a01803db4b1edf642ad411d3184c34aa62baf54061acaac236a8f268257", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n", "mimetype": "text/plain", "start_char_idx": 25149, "end_char_idx": 25232, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5610d03-f0e6-411c-98c5-0de3e74155e3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). ", "original_text": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da07df07-a2e6-47b4-8482-b3c87b2ad32b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Since u<sub>g</sub> \u2208 [\u22121, 1], it has to be scaled (multiplied by (x<sub>i+1</sub> \u2212 x<sub>i</sub>)/2) and shifted (increased by (x<sub>i+1</sub> + x<sub>i</sub>)/2), thanks to that we obtain a random value that belongs to the interval [x<sub>i</sub> + x<sub>i+1</sub>] and is generated in accordance with piecewise defined probability density function built on the basis of the training sample.\n\n ### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. ", "original_text": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n"}, "hash": "e62475697f5df38f3524ce234f55330593e7ee621c1b5e2828b2c76e8401384b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe4aad08-d572-4b93-85fe-295d962ded56", "node_type": "1", "metadata": {"window": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig. ", "original_text": "Setting c = 0 we obtain uniform probability density that is used in the original IF. "}, "hash": "a2bebc57138e2d1b580f21b37c2a1ed2a30a271fc6836e137ead164f6ccf6bc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n", "mimetype": "text/plain", "start_char_idx": 25232, "end_char_idx": 25486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe4aad08-d572-4b93-85fe-295d962ded56", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig. ", "original_text": "Setting c = 0 we obtain uniform probability density that is used in the original IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5610d03-f0e6-411c-98c5-0de3e74155e3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 3.5.  Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). ", "original_text": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n"}, "hash": "b02da4b1cb1422da2b847639627f806ffa294935dfa46525f625af600157d888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1253480c-6f1d-4165-9bd5-e65b4bfe3510", "node_type": "1", "metadata": {"window": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function. ", "original_text": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). "}, "hash": "22337946fe77728e0ae5a588e4b0ef7d9834e9975b6b1c4c4ba0d334dea33492", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Setting c = 0 we obtain uniform probability density that is used in the original IF. ", "mimetype": "text/plain", "start_char_idx": 25486, "end_char_idx": 25571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1253480c-6f1d-4165-9bd5-e65b4bfe3510", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function. ", "original_text": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe4aad08-d572-4b93-85fe-295d962ded56", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Yet another generalization: U-shaped probability density\n\nAnother possibly profitable generalization can be the introduction of U-shaped probability density function for generating c instead of uniform distribution as described in the line 7 of the Algorithm 2.  The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig. ", "original_text": "Setting c = 0 we obtain uniform probability density that is used in the original IF. "}, "hash": "7ba22145d346e641f874c53315d788ad768cf89c751bd04a7f53bc64409f74d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81831d87-de4d-494e-bcfb-12f943c340c9", "node_type": "1", "metadata": {"window": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n", "original_text": "The procedure consists of two steps. "}, "hash": "88641303fbedaf71fd127d610bfa9910b740a944a919c68fba7cf91a7c25c9c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). ", "mimetype": "text/plain", "start_char_idx": 25571, "end_char_idx": 25715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "81831d87-de4d-494e-bcfb-12f943c340c9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n", "original_text": "The procedure consists of two steps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1253480c-6f1d-4165-9bd5-e65b4bfe3510", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The intuition behind the proposed approach is based upon the assumption that anomalies tend to be located on the periphery of the dataset.  Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function. ", "original_text": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26). "}, "hash": "3407efd5a1e7b9903a7c5e36dc67ec53f3ec1dc6cacccb9daaf5533b0057b984", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd000c73-19af-4e01-b071-5a89a99eca49", "node_type": "1", "metadata": {"window": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. ", "original_text": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. "}, "hash": "3b7ec9a3b05400dc3e1163c86fbe74ca23341fc81fe054264ba063b55d71a9f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The procedure consists of two steps. ", "mimetype": "text/plain", "start_char_idx": 25715, "end_char_idx": 25752, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd000c73-19af-4e01-b071-5a89a99eca49", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. ", "original_text": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81831d87-de4d-494e-bcfb-12f943c340c9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hence, slightly higher probability density at the sides can lead to improvement of anomaly detection quality.\n\n Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n", "original_text": "The procedure consists of two steps. "}, "hash": "f7fe0fd35087cf3289bd5cdd47b073be5c25baef568cde4ee7d4b435d0b0bc36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94adeac9-aeb3-4a1b-af45-5fd3c2719e25", "node_type": "1", "metadata": {"window": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). ", "original_text": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). "}, "hash": "ffc185bb6428e0fa07f231253e6d175f07a034d4c88bd0e1c05d2bd463e946fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. ", "mimetype": "text/plain", "start_char_idx": 25752, "end_char_idx": 25848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94adeac9-aeb3-4a1b-af45-5fd3c2719e25", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). ", "original_text": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd000c73-19af-4e01-b071-5a89a99eca49", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such function can be built by means of any Kernel Density Function as (26) shows.\n\n \u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. ", "original_text": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution. "}, "hash": "f262947687e6fd70b47b35deb9a81fb4e3d2c71cf23914b49e21fe98e8fc5482", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3ecbbc1-13a4-497f-bee2-1818ab99e8f9", "node_type": "1", "metadata": {"window": "Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm. ", "original_text": "Fig. "}, "hash": "7e793d4ae35beb7711ce86977cf08214a4060fb9533aaa45d67010be0c8ff1d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). ", "mimetype": "text/plain", "start_char_idx": 25848, "end_char_idx": 25992, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c3ecbbc1-13a4-497f-bee2-1818ab99e8f9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94adeac9-aeb3-4a1b-af45-5fd3c2719e25", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u03b8(x) = { (1 - c * K(x)) / (2 - c),  x \u2208 [-1, 1]; 0, otherwise } (26)\n\nIf we use a kernel function, we may be sure that \u03b8(\u00b7) is symmetric, nonnegative, if c \u2208 [0, 1/max(K(x))], and \u222b<sup>1</sup><sub>-1</sub> \u03b8(x)dx = 1, thanks to multiplying by 1/(2-c).\n\n Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). ", "original_text": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25). "}, "hash": "68fcd82e9d57eeea30a0c3e4292732d50d6c6e9cc1f3bf4dbc1157033ff1a7a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c144f6c-d1ff-44b4-a344-a644865910c0", "node_type": "1", "metadata": {"window": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. ", "original_text": "2 shows the example of scaled and shifted U-shaped probability density function. "}, "hash": "bf1bbd1f8ee41fc6d0b5b7f0ceb75c29ed7f8f5d432fd2f77201fe0f30fc6724", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 25992, "end_char_idx": 25997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c144f6c-d1ff-44b4-a344-a644865910c0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. ", "original_text": "2 shows the example of scaled and shifted U-shaped probability density function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3ecbbc1-13a4-497f-bee2-1818ab99e8f9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Setting c = 0 we obtain uniform probability density that is used in the original IF.  In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm. ", "original_text": "Fig. "}, "hash": "0ae9d4ecd007b843caed70d68572db5850066fa49ca002e5c196eb62593a2ee7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36a279c1-ced2-4988-b74d-5052d5eba49a", "node_type": "1", "metadata": {"window": "The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g. ", "original_text": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n"}, "hash": "71d7fbd3427b0772f9fa253535825c080c6a9dbc0a54769f8719380a20f89992", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 shows the example of scaled and shifted U-shaped probability density function. ", "mimetype": "text/plain", "start_char_idx": 25997, "end_char_idx": 26078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36a279c1-ced2-4988-b74d-5052d5eba49a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g. ", "original_text": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c144f6c-d1ff-44b4-a344-a644865910c0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of nonuniform Kernel and c > 0 the Algorithm 2 is modified by adding generation of random value on the basis of U-shaped function (26).  The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. ", "original_text": "2 shows the example of scaled and shifted U-shaped probability density function. "}, "hash": "e2ff0e38a6752febc4ae81fbe29ff433a21714f2fec2e2a4b3fb3e856af32a43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34e7bbc3-5414-4363-bcf7-22c19f5c0cd5", "node_type": "1", "metadata": {"window": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. ", "original_text": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. "}, "hash": "ac2a29ff27a18210fb1acbddf7810690f9734ff1f0d359ac0c9c08536dcbe113", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n", "mimetype": "text/plain", "start_char_idx": 26078, "end_char_idx": 26146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34e7bbc3-5414-4363-bcf7-22c19f5c0cd5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. ", "original_text": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36a279c1-ced2-4988-b74d-5052d5eba49a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The procedure consists of two steps.  In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g. ", "original_text": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n"}, "hash": "2186499eb2eef77625409622ac020762d5215f7f4852bee77044709df3a9904a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "659ec59e-2641-4d5a-b1ab-1e7a3ce3f4e9", "node_type": "1", "metadata": {"window": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. ", "original_text": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). "}, "hash": "4f5afc03890516eea8c41d045d0aae41cf9559e0e0cfcacca90bfc7b23546b17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. ", "mimetype": "text/plain", "start_char_idx": 26146, "end_char_idx": 26188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "659ec59e-2641-4d5a-b1ab-1e7a3ce3f4e9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. ", "original_text": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34e7bbc3-5414-4363-bcf7-22c19f5c0cd5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In the first of them a random number c<sub>u</sub> \u2208 [0, 1) is drawn from uniform distribution.  In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. ", "original_text": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6. "}, "hash": "57f00d4163ad5f1945c69dd4cd00b56ec0b250e414eb040959b9fbb1e169ae92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8b7e038-9153-4b71-8a4e-461e2f1c2cb3", "node_type": "1", "metadata": {"window": "Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n", "original_text": "Here, we present the estimation of average time complexity of the proposed algorithm. "}, "hash": "22c6f9633d49bc42cd5700390f0541e363fa6d9127321737f255d1f7f731c6e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). ", "mimetype": "text/plain", "start_char_idx": 26188, "end_char_idx": 26319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8b7e038-9153-4b71-8a4e-461e2f1c2cb3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n", "original_text": "Here, we present the estimation of average time complexity of the proposed algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "659ec59e-2641-4d5a-b1ab-1e7a3ce3f4e9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In the second one, on the basis of c<sub>u</sub> the value of c is obtained with the use of inverse cumulative probability function (23), (25).  Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. ", "original_text": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n). "}, "hash": "a7fc6d65ac3a343d4916979509e9a7fe13810492463c94456ae0a7779c6d3c43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4497fc10-531a-43d8-b4c7-295105002c9f", "node_type": "1", "metadata": {"window": "2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function. ", "original_text": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. "}, "hash": "365b1d5966f90ff43e9d78218dcb12f2d5e779f37d4f7479bd2447e22c5dd419", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, we present the estimation of average time complexity of the proposed algorithm. ", "mimetype": "text/plain", "start_char_idx": 26319, "end_char_idx": 26405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4497fc10-531a-43d8-b4c7-295105002c9f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function. ", "original_text": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8b7e038-9153-4b71-8a4e-461e2f1c2cb3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n", "original_text": "Here, we present the estimation of average time complexity of the proposed algorithm. "}, "hash": "e3e87c3d71c6021d7212a8a2be56eb16049ee8c20ffc95a90da81585b9408c37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7099953-44c0-433a-bb1a-cfe7dea3aa00", "node_type": "1", "metadata": {"window": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. ", "original_text": "Assuming that we apply some log-linear sorting, e.g. "}, "hash": "7a1ea2f9ac7dbf0a64845b3e5ccfb588fdd078264a4c7b9fa8ab67822b4aa101", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. ", "mimetype": "text/plain", "start_char_idx": 26405, "end_char_idx": 26534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7099953-44c0-433a-bb1a-cfe7dea3aa00", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. ", "original_text": "Assuming that we apply some log-linear sorting, e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4497fc10-531a-43d8-b4c7-295105002c9f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2 shows the example of scaled and shifted U-shaped probability density function.  It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function. ", "original_text": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points. "}, "hash": "acb6d448bcf9ab78f328171b37ce8578fef8cf2bf3bbfd443ba2ab9330f37629", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b08fd738-7078-4877-9f16-f351a1287fb9", "node_type": "1", "metadata": {"window": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n", "original_text": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. "}, "hash": "6a7d1f0562dcf2cef8758c16f62f3429ae91aa2c1920e2b53915adb0fed0316e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assuming that we apply some log-linear sorting, e.g. ", "mimetype": "text/plain", "start_char_idx": 26534, "end_char_idx": 26587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b08fd738-7078-4877-9f16-f351a1287fb9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n", "original_text": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7099953-44c0-433a-bb1a-cfe7dea3aa00", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It was built with c = 0.5 and trisquare Kernel, expressed by (27).\n\n K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. ", "original_text": "Assuming that we apply some log-linear sorting, e.g. "}, "hash": "4c2351df305cacbbd12e588d5546d714f72979d5e59f75a43c7b99aab5110fa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5902f7a-a435-489f-a82c-6c3639446456", "node_type": "1", "metadata": {"window": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig. ", "original_text": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. "}, "hash": "912c4e292b8df1f8af30f17189db5db09c5355628808915deb538e23557ea8f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. ", "mimetype": "text/plain", "start_char_idx": 26587, "end_char_idx": 26720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5902f7a-a435-489f-a82c-6c3639446456", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig. ", "original_text": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b08fd738-7078-4877-9f16-f351a1287fb9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "K(x) = (35/32) * (1 - u\u00b2)\u00b3 (27)\n\n### 3.6.  Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n", "original_text": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree. "}, "hash": "688085016ac0f07e2e5e20d40d5e536c7925fe51d4d531738febfdcca086368d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c5cbed7-60e6-4864-973c-addd9dccb1f5", "node_type": "1", "metadata": {"window": "Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2.", "original_text": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n"}, "hash": "0f81f1e431a2e39ff8b592abadaf91b54bfe4b1adfa8e617bc8bb3599027ae23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. ", "mimetype": "text/plain", "start_char_idx": 26720, "end_char_idx": 26877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c5cbed7-60e6-4864-973c-addd9dccb1f5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2.", "original_text": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5902f7a-a435-489f-a82c-6c3639446456", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Computational complexity analysis\n\nThe time complexity of building an isolation tree in an original Isolation Forest is O(nlog\u2082n).  Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig. ", "original_text": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree. "}, "hash": "66ceba8f80bb6663b80ca32b985f579c7fefd9e6bd7ab997ed4378bd78b286eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f28a2516-bee3-4820-aa85-8bd942bbe52c", "node_type": "1", "metadata": {"window": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function. ", "original_text": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function. "}, "hash": "9f91596358dec2b6b55f3f702bd9059fdb5917f68b5b481b619bb73518787e92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 26877, "end_char_idx": 27106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f28a2516-bee3-4820-aa85-8bd942bbe52c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function. ", "original_text": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c5cbed7-60e6-4864-973c-addd9dccb1f5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Here, we present the estimation of average time complexity of the proposed algorithm.  The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2.", "original_text": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n"}, "hash": "0756786f650182fb49b58532b3fcead0ce5e219adefd7fdabcdbf7289ff258cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffe5441f-cc50-4974-b937-57dd54e1161d", "node_type": "1", "metadata": {"window": "Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n", "original_text": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. "}, "hash": "25c101d50afdd8736ef0d4dca64344a346926fb88099b1b0e23037f65f06364c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function. ", "mimetype": "text/plain", "start_char_idx": 27106, "end_char_idx": 27192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ffe5441f-cc50-4974-b937-57dd54e1161d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n", "original_text": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f28a2516-bee3-4820-aa85-8bd942bbe52c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The most costly operation introduced by our modification is sorting, needed to obtain the distances between sequent data points.  Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function. ", "original_text": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function. "}, "hash": "6f92a26761e05c8a622a9ac1a57194ca18fa44e1304961dae18fe9fc866cd383", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dc38682-cff2-4def-bbe8-87aaddacb8d5", "node_type": "1", "metadata": {"window": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4. ", "original_text": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n"}, "hash": "4371a6b4d604baa22ff8e0cf201b598e45fb3fbd79fccd266e124f324699329c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. ", "mimetype": "text/plain", "start_char_idx": 27192, "end_char_idx": 27267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1dc38682-cff2-4def-bbe8-87aaddacb8d5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4. ", "original_text": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffe5441f-cc50-4974-b937-57dd54e1161d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Assuming that we apply some log-linear sorting, e.g.  Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n", "original_text": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0. "}, "hash": "1eaa2caafbc934a02a27b639d8df8dbc9ef9e921142ae5f3d13b58b23ed2808d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b69c645-456d-4cdc-9c8d-a4e4d05f4c95", "node_type": "1", "metadata": {"window": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1. ", "original_text": "**Fig. "}, "hash": "47780bdac324e4b6c4a451cf35d172481569392b0c42cd7c66fc04b0eb9486f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n", "mimetype": "text/plain", "start_char_idx": 27267, "end_char_idx": 27361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b69c645-456d-4cdc-9c8d-a4e4d05f4c95", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dc38682-cff2-4def-bbe8-87aaddacb8d5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mergesort, it is easy to see that additional operations would have the cost equal to O(nlog\u2082n) per a level of the built binary tree.  The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4. ", "original_text": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n"}, "hash": "aa04cf58390067992007bc57b4cfd6e69d44885620d9967203b55525757272fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6641a83-cdfa-4a6f-b292-854e99925d8e", "node_type": "1", "metadata": {"window": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. ", "original_text": "2."}, "hash": "dd0a5baf6b8d50758fe98d5552771615e96dab6fb281825377f60bccd5990e97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 27361, "end_char_idx": 27368, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6641a83-cdfa-4a6f-b292-854e99925d8e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. ", "original_text": "2."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b69c645-456d-4cdc-9c8d-a4e4d05f4c95", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The number of levels is equal to log\u2082n, so, theoretically, in the worst case the total complexity would increase from O(nlog\u2082n) to O(n (log\u2082n)\u00b2) per a tree.  The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1. ", "original_text": "**Fig. "}, "hash": "e73791dcd09e200058453c963c00dd4de1601f0adabb94f5cf799fbf5c4be14e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83683143-8932-49c7-8f13-6452011ea85e", "node_type": "1", "metadata": {"window": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language. ", "original_text": "** Example of U-shaped probability density function. "}, "hash": "6b6df1a7bd0680a7e3135b8ea148b1da85fe35ecab1ff362e8247ca5cafcda12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2.", "mimetype": "text/plain", "start_char_idx": 27368, "end_char_idx": 27370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83683143-8932-49c7-8f13-6452011ea85e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language. ", "original_text": "** Example of U-shaped probability density function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6641a83-cdfa-4a6f-b292-854e99925d8e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The numeric experiments conducted on artificial and real data show that the modification does not cause significant increase in the time cost of training and even lead to decrease of training time in case of extensive datasets.\n\n ---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. ", "original_text": "2."}, "hash": "62c57acbadaf8e6912e9a15488c2af3a10eb175aa3e8e4c26da8a3085f0a0419", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e5827a7-3c0f-4d4e-bdda-174221cc8c0e", "node_type": "1", "metadata": {"window": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. ", "original_text": "c = 0.5.\n"}, "hash": "e7d048bb68e5e95078ac4b12668f0bd57c2c49e9aeba63b2604242ae48870c27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Example of U-shaped probability density function. ", "mimetype": "text/plain", "start_char_idx": 27370, "end_char_idx": 27423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e5827a7-3c0f-4d4e-bdda-174221cc8c0e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. ", "original_text": "c = 0.5.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83683143-8932-49c7-8f13-6452011ea85e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 2: A plot showing a U-shaped probability density function.  The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language. ", "original_text": "** Example of U-shaped probability density function. "}, "hash": "9a5589544cb96c157c57b7ed6ad740bba95c7a2d2c09b1d49c9aceda678f3d26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73cf646c-65dc-4896-870e-0632892d071d", "node_type": "1", "metadata": {"window": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. ", "original_text": "---\n\n## 4. "}, "hash": "876a8af284f0cdba034b19c6a5da4a9461373edd8e99c69a2ebb9b29f804a993", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "c = 0.5.\n", "mimetype": "text/plain", "start_char_idx": 27423, "end_char_idx": 27432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73cf646c-65dc-4896-870e-0632892d071d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. ", "original_text": "---\n\n## 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e5827a7-3c0f-4d4e-bdda-174221cc8c0e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The y-axis is \"Probability density\" and the x-axis is \"x\" from 0.0 to 1.0.  The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. ", "original_text": "c = 0.5.\n"}, "hash": "5db240bdeaa9654c0df7f0a883108744afea5e9b2431cfda0018d1e750a5cb76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b2584a6-8e97-498a-a9ac-12294af12419", "node_type": "1", "metadata": {"window": "**Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods. ", "original_text": "Numerical experiments\n\n### 4.1. "}, "hash": "69bb33682407e607482ae595ffcad331247e4258ac79bb14c516e13998b52501", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n## 4. ", "mimetype": "text/plain", "start_char_idx": 27432, "end_char_idx": 27443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b2584a6-8e97-498a-a9ac-12294af12419", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods. ", "original_text": "Numerical experiments\n\n### 4.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73cf646c-65dc-4896-870e-0632892d071d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The curve starts high, dips down in the middle around x=0.5, and rises again towards x=1.0.]\n\n **Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. ", "original_text": "---\n\n## 4. "}, "hash": "5990b7fcd77fcb003d1bb1fb544f4c0177d27ee8acfa5bf198f4132ade94ef19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1275a48-8b57-484b-b944-8f32b142b099", "node_type": "1", "metadata": {"window": "2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. ", "original_text": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. "}, "hash": "7ca5156ce6a3d55a97869dfb6b114b37be6580d2ef3e567c0a0cb3d7b8e342ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Numerical experiments\n\n### 4.1. ", "mimetype": "text/plain", "start_char_idx": 27443, "end_char_idx": 27475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e1275a48-8b57-484b-b944-8f32b142b099", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. ", "original_text": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b2584a6-8e97-498a-a9ac-12294af12419", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods. ", "original_text": "Numerical experiments\n\n### 4.1. "}, "hash": "37506645ba38f43ba83fbe68710eddeb5d086532432fcd3c311f1ea9c5aa2e38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ca54ec6-cd80-4499-9a2f-b1fce0fe47c7", "node_type": "1", "metadata": {"window": "** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n", "original_text": "The program is written in high-level Python 3 language. "}, "hash": "cf12ad69beb23d22f024296059f5cacfa07ef4876e1acc8869aa7831dd0eb052", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. ", "mimetype": "text/plain", "start_char_idx": 27475, "end_char_idx": 27650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ca54ec6-cd80-4499-9a2f-b1fce0fe47c7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n", "original_text": "The program is written in high-level Python 3 language. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1275a48-8b57-484b-b944-8f32b142b099", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2. ** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. ", "original_text": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41]. "}, "hash": "cc09c4dbca9abf0a7531e65b1c971a8a926b32c87156cc0dd98a06b42f4fdd10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04c673af-454e-4d57-a4e4-5fbb8545bdd3", "node_type": "1", "metadata": {"window": "c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n", "original_text": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. "}, "hash": "7beb5bce040f44f4a18bf154ad5db545c5b260ccbcb774e4cc68f42f8a932ac3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The program is written in high-level Python 3 language. ", "mimetype": "text/plain", "start_char_idx": 27650, "end_char_idx": 27706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04c673af-454e-4d57-a4e4-5fbb8545bdd3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n", "original_text": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ca54ec6-cd80-4499-9a2f-b1fce0fe47c7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Example of U-shaped probability density function.  c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n", "original_text": "The program is written in high-level Python 3 language. "}, "hash": "c19c39997686f04b146180ce1d0babb928aff87f7e728163e3e98142785d6432", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "235def05-2ab1-4010-9ac8-e8e98fc725e0", "node_type": "1", "metadata": {"window": "---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted. ", "original_text": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. "}, "hash": "c23c37752210b1a70d5289d359a84ac7236b80251f861c9a74a87281c03476d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. ", "mimetype": "text/plain", "start_char_idx": 27706, "end_char_idx": 27856, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "235def05-2ab1-4010-9ac8-e8e98fc725e0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted. ", "original_text": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04c673af-454e-4d57-a4e4-5fbb8545bdd3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "c = 0.5.\n ---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n", "original_text": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions. "}, "hash": "d95212403c358e54e017ea604f8554774ef514041d62527b03f3ed4a74ee8367", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b83a45a2-5151-4399-8a9c-90e065164e21", "node_type": "1", "metadata": {"window": "Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. ", "original_text": "All the modifications were implemented as the children of this class, overriding respective methods. "}, "hash": "f4d7cba28efd50493356f9cbec851ef602c3e6ce04f2e7b3fe91f325a39a6b64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. ", "mimetype": "text/plain", "start_char_idx": 27856, "end_char_idx": 27947, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b83a45a2-5151-4399-8a9c-90e065164e21", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. ", "original_text": "All the modifications were implemented as the children of this class, overriding respective methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "235def05-2ab1-4010-9ac8-e8e98fc725e0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n## 4.  Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted. ", "original_text": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined. "}, "hash": "82fd08763e40b61a7452779e5f00d6c2d441baa501afc43a47a5bbb8803f285d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b7a2723-0b80-46d0-aa0f-c76a32cd9d82", "node_type": "1", "metadata": {"window": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. ", "original_text": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. "}, "hash": "e71206bc659ef320a2ac455c6738864b29f929478576bfe164e0ff093ad618f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All the modifications were implemented as the children of this class, overriding respective methods. ", "mimetype": "text/plain", "start_char_idx": 27947, "end_char_idx": 28048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b7a2723-0b80-46d0-aa0f-c76a32cd9d82", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. ", "original_text": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b83a45a2-5151-4399-8a9c-90e065164e21", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Numerical experiments\n\n### 4.1.  Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. ", "original_text": "All the modifications were implemented as the children of this class, overriding respective methods. "}, "hash": "fe84a2e2e8b0f1597b6fab7a05fdab7976c96a631382c6f1ab26dc603e510186", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf3eb59-3035-4f18-a872-9aa63c112e70", "node_type": "1", "metadata": {"window": "The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n", "original_text": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n"}, "hash": "1b4d169487d86a84d5358a9bd255592f3a4efe011f6ef6f37df97bce67a48484", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. ", "mimetype": "text/plain", "start_char_idx": 28048, "end_char_idx": 28165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdf3eb59-3035-4f18-a872-9aa63c112e70", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n", "original_text": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b7a2723-0b80-46d0-aa0f-c76a32cd9d82", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Experiments on artificial data\n\nThe implementation of our algorithm utilized selected elements of the publicly available implementation of Isolation Forest published by [41].  The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. ", "original_text": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz. "}, "hash": "6097a821721f74d44096acac4a445eb9fa5cb69d57d0caf6c0cb53b3b4eb8354", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d32e84ee-6856-4bdd-9fbf-43eb3e2ecb8e", "node_type": "1", "metadata": {"window": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2. ", "original_text": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n"}, "hash": "cea50e11012c96548387473f9fdbe0148476cb318cbbd61e70ab417fdbcc07e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n", "mimetype": "text/plain", "start_char_idx": 28165, "end_char_idx": 28371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d32e84ee-6856-4bdd-9fbf-43eb3e2ecb8e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2. ", "original_text": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdf3eb59-3035-4f18-a872-9aa63c112e70", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The program is written in high-level Python 3 language.  In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n", "original_text": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n"}, "hash": "baa0e27efd2b70e2f420fe1aa17887fbb8d7f604ec883c4f1a555745acfba8fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceaccebe-81c5-40ef-baea-ba592f540a95", "node_type": "1", "metadata": {"window": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n", "original_text": "A series of experiments on artificially generated datasets was conducted. "}, "hash": "79ce6b9b6561078452cfde5ead82c15b4cdc84343029c64afa1efcc609e5946b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n", "mimetype": "text/plain", "start_char_idx": 28371, "end_char_idx": 28535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ceaccebe-81c5-40ef-baea-ba592f540a95", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n", "original_text": "A series of experiments on artificially generated datasets was conducted. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d32e84ee-6856-4bdd-9fbf-43eb3e2ecb8e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In our realization the code underwent major refactoring in order to ensure the module structure convenient for experimenting with various extensions.  Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2. ", "original_text": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n"}, "hash": "2a2356eb3701dd82568423ccafabed29c9239a3466cacbc5f938c0a9e0d6aad9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90dff9a6-cc80-46ee-8270-a6d74b391221", "node_type": "1", "metadata": {"window": "All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3. ", "original_text": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. "}, "hash": "0119e3a6be892856dc784aa908ed6de280a074b5df8c82351ae882c5604c0f74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A series of experiments on artificially generated datasets was conducted. ", "mimetype": "text/plain", "start_char_idx": 28535, "end_char_idx": 28609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90dff9a6-cc80-46ee-8270-a6d74b391221", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3. ", "original_text": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceaccebe-81c5-40ef-baea-ba592f540a95", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Namely, specific class TreeGrower, responsible for building an Isolation Tree was defined.  All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n", "original_text": "A series of experiments on artificially generated datasets was conducted. "}, "hash": "72a82fed0dc7b17d93c789ed434e83ee8e0dd7c69f0e3bf1a98ebab64d44c99a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ac40019-c767-4aea-9e44-2fadd6e2cc23", "node_type": "1", "metadata": {"window": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . ", "original_text": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. "}, "hash": "b34c6a831b723788591787b090305ddc2653895918dd325cdf1840f9bfd03e74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. ", "mimetype": "text/plain", "start_char_idx": 28609, "end_char_idx": 28742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ac40019-c767-4aea-9e44-2fadd6e2cc23", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . ", "original_text": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90dff9a6-cc80-46ee-8270-a6d74b391221", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "All the modifications were implemented as the children of this class, overriding respective methods.  The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3. ", "original_text": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration. "}, "hash": "4f7af52f53bf6aa8144a349369347cf106f1b89f6f73ec1215d61a792bc1cf39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "723a02ae-bf64-4b7c-ada8-b4b543b25989", "node_type": "1", "metadata": {"window": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  . ", "original_text": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n"}, "hash": "c6a21985288c7434586c9969ca483ed74e678c330259e641b83aa779ecf28d33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. ", "mimetype": "text/plain", "start_char_idx": 28742, "end_char_idx": 28842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "723a02ae-bf64-4b7c-ada8-b4b543b25989", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  . ", "original_text": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ac40019-c767-4aea-9e44-2fadd6e2cc23", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The tests were conducted on a laptop personal computer with the following parameters: RAM: 8 Gb, CPU: 4-core 1.8 Hz.  The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . ", "original_text": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1. "}, "hash": "e952287f45607bdf03e67b0870873543fb01713fb37463d3b10e71f5dacbb128", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebf08b81-368d-47a7-8b19-9cb01a669f41", "node_type": "1", "metadata": {"window": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  . ", "original_text": "2. "}, "hash": "bc8d5818171d038b5f148c49b4fe21f831afd120b7d294e25e22b84bf3f47d29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n", "mimetype": "text/plain", "start_char_idx": 28842, "end_char_idx": 28949, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebf08b81-368d-47a7-8b19-9cb01a669f41", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  . ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "723a02ae-bf64-4b7c-ada8-b4b543b25989", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The complete implementation together with artificially generated and real datasets utilized in the experiments are available at https://github.com/mtokovarov/probabilistic_generalization_isolation_forest.\n\n In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  . ", "original_text": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n"}, "hash": "5e2d32c18345a3e1185ed5f9efe41402e5e8d7973e4f3e9ce18d8e6adcb5817c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5b0ed11-e5d7-4fe6-9e84-355dcc867dc6", "node_type": "1", "metadata": {"window": "A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, . ", "original_text": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n"}, "hash": "0049b090ca3466271500aae509278fc3bc7af61a0739d3b98505465ae9cbe3c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 28949, "end_char_idx": 28952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5b0ed11-e5d7-4fe6-9e84-355dcc867dc6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, . ", "original_text": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebf08b81-368d-47a7-8b19-9cb01a669f41", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In order to compare the performance of the generalized method the following parameters were used for testing on the artificial datasets: Uniform Kernel and k = 2.\n\n A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  . ", "original_text": "2. "}, "hash": "d448c8fd076a31844e9a5951e3c0a8cf9f40f1ab1e4607c57e69d29cd4c33651", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f649cf8-e7a9-42cf-aa42-53589b033765", "node_type": "1", "metadata": {"window": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  . ", "original_text": "3. "}, "hash": "dbd3cbab1b5c851ea16d6f6f605f5d3423591067e333498ec0be9944cf04e97d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n", "mimetype": "text/plain", "start_char_idx": 28952, "end_char_idx": 29068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f649cf8-e7a9-42cf-aa42-53589b033765", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  . ", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5b0ed11-e5d7-4fe6-9e84-355dcc867dc6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A series of experiments on artificially generated datasets was conducted.  The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, . ", "original_text": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n"}, "hash": "18171ba9004572739a935ad92e7d84394211b33d2cd24ef5140b15bdbaf92ce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1fe3feb-51f8-49a1-99d5-ff730a301dc0", "node_type": "1", "metadata": {"window": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  . ", "original_text": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . "}, "hash": "53f5d73827d22e241c88cf5fd985f70ebb8a0ae23a347a516156df0fe950a2c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 29068, "end_char_idx": 29071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1fe3feb-51f8-49a1-99d5-ff730a301dc0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  . ", "original_text": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f649cf8-e7a9-42cf-aa42-53589b033765", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The main goal of the experiments was to ensure testing of the examined models on broad range of datasets of specified configuration.  The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  . ", "original_text": "3. "}, "hash": "b9051c6a196a7d04739d3f69ad6b58a0905d1ca81cc066f5a12f550e27c7145b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d224383-2f56-4c6b-983b-bda0d8479e03", "node_type": "1", "metadata": {"window": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n", "original_text": ". "}, "hash": "11bbe95cc91af4b85b26ffc1444d8b6af378b3a5884cc726753b4f380bc7805d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . ", "mimetype": "text/plain", "start_char_idx": 29071, "end_char_idx": 29166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d224383-2f56-4c6b-983b-bda0d8479e03", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1fe3feb-51f8-49a1-99d5-ff730a301dc0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The artificial datasets were generated randomly, but in accordance with the following criteria:\n\n1.  Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  . ", "original_text": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, . "}, "hash": "9823a49eb4196e8e61f41afe5878378f907f75d178275dccfa42f83362174e32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "113fe66b-8abb-49e9-8007-3ccd789fb303", "node_type": "1", "metadata": {"window": "2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4. ", "original_text": ". "}, "hash": "d8e00bc5c687b63c8b95b9c116e21328074946f923c5fb13970d52fd3244e657", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 29164, "end_char_idx": 29166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "113fe66b-8abb-49e9-8007-3ccd789fb303", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d224383-2f56-4c6b-983b-bda0d8479e03", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Every dataset contained 5 non-intersecting clusters of random radius and outliers located in-between them.\n 2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n", "original_text": ". "}, "hash": "b1e0727df08648075387726f091f857085f1af01152a6dd0f5aed69198a31d02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3bc9c62-7485-4a4a-bea8-12595c94c552", "node_type": "1", "metadata": {"window": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n", "original_text": ", 0)\u1d40, (1, 1, . "}, "hash": "7b19aa6eecfbecc137c70ff1454041a1000e0a7b93929cf400fe83ea3ee70e77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 29166, "end_char_idx": 29168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b3bc9c62-7485-4a4a-bea8-12595c94c552", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n", "original_text": ", 0)\u1d40, (1, 1, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "113fe66b-8abb-49e9-8007-3ccd789fb303", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2.  Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4. ", "original_text": ". "}, "hash": "f745dfbcfab3e5be795ae62830c531ffee8e55467d1fcc58ced8c1f466d81dd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47017970-188e-4332-b4b4-682fc191274c", "node_type": "1", "metadata": {"window": "3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. ", "original_text": ". "}, "hash": "98330c1b6b199e48c98129d9a918fbdd69c2ac22bb37dfdd4696852fd32f0afd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", 0)\u1d40, (1, 1, . ", "mimetype": "text/plain", "start_char_idx": 29170, "end_char_idx": 29186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47017970-188e-4332-b4b4-682fc191274c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3bc9c62-7485-4a4a-bea8-12595c94c552", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Total number of data points in a dataset was equal to 5000 and the outlier percentage was 1% that gave 50 outliers.\n 3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n", "original_text": ", 0)\u1d40, (1, 1, . "}, "hash": "e5881921d4261f0849f5ceb0677618d5f62134ef0bf1b78e100bbad4f14fbb19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ce73fd2-3e70-4c69-aae5-23e870fd4666", "node_type": "1", "metadata": {"window": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. ", "original_text": ". "}, "hash": "ddd8ea7e15ecee3372f9ddbcfdc424cd9f748d574bb14a4e9ef6a0d686f9620a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 29184, "end_char_idx": 29186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ce73fd2-3e70-4c69-aae5-23e870fd4666", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47017970-188e-4332-b4b4-682fc191274c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.  All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. ", "original_text": ". "}, "hash": "d29f2c66441e639fd125833e324224b826b51ebf2b10455b82cfc4e05a76a623", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3767550-47ed-4235-ba35-b3c71a693a23", "node_type": "1", "metadata": {"window": ".  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets. ", "original_text": ", 1)\u1d40), where k is the number of dimensions.\n"}, "hash": "5358c7094777d815a6cf7f8aac3660e4c1e6f3d79e367575c33091d2057b3c26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 29186, "end_char_idx": 29188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3767550-47ed-4235-ba35-b3c71a693a23", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets. ", "original_text": ", 1)\u1d40), where k is the number of dimensions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce73fd2-3e70-4c69-aae5-23e870fd4666", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "All the points of a dataset were located in a unite hypercube with the main diagonal ((0, 0, .  .  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. ", "original_text": ". "}, "hash": "08d72e8fc1fee4fd5cc1eae9fe77a243fd7f35a994337d900faadee91741be8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05b970b7-1346-4aca-98dd-6606957beff4", "node_type": "1", "metadata": {"window": ".  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric. ", "original_text": "4. "}, "hash": "14f1f2a7935d75461d46e8d89d255904f8aa53a782b062f0b933a45eb72a2c54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", 1)\u1d40), where k is the number of dimensions.\n", "mimetype": "text/plain", "start_char_idx": 29190, "end_char_idx": 29235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05b970b7-1346-4aca-98dd-6606957beff4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric. ", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3767550-47ed-4235-ba35-b3c71a693a23", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  .  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets. ", "original_text": ", 1)\u1d40), where k is the number of dimensions.\n"}, "hash": "026ef793112e11b72aee8bc74c58af51934432402b4a9f80ce43dfd4bb40a590", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb84cc45-d41e-415f-b947-0ad818f7c86d", "node_type": "1", "metadata": {"window": ", 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. ", "original_text": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n"}, "hash": "0a69b34770ae4b3da60f7b548031286d51f7d388b83c39f81a218d11ee47e793", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 29235, "end_char_idx": 29238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb84cc45-d41e-415f-b947-0ad818f7c86d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ", 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. ", "original_text": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05b970b7-1346-4aca-98dd-6606957beff4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  , 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric. ", "original_text": "4. "}, "hash": "c5fbceab3f13bdfc6a95c76712115de10079de12af6936ee3868ff5b8ee7a44a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67e268e6-2fbf-4ca8-9f1a-10fbe172f837", "node_type": "1", "metadata": {"window": ".  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets. ", "original_text": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. "}, "hash": "94a6e0c810b9e2ebb1709b9524c9726b7e9eabd9895120076bc5361e15fb3616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n", "mimetype": "text/plain", "start_char_idx": 29238, "end_char_idx": 29328, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67e268e6-2fbf-4ca8-9f1a-10fbe172f837", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets. ", "original_text": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb84cc45-d41e-415f-b947-0ad818f7c86d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ", 0)\u1d40, (1, 1, .  .  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. ", "original_text": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n"}, "hash": "ed462da4cdbf2577b7c75e152980ec4dba30ea09cf0d2398044f6d3e6184ecd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80f5408b-1079-450d-8335-63d35a6f2673", "node_type": "1", "metadata": {"window": ".  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n", "original_text": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. "}, "hash": "f83eb8da55c985c8a1c40f06c50b5864caac3d87fbd2c09d429aceab3b9af77f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. ", "mimetype": "text/plain", "start_char_idx": 29328, "end_char_idx": 29566, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80f5408b-1079-450d-8335-63d35a6f2673", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n", "original_text": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67e268e6-2fbf-4ca8-9f1a-10fbe172f837", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  .  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets. ", "original_text": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}. "}, "hash": "b0333bbcb5e9e12c966bb394f804de596da595b2c691527eaa3c4a6e72070ee1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a0f242d-a660-4649-a3a7-b88cb0eebd01", "node_type": "1", "metadata": {"window": ", 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results. ", "original_text": "100 datasets were generated for every dimension, providing in total 800 datasets. "}, "hash": "be5515586c99e701d2ffb602e9a82ebb04eb74cf0b883289c3701835a1d77406", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. ", "mimetype": "text/plain", "start_char_idx": 29566, "end_char_idx": 29698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a0f242d-a660-4649-a3a7-b88cb0eebd01", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ", 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results. ", "original_text": "100 datasets were generated for every dimension, providing in total 800 datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80f5408b-1079-450d-8335-63d35a6f2673", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ".  , 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n", "original_text": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models. "}, "hash": "a6cb8d8161ea9f2a3c168121c583174eb1aa426a032cb6a792e97bf9f56a5ae4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f22b700-2e1c-4810-8562-e416d8238d31", "node_type": "1", "metadata": {"window": "4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig. ", "original_text": "ROC AUC was selected to be the quality metric. "}, "hash": "e673ffc1d1b7ea4c0a09539a5bab054f33c417959d2549c0addf94926c6b54aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "100 datasets were generated for every dimension, providing in total 800 datasets. ", "mimetype": "text/plain", "start_char_idx": 29698, "end_char_idx": 29780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f22b700-2e1c-4810-8562-e416d8238d31", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig. ", "original_text": "ROC AUC was selected to be the quality metric. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a0f242d-a660-4649-a3a7-b88cb0eebd01", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ", 1)\u1d40), where k is the number of dimensions.\n 4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results. ", "original_text": "100 datasets were generated for every dimension, providing in total 800 datasets. "}, "hash": "669c6f4672000e611996a3cb89b0dcb6b71a038286ca0505661bdb666f8c75bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "021dffbb-9d78-4cf4-9d16-f0d72dad871d", "node_type": "1", "metadata": {"window": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3. ", "original_text": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. "}, "hash": "1b06e417fc84ee7f4e02b31a9503eafa88c8a9409b93acf69b3ba2db94bca45a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ROC AUC was selected to be the quality metric. ", "mimetype": "text/plain", "start_char_idx": 29780, "end_char_idx": 29827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "021dffbb-9d78-4cf4-9d16-f0d72dad871d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3. ", "original_text": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f22b700-2e1c-4810-8562-e416d8238d31", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4.  Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig. ", "original_text": "ROC AUC was selected to be the quality metric. "}, "hash": "e386ad1587046f2d00c1515065e8b9bc895392ab0e4711c159ad5d09a56e991d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e26b954-5bda-4bd8-9993-9abebe6ac0d1", "node_type": "1", "metadata": {"window": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n", "original_text": "In most cases it has to be defined by an expert and may be different for various datasets. "}, "hash": "bc3923f2936f9ff502f8edc2112525103f8369e5867a91bed9b20b9b9ce99c37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. ", "mimetype": "text/plain", "start_char_idx": 29827, "end_char_idx": 29937, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e26b954-5bda-4bd8-9993-9abebe6ac0d1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n", "original_text": "In most cases it has to be defined by an expert and may be different for various datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "021dffbb-9d78-4cf4-9d16-f0d72dad871d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Outliers are located not closer than 0.05 (in sense of Euclidian metric) to any cluster.\n\n The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3. ", "original_text": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold. "}, "hash": "2fccbbd9c1e79933ab2e3ef81e2dfad4cc90574f670fc4b4959c0973290e237a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06716b42-d8e7-47ac-9a8c-002727aa14e6", "node_type": "1", "metadata": {"window": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). ", "original_text": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n"}, "hash": "2ce4633043f22196c5b587b77dd4b3fd94de0d233fdccdbe9b995d9c9121d704", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In most cases it has to be defined by an expert and may be different for various datasets. ", "mimetype": "text/plain", "start_char_idx": 29937, "end_char_idx": 30028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06716b42-d8e7-47ac-9a8c-002727aa14e6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). ", "original_text": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e26b954-5bda-4bd8-9993-9abebe6ac0d1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The ensembles were built with the following parameters:\na) Tree number (t): 100\nb) Sample number(n): 256\nc) Tree depth limit(l): log\u2082n = log\u2082256 = 8\n\nThe tests were conducted for the following dimension numbers: {2, 3, 4, 5, 6, 7, 8, 9}.  The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n", "original_text": "In most cases it has to be defined by an expert and may be different for various datasets. "}, "hash": "d87cff0bfdb97574bf8e85a0aa3bf418333075eb850319c6b891a813ee175f27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d01a2001-91b9-427f-bf41-f11eab5a2bf6", "node_type": "1", "metadata": {"window": "100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99. ", "original_text": "The experiments provided us with extensive results. "}, "hash": "a34a52464d4643988162753ade200d79af5f5311e38f53c615da225419d406bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n", "mimetype": "text/plain", "start_char_idx": 30028, "end_char_idx": 30150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d01a2001-91b9-427f-bf41-f11eab5a2bf6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99. ", "original_text": "The experiments provided us with extensive results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06716b42-d8e7-47ac-9a8c-002727aa14e6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The experiments for these dimension numbers revealed the greatest differences between performance of the basic and modified models.  100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). ", "original_text": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n"}, "hash": "7d5a29de80b17ad5664d79f12ce1daf5c684b2d290be414b8194b9d0028a7d3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8876d8b2-2e9c-4628-9aee-d3d8588fb8d1", "node_type": "1", "metadata": {"window": "ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9. ", "original_text": "The average values of AUC for the respective models are presented on the Fig. "}, "hash": "c8a2c90c3c47f62e2ed817c1443fa66f62060746d24ab0fff753802083b7f71e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiments provided us with extensive results. ", "mimetype": "text/plain", "start_char_idx": 30150, "end_char_idx": 30202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8876d8b2-2e9c-4628-9aee-d3d8588fb8d1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9. ", "original_text": "The average values of AUC for the respective models are presented on the Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d01a2001-91b9-427f-bf41-f11eab5a2bf6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "100 datasets were generated for every dimension, providing in total 800 datasets.  ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99. ", "original_text": "The experiments provided us with extensive results. "}, "hash": "14821b3772d91e76435009807f286f19db22c5e98324bf339ba79f422d1376fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83d8cd32-d571-4445-b9a9-07d35593a873", "node_type": "1", "metadata": {"window": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n", "original_text": "3. "}, "hash": "5a0eed07ffd68d9d9f7ce4fcfe8b978b4fc3a87db003b06c8fbcb0058b9ab1b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The average values of AUC for the respective models are presented on the Fig. ", "mimetype": "text/plain", "start_char_idx": 30202, "end_char_idx": 30280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83d8cd32-d571-4445-b9a9-07d35593a873", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8876d8b2-2e9c-4628-9aee-d3d8588fb8d1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "ROC AUC was selected to be the quality metric.  This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9. ", "original_text": "The average values of AUC for the respective models are presented on the Fig. "}, "hash": "5e9f410ab255613e2529f6da77d092f075f8c50005ca574eea077a627577773d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "828304ad-b489-498b-8d1f-ffd0a01f34ac", "node_type": "1", "metadata": {"window": "In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig. ", "original_text": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n"}, "hash": "18b1431d4074a63ddd40760b740fbc97f76efcf89869d6e8c78ff26b848a8a5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 30280, "end_char_idx": 30283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "828304ad-b489-498b-8d1f-ffd0a01f34ac", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig. ", "original_text": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83d8cd32-d571-4445-b9a9-07d35593a873", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This choice was made due to the fact that confusion matrix-based metrics require the anomaly score threshold.  In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n", "original_text": "3. "}, "hash": "13db270e93efe36c254fda4d8635270a90ca9e14c1775ee5997852aa870a12dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0d78316-2ed8-4bc4-aaef-08fcb3c4dced", "node_type": "1", "metadata": {"window": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3.", "original_text": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). "}, "hash": "114cb3b99acfbc135ce5cf8b5a9a1b6460d35b5a5a43d899468cc6f6fdb5e0db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n", "mimetype": "text/plain", "start_char_idx": 30283, "end_char_idx": 30486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a0d78316-2ed8-4bc4-aaef-08fcb3c4dced", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3.", "original_text": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "828304ad-b489-498b-8d1f-ffd0a01f34ac", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In most cases it has to be defined by an expert and may be different for various datasets.  Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig. ", "original_text": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n"}, "hash": "1769b219825a469614975e5c18786ee8e877f6e2e3aadb78305f33125926d0e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7323ed0c-15a4-497b-8930-9162d228fe89", "node_type": "1", "metadata": {"window": "The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n", "original_text": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99. "}, "hash": "5ae6d334ea713d5e69de94b8db9b40abd131a40dfd6dc1371ebc48330ecd644e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). ", "mimetype": "text/plain", "start_char_idx": 30486, "end_char_idx": 30596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7323ed0c-15a4-497b-8930-9162d228fe89", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n", "original_text": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0d78316-2ed8-4bc4-aaef-08fcb3c4dced", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, original paper on Isolation Forest [24], that we were basing our research on, also utilized solely AUC metric.\n\n The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3.", "original_text": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars). "}, "hash": "89a7bf42adf6d9f7504f3d675d1310660406deaf7befff340bc21f0dedb39738", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16044425-dc2c-41e3-afae-2a3213b7372d", "node_type": "1", "metadata": {"window": "The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig. ", "original_text": "The x-axis represents the number of dimensions from 2 to 9. "}, "hash": "475529e8674876cbb938d53c6616e73367e35c669f309e0c7cd94705b0ca97d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99. ", "mimetype": "text/plain", "start_char_idx": 30596, "end_char_idx": 30659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16044425-dc2c-41e3-afae-2a3213b7372d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig. ", "original_text": "The x-axis represents the number of dimensions from 2 to 9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7323ed0c-15a4-497b-8930-9162d228fe89", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The experiments provided us with extensive results.  The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n", "original_text": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99. "}, "hash": "a48329e41a20122d5aa86976e1574c50e7984d37c2ec2439060a5396464d5721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b195ffb-7966-4842-9156-72eb392a9392", "node_type": "1", "metadata": {"window": "3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF. ", "original_text": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n"}, "hash": "0b2b6604bdbd6870f398941956c0b6b49e5d6876855b129858a7941c72f8a2bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis represents the number of dimensions from 2 to 9. ", "mimetype": "text/plain", "start_char_idx": 30659, "end_char_idx": 30719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b195ffb-7966-4842-9156-72eb392a9392", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF. ", "original_text": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16044425-dc2c-41e3-afae-2a3213b7372d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The average values of AUC for the respective models are presented on the Fig.  3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig. ", "original_text": "The x-axis represents the number of dimensions from 2 to 9. "}, "hash": "780080d0440e0ed78b1b6fcc52443e9365622c16e57dd716eed62fe8069c7bea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82870455-7a39-4bf9-bbf8-f93b248617e7", "node_type": "1", "metadata": {"window": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n", "original_text": "**Fig. "}, "hash": "d7a0b5f4d7891261e66a301b05b748557b3f4b5876ceb087a27ed05d45841237", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n", "mimetype": "text/plain", "start_char_idx": 30719, "end_char_idx": 30828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82870455-7a39-4bf9-bbf8-f93b248617e7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b195ffb-7966-4842-9156-72eb392a9392", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.  As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF. ", "original_text": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n"}, "hash": "3738bf7473d16ddcb8f8955b712d56e181f38eac62f2d0941f62553453a00529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db3d156e-6613-4c93-8e7b-48611f3a5bac", "node_type": "1", "metadata": {"window": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. ", "original_text": "3."}, "hash": "870c32266b99e1b1a5cfef4c4c7c66bbb18fc1876c9e11525d2379c31ab77d55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 30828, "end_char_idx": 30835, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db3d156e-6613-4c93-8e7b-48611f3a5bac", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. ", "original_text": "3."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82870455-7a39-4bf9-bbf8-f93b248617e7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As it may be observed, the proposed model ensures superior performance compared to the original model for all the numbers of dimensions, particularly for two-, three-, four-, and five-dimensional data.\n\n ---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n", "original_text": "**Fig. "}, "hash": "1928f94156d8b3b99439800c238aa85210e5aefeb1d18df0f297baf13e99f041", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64a112ed-93ca-4929-80ca-b9d1ef3b1a44", "node_type": "1", "metadata": {"window": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig. ", "original_text": "** Mean AUC values for separate models for specified dimension numbers.\n"}, "hash": "0478aba41ecb397f68d97f9fa87f18e45a89fe8711cec2ce507431f3ce4b65a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.", "mimetype": "text/plain", "start_char_idx": 30835, "end_char_idx": 30837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64a112ed-93ca-4929-80ca-b9d1ef3b1a44", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig. ", "original_text": "** Mean AUC values for separate models for specified dimension numbers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db3d156e-6613-4c93-8e7b-48611f3a5bac", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 3: A bar chart comparing the performance of PGIF (blue bars) and IF (orange bars).  The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. ", "original_text": "3."}, "hash": "969d9b80fd6027d517aa27d171f255673ce53a87a3bcc86d2bb21f8dcc577ded", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39481424-15b4-45ba-991c-254dd9050041", "node_type": "1", "metadata": {"window": "The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets. ", "original_text": "---\n\nFig. "}, "hash": "0744e17f0d64f1eef3281e9f53322a2cfeaae3eb6917c1b2dc54ad8f6771f12e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Mean AUC values for separate models for specified dimension numbers.\n", "mimetype": "text/plain", "start_char_idx": 30837, "end_char_idx": 30909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39481424-15b4-45ba-991c-254dd9050041", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets. ", "original_text": "---\n\nFig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64a112ed-93ca-4929-80ca-b9d1ef3b1a44", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The y-axis represents the mean AUC, ranging from 0.85 to 0.99.  The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig. ", "original_text": "** Mean AUC values for separate models for specified dimension numbers.\n"}, "hash": "b72a752cf32b484de1b61e1cf7ef43e2740b70af21bd03f81b3611347c0a2ec7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8a5286f-4c18-43f5-8600-558ef82a203e", "node_type": "1", "metadata": {"window": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n", "original_text": "4 shows the comparison between mean time required for training an original IF and PGIF. "}, "hash": "ab88e6ec43665167d43e4f1edd25c3ceeb1651d5aaeec15697a436f57c51a0f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\nFig. ", "mimetype": "text/plain", "start_char_idx": 30909, "end_char_idx": 30919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8a5286f-4c18-43f5-8600-558ef82a203e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n", "original_text": "4 shows the comparison between mean time required for training an original IF and PGIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39481424-15b4-45ba-991c-254dd9050041", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The x-axis represents the number of dimensions from 2 to 9.  For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets. ", "original_text": "---\n\nFig. "}, "hash": "c3e856eba8479d8d0097dd1ad8d13e06f6ea0416f575fc3493d9a45645485793", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "245480b4-0bb0-439b-8171-982be1153d05", "node_type": "1", "metadata": {"window": "**Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig. ", "original_text": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n"}, "hash": "6cb9f896cec3f3aa859ba32c879dc0f817e240a096539183545ad829e6137d73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 shows the comparison between mean time required for training an original IF and PGIF. ", "mimetype": "text/plain", "start_char_idx": 30919, "end_char_idx": 31007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "245480b4-0bb0-439b-8171-982be1153d05", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig. ", "original_text": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8a5286f-4c18-43f5-8600-558ef82a203e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "For all dimensions, the blue bar (PGIF) is taller than the orange bar (IF), indicating better performance.]\n\n **Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n", "original_text": "4 shows the comparison between mean time required for training an original IF and PGIF. "}, "hash": "b7b9476805202250f229e8a43252011a903af1f4e83a9104d71572d9a6078bde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab3084e5-89da-45fb-9046-f3c42dd8b82e", "node_type": "1", "metadata": {"window": "3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. ", "original_text": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. "}, "hash": "6c41672c3e1441034167b52fb74300148289f8bca58d6426cb0a4cbf37b2d4c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 31007, "end_char_idx": 31189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab3084e5-89da-45fb-9046-f3c42dd8b82e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. ", "original_text": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "245480b4-0bb0-439b-8171-982be1153d05", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig. ", "original_text": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n"}, "hash": "43d9e82946ef2800487b43a18270659233fb32311c10c165c5bde64df4f272a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39500fda-477b-46c8-a5a5-e77de04fd28f", "node_type": "1", "metadata": {"window": "** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster. ", "original_text": "Fig. "}, "hash": "73a479affea0b3e3ef0b4d071eb5a3cf67baf4580610b0dd4d5f3c01342222c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. ", "mimetype": "text/plain", "start_char_idx": 31189, "end_char_idx": 31298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39500fda-477b-46c8-a5a5-e77de04fd28f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab3084e5-89da-45fb-9046-f3c42dd8b82e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3. ** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. ", "original_text": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets. "}, "hash": "3f65cdf5519d90fb8e0d9ec1682e39ff59a271b08ed6b128645154586c473a56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcdf50e1-618c-4734-808e-7ed4b8115e37", "node_type": "1", "metadata": {"window": "---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. ", "original_text": "5 presents a series of results obtained with example datasets. "}, "hash": "63064758a5ef46d7a8f553305b545f2b7006f8e1eb788319aa70e26cbf3f0c67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 31298, "end_char_idx": 31303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fcdf50e1-618c-4734-808e-7ed4b8115e37", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. ", "original_text": "5 presents a series of results obtained with example datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39500fda-477b-46c8-a5a5-e77de04fd28f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Mean AUC values for separate models for specified dimension numbers.\n ---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster. ", "original_text": "Fig. "}, "hash": "e350254da8713f4dadf490ba9b7ad01fe3e59789e370971058a164581e4e18d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b5443e6-97be-4dc0-94ee-95df6902d2c1", "node_type": "1", "metadata": {"window": "4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps. ", "original_text": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n"}, "hash": "41e4dc0323e7589ad89f1f4a7bcbb89b106b86ed6d71ebcf0804767a065ee363", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 presents a series of results obtained with example datasets. ", "mimetype": "text/plain", "start_char_idx": 31303, "end_char_idx": 31366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b5443e6-97be-4dc0-94ee-95df6902d2c1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps. ", "original_text": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcdf50e1-618c-4734-808e-7ed4b8115e37", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nFig.  4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. ", "original_text": "5 presents a series of results obtained with example datasets. "}, "hash": "baec5a86a594ca57bfe6cb10aa5c4e829ce836be81546c33960ea5464910f91e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e018234f-2ad4-470c-8ba3-525e8a6833f7", "node_type": "1", "metadata": {"window": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. ", "original_text": "The results presented on the Fig. "}, "hash": "96430730cf625f5b4af4c887194b7fc37783b97d1d79d64a054edbd4ef8e6383", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n", "mimetype": "text/plain", "start_char_idx": 31366, "end_char_idx": 31685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e018234f-2ad4-470c-8ba3-525e8a6833f7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. ", "original_text": "The results presented on the Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b5443e6-97be-4dc0-94ee-95df6902d2c1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4 shows the comparison between mean time required for training an original IF and PGIF.  It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps. ", "original_text": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n"}, "hash": "47ee8735c489acba5ce89702581d596db4bdc15c04993da3869ab00518d7b826", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e74d78c5-e297-4f91-ab96-864069586479", "node_type": "1", "metadata": {"window": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n", "original_text": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. "}, "hash": "0197a81fb7f5fa93b0cda547a085d95f2f2c374048ea8bbbc10e1079fb0e700e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results presented on the Fig. ", "mimetype": "text/plain", "start_char_idx": 31685, "end_char_idx": 31719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e74d78c5-e297-4f91-ab96-864069586479", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n", "original_text": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e018234f-2ad4-470c-8ba3-525e8a6833f7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is clearly seen that even in case of the greatest difference the proposed model is less than two times slower in training compared to the original IF on the artificial datasets.\n\n Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. ", "original_text": "The results presented on the Fig. "}, "hash": "c0f3cd0bbb2061043bd41864e5bbee26b098acd4799540e96539d95f59f3daa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "179c5736-2f03-487c-a077-e084608f21c2", "node_type": "1", "metadata": {"window": "Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). ", "original_text": "Note how the original IF treats the whole dataset as one large cluster. "}, "hash": "ef73e5c14a4ddcbafad86ea03f7688cef09b01da20895f9e576c39e671e4f70d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. ", "mimetype": "text/plain", "start_char_idx": 31719, "end_char_idx": 31816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "179c5736-2f03-487c-a077-e084608f21c2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). ", "original_text": "Note how the original IF treats the whole dataset as one large cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e74d78c5-e297-4f91-ab96-864069586479", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Very interesting insights can be observed from visualization of the anomaly scores of the analyzed datasets.  Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n", "original_text": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters. "}, "hash": "6989df74a45578cd5f718c1d0d1d6797ce104a3820f7b658ab3c8cdc6bd65c2b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a35e008-42a9-4d11-8244-ac69fecf160a", "node_type": "1", "metadata": {"window": "5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. ", "original_text": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. "}, "hash": "e2090635c4c519c49f6fe6f66e8a3602c23cc116420e26f5ff5a97861096ce4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note how the original IF treats the whole dataset as one large cluster. ", "mimetype": "text/plain", "start_char_idx": 31816, "end_char_idx": 31888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a35e008-42a9-4d11-8244-ac69fecf160a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. ", "original_text": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "179c5736-2f03-487c-a077-e084608f21c2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). ", "original_text": "Note how the original IF treats the whole dataset as one large cluster. "}, "hash": "f8a3421798bd32aac41cca8857271ca2390cfd25d63dcfb5a9317bb263223ff9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "522bf40d-a786-4be5-9238-27f96d5660d0", "node_type": "1", "metadata": {"window": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). ", "original_text": "It means that the low anomaly score was assigned even to the intra-cluster gaps. "}, "hash": "d0a0d1fbe06d4b15cc98cca7429d60bb1c1e5bb37b66020ba4d2ccc1266d2093", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. ", "mimetype": "text/plain", "start_char_idx": 31888, "end_char_idx": 32014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "522bf40d-a786-4be5-9238-27f96d5660d0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). ", "original_text": "It means that the low anomaly score was assigned even to the intra-cluster gaps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a35e008-42a9-4d11-8244-ac69fecf160a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5 presents a series of results obtained with example datasets.  Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. ", "original_text": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue. "}, "hash": "500a5c52cf06fd6e27134eb117b2ebc547fa50c18c5018b285919ba0f70101aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3623dbf4-80a6-4e19-8a98-c30adc5041ed", "node_type": "1", "metadata": {"window": "The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). ", "original_text": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. "}, "hash": "1468aa5b9888b89f71234dedebb0c8e6715f100c6eee3362fb70c07e5711be92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It means that the low anomaly score was assigned even to the intra-cluster gaps. ", "mimetype": "text/plain", "start_char_idx": 32014, "end_char_idx": 32095, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3623dbf4-80a6-4e19-8a98-c30adc5041ed", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). ", "original_text": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "522bf40d-a786-4be5-9238-27f96d5660d0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each example contains the illustrations of the dataset with the outliers marked red (1), anomaly score assigned to particular points (2), the normalized order numbers obtained after sorting the points according to their anomaly score (3) in ascending order, and heatmap of the anomaly score in the analyzed space (4).\n\n The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). ", "original_text": "It means that the low anomaly score was assigned even to the intra-cluster gaps. "}, "hash": "a9c6a68537818854589e074e3313c8d8bb9281c1cf44a067ac42f758e9fd8508", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aad04d83-8eb5-48f5-8d4b-c51a89bc85ea", "node_type": "1", "metadata": {"window": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. ", "original_text": "It can be seen from orange and red taints that can be observed in between clusters.\n\n"}, "hash": "9eda7997b89e4a4ce5e4114585dbe0a3be6307dca8c975be39137124065782f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. ", "mimetype": "text/plain", "start_char_idx": 32095, "end_char_idx": 32194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aad04d83-8eb5-48f5-8d4b-c51a89bc85ea", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. ", "original_text": "It can be seen from orange and red taints that can be observed in between clusters.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3623dbf4-80a6-4e19-8a98-c30adc5041ed", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The results presented on the Fig.  5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). ", "original_text": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions. "}, "hash": "178ba46da2f02384ef4c34eb3b1a4afba5317c58c8c8f3a01f1845ce8d7c9daa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb7affa4-f2a9-482b-8732-51d43e090dca", "node_type": "1", "metadata": {"window": "Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space. ", "original_text": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). "}, "hash": "d84f520a688e167ace447f8706dac4d323238c348d37e819936ca111f7c85448", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It can be seen from orange and red taints that can be observed in between clusters.\n\n", "mimetype": "text/plain", "start_char_idx": 32194, "end_char_idx": 32279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb7affa4-f2a9-482b-8732-51d43e090dca", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space. ", "original_text": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aad04d83-8eb5-48f5-8d4b-c51a89bc85ea", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5 show that PGIF performs favorably especially in the cases of datasets with extensive clusters.  Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. ", "original_text": "It can be seen from orange and red taints that can be observed in between clusters.\n\n"}, "hash": "b74cc9131f86220a540e7d565ef2175d7392a857872ee8153d584f03bd73ce07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "209673c3-1b77-4c6a-8ebe-9e6cc5182c65", "node_type": "1", "metadata": {"window": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. ", "original_text": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. "}, "hash": "fa9c86565e03ad6e8140300d077b44966ac216ebe78dc08779dfca935e0aeba2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). ", "mimetype": "text/plain", "start_char_idx": 32279, "end_char_idx": 32563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "209673c3-1b77-4c6a-8ebe-9e6cc5182c65", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. ", "original_text": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb7affa4-f2a9-482b-8732-51d43e090dca", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Note how the original IF treats the whole dataset as one large cluster.  It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space. ", "original_text": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B). "}, "hash": "b20dd081d457f4ff70c497f472ccc9bfbc901bf1d8c63ec887c4207135cd7efb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "063eff55-56a5-4f1e-96c4-d118911a4e0c", "node_type": "1", "metadata": {"window": "It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. ", "original_text": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). "}, "hash": "007284e418a2c74f78dc6e80357923b3fa763d53285757773abcb8b80f9cc735", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. ", "mimetype": "text/plain", "start_char_idx": 32563, "end_char_idx": 32682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "063eff55-56a5-4f1e-96c4-d118911a4e0c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. ", "original_text": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "209673c3-1b77-4c6a-8ebe-9e6cc5182c65", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It can be seen from the fact that the majority of the heatmap for every example of IF is colored with various shades of blue.  It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. ", "original_text": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF. "}, "hash": "b7c2f314492543954fa13578a7a757d7b16fa538ebc7260949b492a85dc09ae9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801f0ccd-2b24-40ba-b9e6-a95b1b3654eb", "node_type": "1", "metadata": {"window": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. ", "original_text": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). "}, "hash": "5ff972489d432cad8b2c3ae13a259df0b9a4b02676f0cafc0fb55f1c43577154", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). ", "mimetype": "text/plain", "start_char_idx": 32682, "end_char_idx": 32812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "801f0ccd-2b24-40ba-b9e6-a95b1b3654eb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. ", "original_text": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "063eff55-56a5-4f1e-96c4-d118911a4e0c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It means that the low anomaly score was assigned even to the intra-cluster gaps.  The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. ", "original_text": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red). "}, "hash": "daad3975fec4d9d69abf0765e0795482bd56d40c2e71a389fc6ca0d78f0b031a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f27e3a36-de65-45e8-9c83-11c05094f4d3", "node_type": "1", "metadata": {"window": "It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n", "original_text": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. "}, "hash": "d2f4e3490275649384fb43024812410cee6b9a32b1c28e70f012b90256d6837b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). ", "mimetype": "text/plain", "start_char_idx": 32812, "end_char_idx": 32918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f27e3a36-de65-45e8-9c83-11c05094f4d3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n", "original_text": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801f0ccd-2b24-40ba-b9e6-a95b1b3654eb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The generalized model, on the contrary, assigns higher anomaly score to the intra-cluster regions.  It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. ", "original_text": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C). "}, "hash": "f250cf08a850504fed3e87b3ed772bccc626fb5b8ec9ae147f93fae010572de8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2afe7eab-357e-4b5a-92b3-65a52e64fb7e", "node_type": "1", "metadata": {"window": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2. ", "original_text": "It has lower number of points and occupies notably less space. "}, "hash": "b045fe239574e7141b9cdb07018e31fd5ff7d8c18467c92bd71b478ef4d8b16a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. ", "mimetype": "text/plain", "start_char_idx": 32918, "end_char_idx": 33042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2afe7eab-357e-4b5a-92b3-65a52e64fb7e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2. ", "original_text": "It has lower number of points and occupies notably less space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f27e3a36-de65-45e8-9c83-11c05094f4d3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It can be seen from orange and red taints that can be observed in between clusters.\n\n Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n", "original_text": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly. "}, "hash": "4f3552b674d356d8aebdfbe47636eae7135516ad434b65159771208461bdeebd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cf9b6b5-8994-4161-954c-f4d6e0220ae2", "node_type": "1", "metadata": {"window": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. ", "original_text": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. "}, "hash": "fa51de2c009262ac5de950808186fb7efd0702a29173e3e8e71d26b2c5f2e148", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It has lower number of points and occupies notably less space. ", "mimetype": "text/plain", "start_char_idx": 33042, "end_char_idx": 33105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1cf9b6b5-8994-4161-954c-f4d6e0220ae2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. ", "original_text": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2afe7eab-357e-4b5a-92b3-65a52e64fb7e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Original IF tends to assign higher anomaly scores to the points close to the borders of the dataset and lower \u2013 to the data at the center, so that the evident outliers surrounded by clusters obtain low anomaly scores that can be noted clearly in the case of the datasets (A) and (B).  Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2. ", "original_text": "It has lower number of points and occupies notably less space. "}, "hash": "057970b5c1c26952e76886c1fac3b7bdb2ae11cea9cf6e31330403622a08eb69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b261acc-77b3-473f-bdf2-1d30946e59d2", "node_type": "1", "metadata": {"window": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results. ", "original_text": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. "}, "hash": "9de4874252958b2a15433f27b174b30df452de164f5118ac76fd786875e44b30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. ", "mimetype": "text/plain", "start_char_idx": 33105, "end_char_idx": 33199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b261acc-77b3-473f-bdf2-1d30946e59d2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results. ", "original_text": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cf9b6b5-8994-4161-954c-f4d6e0220ae2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Although being sparsely populated, this region at the center obtains low anomaly score (it is blue) in the case of IF.  Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. ", "original_text": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score. "}, "hash": "341f8420bc2cc3ac1e29fa35208e94b0667fa1d7780a48d9cc1780b578bac1a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8e48a28-2ae4-4adc-94be-1cfa2c7d5939", "node_type": "1", "metadata": {"window": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n", "original_text": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. "}, "hash": "f6b537bf2428f77377a3c2d8e19848b68d05a2c8d5973c8becd56237a19afc82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. ", "mimetype": "text/plain", "start_char_idx": 33199, "end_char_idx": 33333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8e48a28-2ae4-4adc-94be-1cfa2c7d5939", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n", "original_text": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b261acc-77b3-473f-bdf2-1d30946e59d2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Our model (PGIF) finds all the outliers hidden between clusters, assigning high anomaly score to them (they are marked with red).  The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results. ", "original_text": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows. "}, "hash": "be4e1bc516009150548a2bafe5dc483fa72fea38ad54d02c24e8361ba0723bb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef3316ce-ab33-4fe0-a056-070c37d86469", "node_type": "1", "metadata": {"window": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. ", "original_text": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n"}, "hash": "3978059635f6d03c206fad815837148888455e3d18f47b614e4b0a03292a5352", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. ", "mimetype": "text/plain", "start_char_idx": 33333, "end_char_idx": 33475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef3316ce-ab33-4fe0-a056-070c37d86469", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. ", "original_text": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8e48a28-2ae4-4adc-94be-1cfa2c7d5939", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The PGIF algorithm also assigns high anomaly score to the small cluster at the center of the dataset (C).  This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n", "original_text": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805. "}, "hash": "0e0abb522c2ce5a1c98405a7d55497c08d3343333e68d1e1714cacdb6a3247ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a95dcc4-0e5e-46ec-85dd-28aa067d9dc2", "node_type": "1", "metadata": {"window": "It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. ", "original_text": "### 4.2. "}, "hash": "294b8e7cb21e5d6ff6d93b1c729761e40b401cf71a2ec6f22dca0161024faa32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n", "mimetype": "text/plain", "start_char_idx": 33475, "end_char_idx": 33542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a95dcc4-0e5e-46ec-85dd-28aa067d9dc2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. ", "original_text": "### 4.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef3316ce-ab33-4fe0-a056-070c37d86469", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This result is perfectly reasonable as in comparison to the other clusters, the small cluster can be treated as an anomaly.  It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. ", "original_text": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n"}, "hash": "d33cff85b094de9eef16a6db27defd2ad3728423b63b205f2da8a04823ad8980", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73d786a6-1c0e-48e8-8932-abfd910d0095", "node_type": "1", "metadata": {"window": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid). ", "original_text": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. "}, "hash": "e66c7248cc7de9c67cf05e761d6a63ab3575d7c9ac3dc97af4ad33b21ff5d7aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.2. ", "mimetype": "text/plain", "start_char_idx": 33542, "end_char_idx": 33551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73d786a6-1c0e-48e8-8932-abfd910d0095", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid). ", "original_text": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a95dcc4-0e5e-46ec-85dd-28aa067d9dc2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It has lower number of points and occupies notably less space.  As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. ", "original_text": "### 4.2. "}, "hash": "67999886b5a6ec030176c09627ffb175b40e7618803cb43d0874fb7b05c1137f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b67f274-47d4-4cdf-991c-41b246ecc4bf", "node_type": "1", "metadata": {"window": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n", "original_text": "Such number of repetitions was set in order to obtain stable results. "}, "hash": "95a3e556c847cceb7f66fc7ca8f931307665921753e379ea2c979fb5ec2cac93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. ", "mimetype": "text/plain", "start_char_idx": 33551, "end_char_idx": 33730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b67f274-47d4-4cdf-991c-41b246ecc4bf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n", "original_text": "Such number of repetitions was set in order to obtain stable results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73d786a6-1c0e-48e8-8932-abfd910d0095", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for the original approach, the small cluster of the dataset (C) obtains low anomaly score.  Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid). ", "original_text": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset. "}, "hash": "628ecaf3b4810a5dc73314c7388247dea3aeba40f71a583d06beb7adbae4b9fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bccce9b0-368c-46bb-9805-72f0cc4fc53a", "node_type": "1", "metadata": {"window": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3. ", "original_text": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n"}, "hash": "914e26a9e9db519d3111af6d6f1fc15f18becc7dca1ba569e6c701a4725defb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such number of repetitions was set in order to obtain stable results. ", "mimetype": "text/plain", "start_char_idx": 33730, "end_char_idx": 33800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bccce9b0-368c-46bb-9805-72f0cc4fc53a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3. ", "original_text": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b67f274-47d4-4cdf-991c-41b246ecc4bf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Together with the two greater clusters at the right side of the dataset, it is treated as one stretched cluster as the heatmap shows.  As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n", "original_text": "Such number of repetitions was set in order to obtain stable results. "}, "hash": "48cd093e374c0f9f76bbe12e6b63248f56fbd0e059f99cb31c5f57a489942f4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05c124cc-6050-4748-93b8-aa128de7ac3d", "node_type": "1", "metadata": {"window": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. ", "original_text": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. "}, "hash": "19016947a0bce0a44bf1a25b61375b14d50871bdfd102438891bf9a97cd002ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 33800, "end_char_idx": 33882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05c124cc-6050-4748-93b8-aa128de7ac3d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. ", "original_text": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bccce9b0-368c-46bb-9805-72f0cc4fc53a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for quantitative difference, the basic IF algorithm allowed achieving the following AUCs for the respective datasets: 0.857, 0.887, 0.805.  PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3. ", "original_text": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n"}, "hash": "1161d82369598d8540e14df798a052b4c3e3c6c6e3987606983e544f9d640360", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ed70cc3-7af5-4a71-9f8e-5352edb55e36", "node_type": "1", "metadata": {"window": "### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). ", "original_text": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. "}, "hash": "1ed16876bc4a90305b35c02984ff1c7a8c437d3b8fe31566f029ae963457087f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. ", "mimetype": "text/plain", "start_char_idx": 33882, "end_char_idx": 33989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ed70cc3-7af5-4a71-9f8e-5352edb55e36", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). ", "original_text": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05c124cc-6050-4748-93b8-aa128de7ac3d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "PGIF demonstrated notably better performance: 0.978, 0.99, 0.992.\n\n ### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. ", "original_text": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2. "}, "hash": "661f2112a5cf67548069c818fafb8763009e95840f012b852a5ad2774a122880", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5a1f540-8128-439b-b0e5-f423e8c92686", "node_type": "1", "metadata": {"window": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800. ", "original_text": "The original model provided better results only for two datasets (Mammography, Annthyroid). "}, "hash": "598ffa4c4bea3fc40a9f7502168caebacd75f9cf70155548bf9afc34f5701414", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. ", "mimetype": "text/plain", "start_char_idx": 33989, "end_char_idx": 34175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5a1f540-8128-439b-b0e5-f423e8c92686", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800. ", "original_text": "The original model provided better results only for two datasets (Mammography, Annthyroid). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ed70cc3-7af5-4a71-9f8e-5352edb55e36", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.2.  Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). ", "original_text": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets. "}, "hash": "d2b94d0e7dc976e727db228bf4383681f2c051b61c181fb562da991793169b6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "167f6ba5-d7d8-4f4c-987f-9d3c6aa9f9ff", "node_type": "1", "metadata": {"window": "Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9. ", "original_text": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n"}, "hash": "feea8306f01a160fc2453634df46353c727f45751b2c39244c9c83be03f58ae4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The original model provided better results only for two datasets (Mammography, Annthyroid). ", "mimetype": "text/plain", "start_char_idx": 34175, "end_char_idx": 34267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "167f6ba5-d7d8-4f4c-987f-9d3c6aa9f9ff", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9. ", "original_text": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5a1f540-8128-439b-b0e5-f423e8c92686", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Experiments on real datasets: Piecewise probability density function\n\nIn this section, we present the result of exhaustive experiments including 30 repetitions for every dataset.  Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800. ", "original_text": "The original model provided better results only for two datasets (Mammography, Annthyroid). "}, "hash": "b11ff305a4f267a482e9fb9945a6bce42ea0707205ee6363802ff9beaa8fc381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84aca561-2d2c-4b6e-a77f-61fad42a090b", "node_type": "1", "metadata": {"window": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n", "original_text": "### 4.3. "}, "hash": "9dac389a9e32732be8c06399a0bb927808bb76ebaec83f25decb4523822d1137", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n", "mimetype": "text/plain", "start_char_idx": 34267, "end_char_idx": 34479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84aca561-2d2c-4b6e-a77f-61fad42a090b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n", "original_text": "### 4.3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "167f6ba5-d7d8-4f4c-987f-9d3c6aa9f9ff", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such number of repetitions was set in order to obtain stable results.  Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9. ", "original_text": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n"}, "hash": "52fd9783841658039e1fab7ccb517150d5a08e9559eb8f1ef15f4d1cc24c7ba3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cf6e3a4-f50f-45ef-a0d9-d72b72210815", "node_type": "1", "metadata": {"window": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig. ", "original_text": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. "}, "hash": "ebdc6cb0500c84dd20c049e4224a3e32fcdf837ebc43c4428637f50984b5ad76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.3. ", "mimetype": "text/plain", "start_char_idx": 34479, "end_char_idx": 34488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3cf6e3a4-f50f-45ef-a0d9-d72b72210815", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig. ", "original_text": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84aca561-2d2c-4b6e-a77f-61fad42a090b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Table 1 presents the basic information regarding the applied benchmark datasets.\n\n Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n", "original_text": "### 4.3. "}, "hash": "bae91920b50d044a38ab81d515c7700fb8f2ee652c7115552997e316c5e4d2d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71e6dab5-883c-4f87-b5fa-b481d4cab5d4", "node_type": "1", "metadata": {"window": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4.", "original_text": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). "}, "hash": "d51302b07fa8e8ca4d6256f672343c6fd27df026263a4d0373eb87f15e058253", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. ", "mimetype": "text/plain", "start_char_idx": 34488, "end_char_idx": 34677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71e6dab5-883c-4f87-b5fa-b481d4cab5d4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4.", "original_text": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cf6e3a4-f50f-45ef-a0d9-d72b72210815", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Table 2 contains the results obtained with uniform function used for random value generation and k = 1, 2.  The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig. ", "original_text": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2. "}, "hash": "11a26f340e75025c4907630890ebea60c05f394f9c4dd1a62bb0b381037aa174", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5caaffe-18dc-45a9-b3be-73537c5e1bcb", "node_type": "1", "metadata": {"window": "The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n", "original_text": "The y-axis is \"Mean training time (ms)\" from 0 to 800. "}, "hash": "dc4bf340dbb6d0397875a4dfc34dc903b9cd4253424aca2838110c62e100a6ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). ", "mimetype": "text/plain", "start_char_idx": 34677, "end_char_idx": 34901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5caaffe-18dc-45a9-b3be-73537c5e1bcb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n", "original_text": "The y-axis is \"Mean training time (ms)\" from 0 to 800. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71e6dab5-883c-4f87-b5fa-b481d4cab5d4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The presented results show that the modified method allows achieving superior results in the majority of cases (8/14), it provided better or not worse results for 12 out of 14 datasets.  The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4.", "original_text": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars). "}, "hash": "314ba7e7c061d4808b60092e1d5133dec0eab7feb29ec85794e6972825b9bdd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4ae4d42-3d1c-45a9-8352-e870f4c40ee1", "node_type": "1", "metadata": {"window": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. ", "original_text": "The x-axis is \"Number of dimensions\" from 2 to 9. "}, "hash": "bbb364d2e286a52220483dfc12d380f6228c2834c7a47869add9d820f01909c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis is \"Mean training time (ms)\" from 0 to 800. ", "mimetype": "text/plain", "start_char_idx": 34901, "end_char_idx": 34956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4ae4d42-3d1c-45a9-8352-e870f4c40ee1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. ", "original_text": "The x-axis is \"Number of dimensions\" from 2 to 9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5caaffe-18dc-45a9-b3be-73537c5e1bcb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The original model provided better results only for two datasets (Mammography, Annthyroid).  In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n", "original_text": "The y-axis is \"Mean training time (ms)\" from 0 to 800. "}, "hash": "8603fbc03c40435cce4a1ad5e7f41420234ebf765c80609531c1124a327f4c71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c378f52d-b843-45bd-96a6-bba94765f44f", "node_type": "1", "metadata": {"window": "### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. ", "original_text": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n"}, "hash": "510880ff0c85c56f64716ae0373dda7460658a9254a3fddda0fa1d079903841d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Number of dimensions\" from 2 to 9. ", "mimetype": "text/plain", "start_char_idx": 34956, "end_char_idx": 35006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c378f52d-b843-45bd-96a6-bba94765f44f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. ", "original_text": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4ae4d42-3d1c-45a9-8352-e870f4c40ee1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of 4 datasets the values of AUC were equal for the both approaches, however, in 3 cases AUCs were fairly close to 1, leaving little room for improvement (Http \u2013 0.994, Shuttle \u2013 0.997, Breastw \u2013 0.988).\n\n ### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. ", "original_text": "The x-axis is \"Number of dimensions\" from 2 to 9. "}, "hash": "9401e4df32ca9661b7c07210e86642fba2c12613edb6ab658f625da379c033ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "877079e7-441a-478d-b6a4-a0d935b97640", "node_type": "1", "metadata": {"window": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. ", "original_text": "**Fig. "}, "hash": "a93722cae20a6b50522ba88390268a8be4175e27801078fdcfe7b838534e9594", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n", "mimetype": "text/plain", "start_char_idx": 35006, "end_char_idx": 35148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "877079e7-441a-478d-b6a4-a0d935b97640", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c378f52d-b843-45bd-96a6-bba94765f44f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.3.  Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. ", "original_text": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n"}, "hash": "7fc1b946d766f915095e56f5d79f304db100d3a709dab4be65069c6415487053", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79da953c-d7e5-4c2c-9839-350c2f93914c", "node_type": "1", "metadata": {"window": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). ", "original_text": "4."}, "hash": "1fa29e01458d8abd213eaeb8e179d6e33240adac755ef41067268af9296364ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 35148, "end_char_idx": 35155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "79da953c-d7e5-4c2c-9839-350c2f93914c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). ", "original_text": "4."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "877079e7-441a-478d-b6a4-a0d935b97640", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Analysis of the obtained results\n\nAn interesting point, is the fact that the original IF method shows superior performance in case of two datasets: Annthyroid and Mammography, see Table 2.  It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. ", "original_text": "**Fig. "}, "hash": "d19f652c810ff4b799d98f3a03edea3326f5c858f8dc845458883d48b3b8e065", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0980c0b3-768c-4d0f-8cda-130653f51891", "node_type": "1", "metadata": {"window": "The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores. ", "original_text": "** Comparison of mean training time for various dimension numbers.\n"}, "hash": "9a4f10828fce6aa90b2757a5364ea94af56e90530c42622b64135989c9cb30a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4.", "mimetype": "text/plain", "start_char_idx": 35155, "end_char_idx": 35157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0980c0b3-768c-4d0f-8cda-130653f51891", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores. ", "original_text": "** Comparison of mean training time for various dimension numbers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79da953c-d7e5-4c2c-9839-350c2f93914c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It is worth special consideration as it is important not only to have better performance\n\n---\n[Description of Figure 4: A bar chart comparing the mean training time in milliseconds for PGIF (blue bars) and IF (orange bars).  The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). ", "original_text": "4."}, "hash": "d1e4e7137e0dbada62dea4dabbd58ca93c67a05bfd3787b24b82158ef1900ec4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6142472e-b0a8-41b7-a775-b2758e641621", "node_type": "1", "metadata": {"window": "The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space. ", "original_text": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. "}, "hash": "a3529dfd37c359dce3de4d1dfa10092a362c81ccfaa648ed6b7202f8da5bdfdc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Comparison of mean training time for various dimension numbers.\n", "mimetype": "text/plain", "start_char_idx": 35157, "end_char_idx": 35224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6142472e-b0a8-41b7-a775-b2758e641621", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space. ", "original_text": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0980c0b3-768c-4d0f-8cda-130653f51891", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The y-axis is \"Mean training time (ms)\" from 0 to 800.  The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores. ", "original_text": "** Comparison of mean training time for various dimension numbers.\n"}, "hash": "d4bf95936472ef4aaa1f301a683b1cb6741a4c082b5fde5aaa44853d916d2d45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f39f7134-14b6-4855-a97f-1d4cbd13213c", "node_type": "1", "metadata": {"window": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n", "original_text": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. "}, "hash": "e3debf8efd59a771669b22d3629401a2f2aa17c54a365af527e34269b71aac89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. ", "mimetype": "text/plain", "start_char_idx": 35224, "end_char_idx": 35393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f39f7134-14b6-4855-a97f-1d4cbd13213c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n", "original_text": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6142472e-b0a8-41b7-a775-b2758e641621", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The x-axis is \"Number of dimensions\" from 2 to 9.  In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space. ", "original_text": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset. "}, "hash": "a91b4ae2ab73ab824b75ee0a0bca2acb3832ea6e4821dc9c467909653415ac60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d7e8f4b-a5ca-4b42-9df7-78b55aa99839", "node_type": "1", "metadata": {"window": "**Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig. ", "original_text": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. "}, "hash": "195b08c2f6f805bc0e592eb31bf6840d2cf6c503a367841ffa16a953cc03d6fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. ", "mimetype": "text/plain", "start_char_idx": 35393, "end_char_idx": 35501, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d7e8f4b-a5ca-4b42-9df7-78b55aa99839", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig. ", "original_text": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f39f7134-14b6-4855-a97f-1d4cbd13213c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In all cases, the blue bar (PGIF) is taller than the orange bar (IF), indicating a longer training time, though generally less than double.]\n\n **Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n", "original_text": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model. "}, "hash": "dd8452a0ccad5ca8871b2f5b7b4a7ff84168fd85f9ae73768f01cd9bc64b8157", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0890508e-d5a4-4ca5-b626-a317b2c13d80", "node_type": "1", "metadata": {"window": "4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5.", "original_text": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). "}, "hash": "1b2a5aeb3e8879bda01df197650134a09cb85142e8e5a6c0498245c5bd364b00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. ", "mimetype": "text/plain", "start_char_idx": 35501, "end_char_idx": 35598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0890508e-d5a4-4ca5-b626-a317b2c13d80", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5.", "original_text": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d7e8f4b-a5ca-4b42-9df7-78b55aa99839", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig. ", "original_text": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red. "}, "hash": "0e06595a6d7ec0d6fbb3082f22fb28335042caaf65ec75d8074b8413d7996a7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22602828-9ade-4fd8-9f67-b3ed5a2c3804", "node_type": "1", "metadata": {"window": "** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF. ", "original_text": "(3) A plot of sorted anomaly scores. "}, "hash": "7e9e073afa9e16e21b3af5df5a907042eeedc9dec2eb39d9ec3bf401e6efbf40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). ", "mimetype": "text/plain", "start_char_idx": 35598, "end_char_idx": 35705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22602828-9ade-4fd8-9f67-b3ed5a2c3804", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF. ", "original_text": "(3) A plot of sorted anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0890508e-d5a4-4ca5-b626-a317b2c13d80", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "4. ** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5.", "original_text": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high). "}, "hash": "dbf6c8e5feada6a983530cc9e973a8d23dcf17f040dae6cad9b5bdfb192224ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4646b98-4c97-49ed-b775-9f7d54372fa5", "node_type": "1", "metadata": {"window": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. ", "original_text": "(4) A heatmap of anomaly scores over the 2D space. "}, "hash": "8bf569e8588f997ea891256666b703336e04a04a9bb3d7d40ec162b0e224c99f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(3) A plot of sorted anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 35705, "end_char_idx": 35742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4646b98-4c97-49ed-b775-9f7d54372fa5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. ", "original_text": "(4) A heatmap of anomaly scores over the 2D space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22602828-9ade-4fd8-9f67-b3ed5a2c3804", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Comparison of mean training time for various dimension numbers.\n ---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF. ", "original_text": "(3) A plot of sorted anomaly scores. "}, "hash": "30657f1c03b97a1659d2723c5f4cd2c25cecc104d88ddf6085ec1d6eacce88fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b541b620-099b-436b-a53c-249f32b0b520", "node_type": "1", "metadata": {"window": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. ", "original_text": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n"}, "hash": "b1f0d231edbb4c03866b0bbaf9192b7c0f239e8c8747f7641b7579d8318ab9fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(4) A heatmap of anomaly scores over the 2D space. ", "mimetype": "text/plain", "start_char_idx": 35742, "end_char_idx": 35793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b541b620-099b-436b-a53c-249f32b0b520", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. ", "original_text": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4646b98-4c97-49ed-b775-9f7d54372fa5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 5: A large, multi-panel figure organized into three sections labeled (A), (B), and (C), each corresponding to a different artificial dataset.  Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. ", "original_text": "(4) A heatmap of anomaly scores over the 2D space. "}, "hash": "e74a139fce22c2acb0910557610c7fe080f1be561ba0563a8af44bfc7eaadb3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb5936be-087d-4a49-b1bd-ed6fd7e88b81", "node_type": "1", "metadata": {"window": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "original_text": "**Fig. "}, "hash": "ba28938ae16dfe89f48d77dcd789da910db7702d7e9362ad070d01262382261f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n", "mimetype": "text/plain", "start_char_idx": 35793, "end_char_idx": 36028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb5936be-087d-4a49-b1bd-ed6fd7e88b81", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b541b620-099b-436b-a53c-249f32b0b520", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each section contains two rows of visualizations, one for the PGIF model and one for the original IF model.  Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. ", "original_text": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n"}, "hash": "b63ecc22502591db15ae1fffc8772ef666083d6c7068e2dbec52efa5d4351437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4894ed1c-7b16-4fc9-9986-9d7bbfad6a64", "node_type": "1", "metadata": {"window": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it. ", "original_text": "5."}, "hash": "9865ce1d59b11db0e66e7bda01cb31f942e2b9d0a9d35a19b66930104c8e8848", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 36028, "end_char_idx": 36035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4894ed1c-7b16-4fc9-9986-9d7bbfad6a64", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it. ", "original_text": "5."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb5936be-087d-4a49-b1bd-ed6fd7e88b81", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Each row consists of four panels: (1) A scatter plot of the dataset with outliers marked in red.  (2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "original_text": "**Fig. "}, "hash": "f4a7ae03b0fb8f092f9702aa6c412650316964d9fa6cca3db6da1401c6fa203a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28c02b7d-5634-49a5-9be0-1f29c2976fda", "node_type": "1", "metadata": {"window": "(3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. ", "original_text": "** Comparison of the anomaly scores produced by original IF and PGIF. "}, "hash": "810707ca43d03d5667932eea07d4ab66f6fb9bc14afc77f9b0ab09ba1174a19d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.", "mimetype": "text/plain", "start_char_idx": 36035, "end_char_idx": 36037, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28c02b7d-5634-49a5-9be0-1f29c2976fda", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. ", "original_text": "** Comparison of the anomaly scores produced by original IF and PGIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4894ed1c-7b16-4fc9-9986-9d7bbfad6a64", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(2) A scatter plot where points are colored according to their anomaly score (blue for low, red for high).  (3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it. ", "original_text": "5."}, "hash": "4537f9ccf9d5097df7ac13668e5b3751f565f2c05e3692399343ccc59cac91f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e431190-5158-49a4-89f6-7e9b5ce4f3b4", "node_type": "1", "metadata": {"window": "(4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". ", "original_text": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. "}, "hash": "0483a778b63a85d1c990f9c01e5fdb1574dd00581ce0ed4807de2bc84661a1e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Comparison of the anomaly scores produced by original IF and PGIF. ", "mimetype": "text/plain", "start_char_idx": 36037, "end_char_idx": 36107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e431190-5158-49a4-89f6-7e9b5ce4f3b4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". ", "original_text": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28c02b7d-5634-49a5-9be0-1f29c2976fda", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(3) A plot of sorted anomaly scores.  (4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. ", "original_text": "** Comparison of the anomaly scores produced by original IF and PGIF. "}, "hash": "991b2e52db7ee6f3dfb487c10429061050b35439013a8c22aef105f1da732300", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd10cccd-8c6e-4d6b-9303-b9daea408259", "node_type": "1", "metadata": {"window": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect. ", "original_text": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. "}, "hash": "e3f39dc8d23b00031dd05f79cab3b43a9d3c0aab25735cc3bf3b78b5a0a22cea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. ", "mimetype": "text/plain", "start_char_idx": 36107, "end_char_idx": 36221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd10cccd-8c6e-4d6b-9303-b9daea408259", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect. ", "original_text": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e431190-5158-49a4-89f6-7e9b5ce4f3b4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(4) A heatmap of anomaly scores over the 2D space.  The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". ", "original_text": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset. "}, "hash": "aedaceae291e291b49b8fc098602a42abfa074a15fc86bb179d05dbc31634f5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b53daa41-63b0-44b9-b2f1-d012e3144b3e", "node_type": "1", "metadata": {"window": "**Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. ", "original_text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n"}, "hash": "7ca4b3053b9f6d323bf694c1e95a8852e6e01143f832fae8787166186eeeda43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. ", "mimetype": "text/plain", "start_char_idx": 36221, "end_char_idx": 36476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b53daa41-63b0-44b9-b2f1-d012e3144b3e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. ", "original_text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd10cccd-8c6e-4d6b-9303-b9daea408259", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The visualizations show that PGIF assigns higher anomaly scores to the spaces between clusters, effectively identifying hidden outliers, whereas IF tends to assign low scores to the center of the dataset regardless of point density.]\n\n **Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect. ", "original_text": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space. "}, "hash": "05a49fb171c2da3a1d32ac9c1e78c46e1f526565bdc88a94b16bd4c079c5e48d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3defe92a-89c2-4d09-abd6-673c7a4d2e13", "node_type": "1", "metadata": {"window": "5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions. ", "original_text": "---\n\nwith specific methods but also to understand why they allow to achieve it. "}, "hash": "8bf32a6dd2ca10a39fe0736cd31f4a8fa487465e454a032780722f1323c7dc79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "mimetype": "text/plain", "start_char_idx": 36476, "end_char_idx": 36607, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3defe92a-89c2-4d09-abd6-673c7a4d2e13", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions. ", "original_text": "---\n\nwith specific methods but also to understand why they allow to achieve it. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b53daa41-63b0-44b9-b2f1-d012e3144b3e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. ", "original_text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n"}, "hash": "abdbffcdc93347cc088d41ed5180bdcb8966b191130e50a2864a3b3a8d916d82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b20788c-7ab2-4d12-a8dd-3e045ac37243", "node_type": "1", "metadata": {"window": "** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers. ", "original_text": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. "}, "hash": "4980fa0517fa9e45de46784536cf54fd1916d57001434d7720c88ec9e395e8a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\nwith specific methods but also to understand why they allow to achieve it. ", "mimetype": "text/plain", "start_char_idx": 36607, "end_char_idx": 36687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b20788c-7ab2-4d12-a8dd-3e045ac37243", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers. ", "original_text": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3defe92a-89c2-4d09-abd6-673c7a4d2e13", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5. ** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions. ", "original_text": "---\n\nwith specific methods but also to understand why they allow to achieve it. "}, "hash": "04393dad934b4ddc302b9a25b37af5eec70f6501d3e3c824bf9d16c66ef8b181", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c360bdbe-905d-4a61-a9c6-77514e113b7d", "node_type": "1", "metadata": {"window": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other. ", "original_text": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". "}, "hash": "a3a71c9c6d9e934dd44d58b4da01f8f505224845dae7fd3a0566f5951a2d41a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. ", "mimetype": "text/plain", "start_char_idx": 36687, "end_char_idx": 36847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c360bdbe-905d-4a61-a9c6-77514e113b7d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other. ", "original_text": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b20788c-7ab2-4d12-a8dd-3e045ac37243", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Comparison of the anomaly scores produced by original IF and PGIF.  Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers. ", "original_text": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24]. "}, "hash": "983abea84741f34c3cd199d1fa3fc5783b341c40565a43759bd2ebcde98ded41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2db18ad-4a64-4fd9-bf79-bc86d6beedbd", "node_type": "1", "metadata": {"window": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n", "original_text": "The proposed generalization has its goal in maximizing of this effect. "}, "hash": "843b0ee5d8b8dd70252e5ab2999d0f651b2bc1f7d0e67bcfde6c8249f9980d1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". ", "mimetype": "text/plain", "start_char_idx": 36847, "end_char_idx": 37036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2db18ad-4a64-4fd9-bf79-bc86d6beedbd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n", "original_text": "The proposed generalization has its goal in maximizing of this effect. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c360bdbe-905d-4a61-a9c6-77514e113b7d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Every group of 7 figures rounded with a black frame shows the results obtained for a separate artificial dataset.  For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other. ", "original_text": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\". "}, "hash": "c1d1de2ba68a8fdb744a357bf2284f83d6e3c73db5ea32acca8245b90cae2367", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fe3c9d9-facf-4441-979e-d252cf24ddbf", "node_type": "1", "metadata": {"window": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered. ", "original_text": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. "}, "hash": "3e9210761a0f475eba64f452b5ddf2a65569bb942382b0d5a328ffaac2520d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed generalization has its goal in maximizing of this effect. ", "mimetype": "text/plain", "start_char_idx": 37036, "end_char_idx": 37107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fe3c9d9-facf-4441-979e-d252cf24ddbf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered. ", "original_text": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2db18ad-4a64-4fd9-bf79-bc86d6beedbd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "For every row: (1) the analyzed dataset with the outliers marked with red, (2) point anomaly scores, (3) order numbers of the points sorted ascendingly in accordance with their anomaly score, (4) heatmap of the anomaly score values in the analyzed space.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n", "original_text": "The proposed generalization has its goal in maximizing of this effect. "}, "hash": "51f027f4574a91284fabd360244a91dc5b6b01e11e28af1a38f181ea4d20f61e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "744aa78f-eb2f-448b-8831-388186bbfe91", "node_type": "1", "metadata": {"window": "---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible. ", "original_text": "The outliers are supposed to populate these regions. "}, "hash": "136475de5a3acef7432e964f4638666ad12802cb0d2d0f655e039cda7082bf4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. ", "mimetype": "text/plain", "start_char_idx": 37107, "end_char_idx": 37232, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "744aa78f-eb2f-448b-8831-388186bbfe91", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible. ", "original_text": "The outliers are supposed to populate these regions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fe3c9d9-facf-4441-979e-d252cf24ddbf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered. ", "original_text": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found. "}, "hash": "935600ef654eed20da8048242b36bf0b7508e0c85c348c587d1ac385bf52011b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d478f90-5c96-48ea-97dc-894e903bace8", "node_type": "1", "metadata": {"window": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric. ", "original_text": "Generally speaking, they have to be far from inliers. "}, "hash": "0167c9c6c072a3349cf137b812674c8c4afc109612d3a1fa30e67f347fd52a4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The outliers are supposed to populate these regions. ", "mimetype": "text/plain", "start_char_idx": 37232, "end_char_idx": 37285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d478f90-5c96-48ea-97dc-894e903bace8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric. ", "original_text": "Generally speaking, they have to be far from inliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "744aa78f-eb2f-448b-8831-388186bbfe91", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nwith specific methods but also to understand why they allow to achieve it.  As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible. ", "original_text": "The outliers are supposed to populate these regions. "}, "hash": "83692640b37fcf70300ef123377992a37f1d3d6d73eb231866dc23927e623770", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "319d1af8-d8f6-4327-8af1-e2b301ad2a27", "node_type": "1", "metadata": {"window": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n", "original_text": "On the other hand, the inliers, have to be similar to each other. "}, "hash": "272e73643f7f5e0f7773fc6159aba5500257c70a58eeeca1ef435eeec40e6076", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generally speaking, they have to be far from inliers. ", "mimetype": "text/plain", "start_char_idx": 37285, "end_char_idx": 37339, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "319d1af8-d8f6-4327-8af1-e2b301ad2a27", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n", "original_text": "On the other hand, the inliers, have to be similar to each other. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d478f90-5c96-48ea-97dc-894e903bace8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As the authors of the original method state, the main idea behind the Isolation Forest algorithm is the assumption that anomalies are \"few and different\" [24].  When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric. ", "original_text": "Generally speaking, they have to be far from inliers. "}, "hash": "554b3ff972a51804927a937089b7aa624340a5c0c2f2095cc72a45994ef0adf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0c30831-0210-47bb-a442-e272952adb09", "node_type": "1", "metadata": {"window": "The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n", "original_text": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n"}, "hash": "d99a1830c5f329a638982068cade5ae6dce10cdc2e597a18fac13a237edddc47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, the inliers, have to be similar to each other. ", "mimetype": "text/plain", "start_char_idx": 37339, "end_char_idx": 37405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0c30831-0210-47bb-a442-e272952adb09", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n", "original_text": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "319d1af8-d8f6-4327-8af1-e2b301ad2a27", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "When it comes to space intuition one may formulate this assumption in the following way: \"The anomalies are the points that are located far from the inliers in the analyzed feature space\".  The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n", "original_text": "On the other hand, the inliers, have to be similar to each other. "}, "hash": "30a9ae9dc75084e83fa221b2d843d3042be1fac56e8e193f63825061573e2fe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "328e6cc9-d203-4d60-bcbf-93a4d07fecd8", "node_type": "1", "metadata": {"window": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n", "original_text": "Now, we can take a look at the real datasets considered. "}, "hash": "b28dd9de9280321619e1d53c5fd3f3b3738c7d6450b331fb205a4c180e017e8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n", "mimetype": "text/plain", "start_char_idx": 37405, "end_char_idx": 37504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "328e6cc9-d203-4d60-bcbf-93a4d07fecd8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n", "original_text": "Now, we can take a look at the real datasets considered. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0c30831-0210-47bb-a442-e272952adb09", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization has its goal in maximizing of this effect.  Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n", "original_text": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n"}, "hash": "0a1f4920815821c6fe4dce44836ec347e7623d9262a5983d3277a266d2f2d9ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a7d37e0-29d7-45ee-9f2d-aff92cd293da", "node_type": "1", "metadata": {"window": "The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig. ", "original_text": "All of them contain highly-dimensional data, so straight graphic examination is not possible. "}, "hash": "40f9a54694f3085a298ca981aec9196c93ab4ea435701ee27d173ed29d553065", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Now, we can take a look at the real datasets considered. ", "mimetype": "text/plain", "start_char_idx": 37504, "end_char_idx": 37561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a7d37e0-29d7-45ee-9f2d-aff92cd293da", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig. ", "original_text": "All of them contain highly-dimensional data, so straight graphic examination is not possible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "328e6cc9-d203-4d60-bcbf-93a4d07fecd8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Introducing generalized algorithm we want to ensure that sparsely populated regions of the analyzed feature space are found.  The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n", "original_text": "Now, we can take a look at the real datasets considered. "}, "hash": "0b43b91cf7d23a70e860e746eb70d6939c975a583502918923313581bf1157c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdafad59-f1ae-4248-8967-07735c83df89", "node_type": "1", "metadata": {"window": "Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. ", "original_text": "Instead, we will introduce simple intuitive metric. "}, "hash": "7b8206bd6beaeb6d254e4d3b16e9b000d80ba7e10a07c037fe37223ff9ecfc3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All of them contain highly-dimensional data, so straight graphic examination is not possible. ", "mimetype": "text/plain", "start_char_idx": 37561, "end_char_idx": 37655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdafad59-f1ae-4248-8967-07735c83df89", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. ", "original_text": "Instead, we will introduce simple intuitive metric. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a7d37e0-29d7-45ee-9f2d-aff92cd293da", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The outliers are supposed to populate these regions.  Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig. ", "original_text": "All of them contain highly-dimensional data, so straight graphic examination is not possible. "}, "hash": "512e69bd20a68227b8dab861fdf15e1641103feb930991a53088c8d17058f81d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "730473a4-3647-4c8d-a00c-c64a7f51c650", "node_type": "1", "metadata": {"window": "On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. ", "original_text": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n"}, "hash": "6caff52eef0e2e128cd10b88fd455b8f43aae0b6f4cc16a917ab14a68e2a8461", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Instead, we will introduce simple intuitive metric. ", "mimetype": "text/plain", "start_char_idx": 37655, "end_char_idx": 37707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "730473a4-3647-4c8d-a00c-c64a7f51c650", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. ", "original_text": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdafad59-f1ae-4248-8967-07735c83df89", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Generally speaking, they have to be far from inliers.  On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. ", "original_text": "Instead, we will introduce simple intuitive metric. "}, "hash": "8c505fc8b83851a962dda19504c477b52e725144921514289a85f1ecfac5a8b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4f1d0d7-d1ef-4d47-892f-18465ead8100", "node_type": "1", "metadata": {"window": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig. ", "original_text": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n"}, "hash": "d4a5538d89b385d7b6eb084aba35711ae429328e7777a62586b3dbc17d3586e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n", "mimetype": "text/plain", "start_char_idx": 37707, "end_char_idx": 38049, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4f1d0d7-d1ef-4d47-892f-18465ead8100", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig. ", "original_text": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "730473a4-3647-4c8d-a00c-c64a7f51c650", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "On the other hand, the inliers, have to be similar to each other.  It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. ", "original_text": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n"}, "hash": "328bb012be090b2fe9b88d153dde227e44c6bdd7b59761fd02633c4f6a044f68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82a9d562-781d-4633-aedd-177c8a3e2e84", "node_type": "1", "metadata": {"window": "Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. ", "original_text": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n"}, "hash": "4d49e06f137b65464c73af82b0758e2fa27aba9eb1ad048914c6134ed2cedf37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n", "mimetype": "text/plain", "start_char_idx": 38049, "end_char_idx": 38233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82a9d562-781d-4633-aedd-177c8a3e2e84", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. ", "original_text": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4f1d0d7-d1ef-4d47-892f-18465ead8100", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It means that they should tend to be located closer, forming a dense cluster or several clusters.\n\n Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig. ", "original_text": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n"}, "hash": "cb18be35d711ad0fad141d05a883999613f4116666587c0739455c39f6876f12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76781479-567e-424a-8acb-6e4d50b3b818", "node_type": "1", "metadata": {"window": "All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. ", "original_text": "Fig. "}, "hash": "d907fd7ef5209b3177d34be84d2f8f740f73f0627c8f4dc7fb25cf5e9355d766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n", "mimetype": "text/plain", "start_char_idx": 38233, "end_char_idx": 38398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76781479-567e-424a-8acb-6e4d50b3b818", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. ", "original_text": "Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82a9d562-781d-4633-aedd-177c8a3e2e84", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Now, we can take a look at the real datasets considered.  All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. ", "original_text": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n"}, "hash": "6872b9043c3818265c38aac10f2bf317b9573ce9c45ec7dec7df8d6a2ad4da7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6266c873-c282-46ab-b032-e58ebd76dcd0", "node_type": "1", "metadata": {"window": "Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. ", "original_text": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. "}, "hash": "38f7c85160fa0b2ba96fe1570d7d60ffdc70d378c46017f0ffc9d44f89f6a53b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fig. ", "mimetype": "text/plain", "start_char_idx": 38398, "end_char_idx": 38403, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6266c873-c282-46ab-b032-e58ebd76dcd0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. ", "original_text": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76781479-567e-424a-8acb-6e4d50b3b818", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "All of them contain highly-dimensional data, so straight graphic examination is not possible.  Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. ", "original_text": "Fig. "}, "hash": "afb3476d24808d0c6e819878633af8b986cac9c6da277110ff8d35c5827102d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9192578b-27a8-4def-856d-9e2d6c84b3ee", "node_type": "1", "metadata": {"window": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n", "original_text": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. "}, "hash": "b39e585c27f70d13b4a7735276f60da2f9dbd74e4a566d9de716a0b7cfd869a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. ", "mimetype": "text/plain", "start_char_idx": 38403, "end_char_idx": 38521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9192578b-27a8-4def-856d-9e2d6c84b3ee", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n", "original_text": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6266c873-c282-46ab-b032-e58ebd76dcd0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Instead, we will introduce simple intuitive metric.  Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. ", "original_text": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above. "}, "hash": "faf7c7e000ad33313222d7dc5893dfa5e9b24cb86dbb2cc908ccd1f263d0c4d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcde54ac-13ad-400f-b69e-645522181362", "node_type": "1", "metadata": {"window": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. ", "original_text": "Interpreting the Fig. "}, "hash": "90e0fe00aae964d70c8d4287e934881cd7308fdbfe00edff93d73ee4a890ee30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. ", "mimetype": "text/plain", "start_char_idx": 38521, "end_char_idx": 38707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fcde54ac-13ad-400f-b69e-645522181362", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. ", "original_text": "Interpreting the Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9192578b-27a8-4def-856d-9e2d6c84b3ee", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Let the average Euclidean distance between:\n\n* An outlier and the closest respective inlier be the metric, showing how \"abnormal\" the outliers of the considered dataset be (d<sub>out-in</sub>),\n* An inlier and the closest respective inlier to be the metric, showing how \"normal\" the inliers of the considered dataset be (d<sub>in-in</sub>).\n\n We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n", "original_text": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics. "}, "hash": "e68517fa2568774e9f54ff78f48515df4d2dc1ee169496ce102862399f7b481d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af8af5b9-bb6f-465c-9737-e8af0dc0bc06", "node_type": "1", "metadata": {"window": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). ", "original_text": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. "}, "hash": "2ddd92acbf41d677e9060d835c30da979f121a5e13f0a4a0d673cb2b583f8d7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interpreting the Fig. ", "mimetype": "text/plain", "start_char_idx": 38707, "end_char_idx": 38729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af8af5b9-bb6f-465c-9737-e8af0dc0bc06", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). ", "original_text": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcde54ac-13ad-400f-b69e-645522181362", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We also have to take into account the fact that with the growth of dimensionality Euclidean distance also grows, so the Euclidean metric has to be modified as the formula (28) shows.\n\n d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. ", "original_text": "Interpreting the Fig. "}, "hash": "3d75c7d5743434c02ea803d94658bebd402d811f0bd742266ccd744bb4dbea0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5643cf0e-88fa-4e20-b88d-fc8bced2ce30", "node_type": "1", "metadata": {"window": "Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold. ", "original_text": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. "}, "hash": "2bb111262ef0094f420d813d4008009cff3bf9fa24da5049e889da3b8053c826", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. ", "mimetype": "text/plain", "start_char_idx": 38729, "end_char_idx": 38883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5643cf0e-88fa-4e20-b88d-fc8bced2ce30", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold. ", "original_text": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af8af5b9-bb6f-465c-9737-e8af0dc0bc06", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "d<sub>uct</sub>(a,b) = d<sub>Eucl</sub>(a,b) / \u221aN (28)\n\nwhere a and b are the points between which the metric is computed and N is the dimensionality of the space.\n\n Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). ", "original_text": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics. "}, "hash": "d1f56596374137b87b7a42be1cc4e55f8c479d688923932c91228297ade9cf13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d19db7cb-794c-4fa6-a8a2-6e30b39c519b", "node_type": "1", "metadata": {"window": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n", "original_text": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. "}, "hash": "a30160dbb9efe0eed72dab81298e6484f38696ef9f25516bc9b3b91283611830", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. ", "mimetype": "text/plain", "start_char_idx": 38883, "end_char_idx": 39027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d19db7cb-794c-4fa6-a8a2-6e30b39c519b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n", "original_text": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5643cf0e-88fa-4e20-b88d-fc8bced2ce30", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Fig.  6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold. ", "original_text": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets. "}, "hash": "f553b57a2b7c3f6ef894c21a9e129789cf46b1fc375a1aad8bf4532163d05665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18fe3406-8e68-4780-8b4d-0ed9261c0a2e", "node_type": "1", "metadata": {"window": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). ", "original_text": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n"}, "hash": "017c749068c37a640cd25c397e10b000edf2d75d76fdfb704d017220853bd3c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. ", "mimetype": "text/plain", "start_char_idx": 39027, "end_char_idx": 39176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18fe3406-8e68-4780-8b4d-0ed9261c0a2e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). ", "original_text": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d19db7cb-794c-4fa6-a8a2-6e30b39c519b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6 shows the 2D scatter plot on which every point corresponds to a dataset and the axes are the metrics defined above.  One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n", "original_text": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal. "}, "hash": "7ab00ad887f2bdd1e04e1a149490871713531c490f1be85ef7047e4763f0bd7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70982dc4-2651-4320-8d34-8cdbea3f52fa", "node_type": "1", "metadata": {"window": "Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. ", "original_text": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. "}, "hash": "842341958e7e8f6c7df7c1e7fdc21545b1e4088e738dae24d4b8ea8aa85c37f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 39176, "end_char_idx": 39318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70982dc4-2651-4320-8d34-8cdbea3f52fa", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. ", "original_text": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18fe3406-8e68-4780-8b4d-0ed9261c0a2e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "One can easily notice that exactly the datasets Annthyroid and Mammography on which PGIF did not improve the quality of anomaly detection, show the lowest average value of both metrics.  Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). ", "original_text": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n"}, "hash": "1485c1b5ee7ca923820f00c332137bdbde2389c9a6cc19c90fa9807b4aa218e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7ea49b2-b543-4b4e-9fb1-55402eaa9bf9", "node_type": "1", "metadata": {"window": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. ", "original_text": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). "}, "hash": "e381ba32171031dbfc55f64b7f0e3e296aaa79ff7962cc3491a4a4f3557c5193", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. ", "mimetype": "text/plain", "start_char_idx": 39318, "end_char_idx": 40179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d7ea49b2-b543-4b4e-9fb1-55402eaa9bf9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. ", "original_text": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70982dc4-2651-4320-8d34-8cdbea3f52fa", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Interpreting the Fig.  6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. ", "original_text": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2. "}, "hash": "868bc06f817c1b0a796b6da006b578cc8782b2bf8186f7e300c4b6006a01b60f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc9134ee-5b9c-49af-8120-287127d33af0", "node_type": "1", "metadata": {"window": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). ", "original_text": "The best results in the rows are bold. "}, "hash": "b7cd8029429d560df80da83eaa7798b5a3ab1c17b791e1c70ebdb191ea2ea8f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). ", "mimetype": "text/plain", "start_char_idx": 40179, "end_char_idx": 40283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc9134ee-5b9c-49af-8120-287127d33af0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). ", "original_text": "The best results in the rows are bold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7ea49b2-b543-4b4e-9fb1-55402eaa9bf9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6, one may state that the anomalies in the mentioned datasets are \"less abnormal\", compared to the other datasets in the sense of the introduced metrics.  Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. ", "original_text": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC). "}, "hash": "fb7884ccd407397784723268fc8f78d972c1444fcc22139dcc8651acb298c899", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "630558ac-4264-402a-8243-d026519e20c2", "node_type": "1", "metadata": {"window": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). ", "original_text": "If two or more models show similarly best results no value is marked.\n\n"}, "hash": "3707ba75ae395fda21d1354818a475bee8c83571299dba3e459566fa27f37449", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The best results in the rows are bold. ", "mimetype": "text/plain", "start_char_idx": 40283, "end_char_idx": 40322, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "630558ac-4264-402a-8243-d026519e20c2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). ", "original_text": "If two or more models show similarly best results no value is marked.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc9134ee-5b9c-49af-8120-287127d33af0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Furthermore, in the case of these datasets the difference between the values of these metrics is notably lower, compared to the other datasets.  This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). ", "original_text": "The best results in the rows are bold. "}, "hash": "3325e05402d0af32870aa6646dd61fd200f1bb4f3c618cf4a18524e6d81f22e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31b95000-ef03-436e-9157-941de79d1da8", "node_type": "1", "metadata": {"window": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. ", "original_text": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). "}, "hash": "a9135b269ed9b963c56c5df0e05c2b7c4137322f00e87fe1b1b15f7922d1d887", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If two or more models show similarly best results no value is marked.\n\n", "mimetype": "text/plain", "start_char_idx": 40322, "end_char_idx": 40393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31b95000-ef03-436e-9157-941de79d1da8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. ", "original_text": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "630558ac-4264-402a-8243-d026519e20c2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This phenomenon can be explained in the following way: Probably, the anomalies were marked as such by some expert, who knew that they were abnormal.  However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). ", "original_text": "If two or more models show similarly best results no value is marked.\n\n"}, "hash": "6f932e9a4427d4223520f71bdc41da848fd9a1a0595387a577d8d7a909af857f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2468929-5a7a-4d74-8d23-fca8d7558062", "node_type": "1", "metadata": {"window": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters. ", "original_text": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. "}, "hash": "9225c8eb00af46982bb33ae0947118c6125993ecb078ba881b7b81aee9359d53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). ", "mimetype": "text/plain", "start_char_idx": 40393, "end_char_idx": 41426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2468929-5a7a-4d74-8d23-fca8d7558062", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters. ", "original_text": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31b95000-ef03-436e-9157-941de79d1da8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, the applied features do not show the difference explicitly (many\n\n---\n**Table 1**\nThe parameters of the applied benchmark datasets.\n\n | Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. ", "original_text": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers). "}, "hash": "57022d1c9d25db7186f0186c40530dc0b01c6263ba4607b5a8865bf548ef508f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7428548-51cd-492d-a0a6-a248122f2400", "node_type": "1", "metadata": {"window": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder. ", "original_text": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. "}, "hash": "2a8e1a48df4edfb2010fa5d876129e744705be29c39c354948e778904936a31e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. ", "mimetype": "text/plain", "start_char_idx": 41426, "end_char_idx": 41602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7428548-51cd-492d-a0a6-a248122f2400", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder. ", "original_text": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2468929-5a7a-4d74-8d23-fca8d7558062", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | Number of samples | Anomalies (number/%) | Dimension number |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 [42] | 58,725 | 2209 / 3.8 | 3 |\n| ForestCover [43] | 286,048 | 2747 / 1 | 10 |\n| Mulcross [44] | 262,144 | 26,214 / 10 | 4 |\n| Shuttle [43] | 49,097 | 3511 / 7.2 | 9 |\n| Mammography [45] | 11,183 | 260 / 2.3 | 6 |\n| Annthyroid [43] | 7200 | 534 / 7.4 | 6 |\n| Satellite [43] | 6435 | 2036 / 31.6 | 36 |\n| Pima [43] | 768 | 268 / 35 | 8 |\n| Breastw [43] | 682 | 239 / 35 | 9 |\n| Arrhythmia [43] | 452 | 66 / 14.6 | 274 |\n| Ionosphere [43] | 351 | 126 / 36 | 33 |\n| Wine [45] | 129 | 10 / 7.7 | 13 |\n| Credit card [25,42\u201350] | 284,807 | 492 / 0.17 | 30 |\n| Cardio [42] | 1831 | 176 / 9.6 | 21 |\n---\n\n**Table 2**\nResults obtained with the original method as well as with PGIF applying uniform function for random value generation and k = 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters. ", "original_text": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>. "}, "hash": "89b1bedfa9d098a7de5ed82a250e23005be58b031cecd771818ad308c151ac53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ad7da84-31d0-4fb2-b633-752295613f69", "node_type": "1", "metadata": {"window": "The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g.", "original_text": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). "}, "hash": "138d2aac12b190e86ecbac1aa8499b982e65b35ec9d31b8bdc7818f720249a16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. ", "mimetype": "text/plain", "start_char_idx": 41602, "end_char_idx": 41696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ad7da84-31d0-4fb2-b633-752295613f69", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g.", "original_text": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7428548-51cd-492d-a0a6-a248122f2400", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mean and standard deviation of the AUC values are provided in the following format: Mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder. ", "original_text": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC. "}, "hash": "d7a291ceac4742ce4db2fd34ed4a2fd01c83e6afc1c4d0ae54ca2f03e48f8c32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e057be44-013c-4312-9e4f-427461aed738", "node_type": "1", "metadata": {"window": "If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n", "original_text": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). "}, "hash": "63548390ae074fe0a735912e2b48e86db1abd8923030c81c16ce886ab5462810", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). ", "mimetype": "text/plain", "start_char_idx": 41696, "end_char_idx": 41787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e057be44-013c-4312-9e4f-427461aed738", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n", "original_text": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ad7da84-31d0-4fb2-b633-752295613f69", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The best results in the rows are bold.  If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g.", "original_text": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994). "}, "hash": "3a91cf0217c4a4f59e59b02ee69c0509c62cecc53881f511a6834574a10c816f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f7d09e5-f1d1-4cfc-9f8f-552fc2893939", "node_type": "1", "metadata": {"window": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig. ", "original_text": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. "}, "hash": "7d1748a7e30433743b2fe3ff788ffe6a6bdfa61aef045f1996b106c346bc3669", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). ", "mimetype": "text/plain", "start_char_idx": 41787, "end_char_idx": 41945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f7d09e5-f1d1-4cfc-9f8f-552fc2893939", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig. ", "original_text": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e057be44-013c-4312-9e4f-427461aed738", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results no value is marked.\n\n | Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n", "original_text": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF). "}, "hash": "87338ae6de92eca7fd871d2ac3d927061c273fb55fff70f854efe0b8c96476e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b9e32ef-6186-4543-8637-e920f42c9705", "node_type": "1", "metadata": {"window": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets. ", "original_text": "It means that the inliers do not form such dense clusters. "}, "hash": "a1b938be29402f0e40ac122475d6c9080b0530c6b2d8031999def24c6a685723", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. ", "mimetype": "text/plain", "start_char_idx": 41945, "end_char_idx": 42074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b9e32ef-6186-4543-8637-e920f42c9705", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets. ", "original_text": "It means that the inliers do not form such dense clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f7d09e5-f1d1-4cfc-9f8f-552fc2893939", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | IF | PGIF, k=1 | PGIF, k=2 |\n| :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.994 (0.001) | 0.992 (0.002) |\n| ForestCover | 0.873 (0.024) | 0.929 (0.017) | **0.940 (0.014)** |\n| Mulcross | 0.958 (0.008) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.997 (0.001) |\n| Mammography | **0.866 (0.009)** | 0.731 (0.019) | 0.689 (0.018) |\n| Annthyroid | **0.817 (0.016)** | 0.786 (0.018) | 0.759 (0.029) |\n| Satellite | 0.704 (0.014) | **0.711 (0.014)** | 0.700 (0.016) |\n| Pima | 0.679 (0.013) | **0.686 (0.009)** | 0.672 (0.009) |\n| Breastw | 0.988 (0.002) | 0.989 (0.001) | 0.989 (0.001) |\n| Arrhythmia | 0.792 (0.015) | **0.793 (0.014)** | 0.791 (0.018) |\n| Ionosphere | 0.855 (0.006) | 0.873 (0.005) | **0.879 (0.005)** |\n| Wine | 0.791 (0.041) | 0.838 (0.020) | **0.843 (0.020)** |\n| Credit card | 0.948 (0.003) | **0.951 (0.002)** | 0.950 (0.003) |\n| Cardio | 0.926 (0.011) | 0.939 (0.007) | **0.943 (0.008)** |\n---\nabnormal datapoints are located close to inliers).  Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig. ", "original_text": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low. "}, "hash": "247198c61246d00dd0c0bb442c714dac1e98fd7594f006fe87a5d942e713f013", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67249842-4825-4686-8577-e1c78a9b4555", "node_type": "1", "metadata": {"window": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. ", "original_text": "Therefore, the task of anomaly detection becomes harder. "}, "hash": "d9daf5b343f588e98a42a9aec6ba7e47335d51fbfa5fc3e6019b7b209a90665e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It means that the inliers do not form such dense clusters. ", "mimetype": "text/plain", "start_char_idx": 42074, "end_char_idx": 42133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67249842-4825-4686-8577-e1c78a9b4555", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. ", "original_text": "Therefore, the task of anomaly detection becomes harder. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b9e32ef-6186-4543-8637-e920f42c9705", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Such datasets as breast, Shuttle, Http, Mulcross, Creditcard, Cover, demonstrate extremely low values of d<sub>in-in</sub> and comparatively high values of d<sub>out-in</sub>.  Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets. ", "original_text": "It means that the inliers do not form such dense clusters. "}, "hash": "5dbfa0b66883b5019f53de53bc25f31d7a0d90e0f7a390a43abb9ebd5593d1ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c323ddfd-f43a-4bff-a289-a5df00a4da50", "node_type": "1", "metadata": {"window": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters. ", "original_text": "However, in most cases PGIF allows to improve the performance notably, e.g."}, "hash": "c5452f80c68194d0138f2302c672a91d0a44071cb0026fa06994e55eacc56949", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the task of anomaly detection becomes harder. ", "mimetype": "text/plain", "start_char_idx": 42133, "end_char_idx": 42190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c323ddfd-f43a-4bff-a289-a5df00a4da50", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters. ", "original_text": "However, in most cases PGIF allows to improve the performance notably, e.g."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67249842-4825-4686-8577-e1c78a9b4555", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Because of that fact all the results achieved with PGIF on these datasets are above 0.94 AUC.  In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. ", "original_text": "Therefore, the task of anomaly detection becomes harder. "}, "hash": "94dbca49ad29532a59689cbd51f6bc61e0a8eefadb8d17e682e6bc0cb1ea2785", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8e80498-a57d-4f96-af3a-bdd037609ff4", "node_type": "1", "metadata": {"window": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. ", "original_text": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n"}, "hash": "ff4e7b8b11886b9804a84a033db336d725a8c4bea34892eda08fdd0abe5dc619", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, in most cases PGIF allows to improve the performance notably, e.g.", "mimetype": "text/plain", "start_char_idx": 42190, "end_char_idx": 42265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8e80498-a57d-4f96-af3a-bdd037609ff4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. ", "original_text": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c323ddfd-f43a-4bff-a289-a5df00a4da50", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In some cases they are equally high as for IF (Breast: 0.986, Shuttle:0.997, Http: 0.994).  In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters. ", "original_text": "However, in most cases PGIF allows to improve the performance notably, e.g."}, "hash": "1ff11592a287cc0b43b5fec4cebc67dafdc2d796b9692c343286250c4fb74f1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45e6a336-8b38-4e09-80c6-fcb6f74b2072", "node_type": "1", "metadata": {"window": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n", "original_text": "This point is also supported by the Fig. "}, "hash": "ccd3c7f28653b63ad0b5f551d0ce8ed734ae7c38b03ae8f0131b46767954e048", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n", "mimetype": "text/plain", "start_char_idx": 42265, "end_char_idx": 42513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45e6a336-8b38-4e09-80c6-fcb6f74b2072", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n", "original_text": "This point is also supported by the Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8e80498-a57d-4f96-af3a-bdd037609ff4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In other datasets they are even better, namely: Cover: 0.873 (IF) \u2013 0.943 (PGIF), Creditcard: 0.948 (IF) \u2013 0.951 (PGIF), Mulcross: 0.958 (IF) \u2013 0.995 (PGIF).  In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. ", "original_text": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n"}, "hash": "6e70346182d770a0872fd60530dc5e3252e1a5e34afad2fbf96f709fa4c1485b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e38c442-8646-420b-b5ff-bb0d7d7fa894", "node_type": "1", "metadata": {"window": "It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. ", "original_text": "7, presenting the scatter plots of the first two principal components of the analyzed datasets. "}, "hash": "0ac04aaf0e91dde6d523b0729ca5ccd2b95beeaf4b3922934ac596a2f5f66671", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This point is also supported by the Fig. ", "mimetype": "text/plain", "start_char_idx": 42513, "end_char_idx": 42554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4e38c442-8646-420b-b5ff-bb0d7d7fa894", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. ", "original_text": "7, presenting the scatter plots of the first two principal components of the analyzed datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45e6a336-8b38-4e09-80c6-fcb6f74b2072", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In case of such datasets as Pima, Arrhytmia, Wine, Satellite, Cardio, Ionosphere the value of d<sub>in-in</sub> is not that low.  It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n", "original_text": "This point is also supported by the Fig. "}, "hash": "23f55a7e2594b383e317f75f92d5d2130a151a1b17eb5c6559ed42957456faf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9242cca9-4a4a-41db-ba3a-bd589f58a1aa", "node_type": "1", "metadata": {"window": "Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" ", "original_text": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. "}, "hash": "c8903e9af747d6816b54756b98f81732b731aeb744526f8f6d0409879b57e344", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7, presenting the scatter plots of the first two principal components of the analyzed datasets. ", "mimetype": "text/plain", "start_char_idx": 42554, "end_char_idx": 42650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9242cca9-4a4a-41db-ba3a-bd589f58a1aa", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" ", "original_text": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e38c442-8646-420b-b5ff-bb0d7d7fa894", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It means that the inliers do not form such dense clusters.  Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. ", "original_text": "7, presenting the scatter plots of the first two principal components of the analyzed datasets. "}, "hash": "ba390994d478126305ccfa7383f73504142e5ebedfea530b16800fdc4d0a5f47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "498f7777-0fe6-42b9-92f2-6b7a25da6195", "node_type": "1", "metadata": {"window": "However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled. ", "original_text": "Moreover, many inliers (marked with blue) are located notably far from clusters. "}, "hash": "77f4055e81bc30db064483208ca54a9e7930a70b03300b930b6c3c59ac8ae353", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. ", "mimetype": "text/plain", "start_char_idx": 42650, "end_char_idx": 42779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "498f7777-0fe6-42b9-92f2-6b7a25da6195", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled. ", "original_text": "Moreover, many inliers (marked with blue) are located notably far from clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9242cca9-4a4a-41db-ba3a-bd589f58a1aa", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, the task of anomaly detection becomes harder.  However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" ", "original_text": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it. "}, "hash": "c1367543426f94a1e0544a1d1eb79ac3918d39c2c39af97ce200f6f5caaaa539", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ffb5cf9-68b2-4607-b7c7-7be3919bfdf9", "node_type": "1", "metadata": {"window": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n", "original_text": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. "}, "hash": "f76a198162c197eb0f772679ea5f3bce872db37f0f6d0231a7d3f2081ef02504", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, many inliers (marked with blue) are located notably far from clusters. ", "mimetype": "text/plain", "start_char_idx": 42779, "end_char_idx": 42860, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ffb5cf9-68b2-4607-b7c7-7be3919bfdf9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n", "original_text": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "498f7777-0fe6-42b9-92f2-6b7a25da6195", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, in most cases PGIF allows to improve the performance notably, e.g. : Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled. ", "original_text": "Moreover, many inliers (marked with blue) are located notably far from clusters. "}, "hash": "88d57a09c6208e72a4382e6a3ff926b106c8c90cdd7c97b33dfd1fdb020b033d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca732391-c713-45db-9604-732836a1d744", "node_type": "1", "metadata": {"window": "This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig. ", "original_text": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n"}, "hash": "af287aaa75dee7e2f92649a52ba732b8fdcafa08160101a330ae74dc1e689afe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. ", "mimetype": "text/plain", "start_char_idx": 42860, "end_char_idx": 43086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca732391-c713-45db-9604-732836a1d744", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig. ", "original_text": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ffb5cf9-68b2-4607-b7c7-7be3919bfdf9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": ": Wine: 0.791 (IF) \u2013 0.843 (PGIF), Satellite: 0.703 (IF) \u2013 0.71 (PGIF), Cardio: 0.926 (IF) \u2013 0.943 (PGIF), Pima: 0.679 (IF) \u2013 0.686 (PGIF), Ionosphere: 0.855 (IF) \u2013 0.878 (PGIF) or ensure similar performance: Arrythmia: 0.792 (IF) \u2013 0.793 (PGIF).\n\n This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n", "original_text": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets. "}, "hash": "7296fea8fc7b0b7b733db61cb5e3fc3c4986551918b62d5b894ff0805a017dc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae3aa589-dc00-45f8-ad51-982c1eddccdc", "node_type": "1", "metadata": {"window": "7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6.", "original_text": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. "}, "hash": "35a9e391933354325a521490cdab479e3b4b5a33ab4a0f37ed9cd544ade9c0a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n", "mimetype": "text/plain", "start_char_idx": 43086, "end_char_idx": 43270, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae3aa589-dc00-45f8-ad51-982c1eddccdc", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6.", "original_text": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca732391-c713-45db-9604-732836a1d744", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This point is also supported by the Fig.  7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig. ", "original_text": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n"}, "hash": "5a68612adfc01fea76bd1280cbcc8aaf3ee1f66ac93501d64c2db8aa69375312", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2946bff-e1fc-47d5-9929-6a14d39392ba", "node_type": "1", "metadata": {"window": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n", "original_text": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" "}, "hash": "b95b483735500cef516d8be1e18cd43dbc773e4fa04966f2d2c12ed74bcf5c5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. ", "mimetype": "text/plain", "start_char_idx": 43270, "end_char_idx": 43359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c2946bff-e1fc-47d5-9929-6a14d39392ba", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n", "original_text": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae3aa589-dc00-45f8-ad51-982c1eddccdc", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "7, presenting the scatter plots of the first two principal components of the analyzed datasets.  One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6.", "original_text": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset. "}, "hash": "fb91012197a5e114cb55e5a93e820135385022a67764185587c9a22b7143e24e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75aae879-d20a-4398-9ca5-1f8e7fb072ce", "node_type": "1", "metadata": {"window": "Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. ", "original_text": "Datasets are labeled. "}, "hash": "d33c3843d05fc873505ce4a08fda7bcd71d4b5952ed6e158ed7dca1f75569086", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" ", "mimetype": "text/plain", "start_char_idx": 43359, "end_char_idx": 43505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "75aae879-d20a-4398-9ca5-1f8e7fb072ce", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. ", "original_text": "Datasets are labeled. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2946bff-e1fc-47d5-9929-6a14d39392ba", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "One can clearly see that many outliers (marked with orange) are located close to the inlier cluster and are even included in it.  Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n", "original_text": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\" "}, "hash": "b7318a0b4704c0f55e354d294f4315609153703149666d4bf7197e28c695bf91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d0939ef-9f48-4a28-a5c8-a845428a40dc", "node_type": "1", "metadata": {"window": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n", "original_text": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n"}, "hash": "3a1df8665a447827e9310b449ebee7e33974460c9e55c170103640b28457248e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Datasets are labeled. ", "mimetype": "text/plain", "start_char_idx": 43505, "end_char_idx": 43527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d0939ef-9f48-4a28-a5c8-a845428a40dc", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n", "original_text": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75aae879-d20a-4398-9ca5-1f8e7fb072ce", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, many inliers (marked with blue) are located notably far from clusters.  In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. ", "original_text": "Datasets are labeled. "}, "hash": "5ec6e925e6ce6ead28b2f62eb2ed6db0ae48104f2ab2d5fabff577d371d8a63b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51791ce6-0a1d-4db4-8977-c6596c135526", "node_type": "1", "metadata": {"window": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig. ", "original_text": "**Fig. "}, "hash": "e5efaa5d1fedc9846ba8eb4ae372c51a24b768d47fe73876f5f07612f84a1ec7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n", "mimetype": "text/plain", "start_char_idx": 43527, "end_char_idx": 43719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51791ce6-0a1d-4db4-8977-c6596c135526", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d0939ef-9f48-4a28-a5c8-a845428a40dc", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In this light it is completely reasonable that PGIF demonstrated worse performance \u2013 it is designed to effectively find sparcely populated regions and separately located points that are not always anomalies in these datasets.  However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n", "original_text": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n"}, "hash": "da4b262fb5b1f8678003c1fec1b8ca6e22f3169507c02f85f2a9f88196bc617d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82c22c30-d4b3-4501-94c7-6cc13ed9d6d5", "node_type": "1", "metadata": {"window": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7.", "original_text": "6."}, "hash": "0a139f99e625c869e57d9ee0b199a1f1df9499c7ed6e647af353f635281b2b43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 43719, "end_char_idx": 43726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82c22c30-d4b3-4501-94c7-6cc13ed9d6d5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7.", "original_text": "6."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51791ce6-0a1d-4db4-8977-c6596c135526", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, the generalized approach still can lead to better results as it is shown in the next section where we use U-shaped probability density function for generating random values.\n\n ---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig. ", "original_text": "**Fig. "}, "hash": "d461c78e908546a6ec303127dcb7f53779fd2b13afe859e547de81a931db93b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dec1b78-c22c-4e3d-85f2-7a451ea2db2f", "node_type": "1", "metadata": {"window": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n", "original_text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n"}, "hash": "ff3758e36cd9fefd6ff2844ba9ae958973210bd5da0fa9141cb469a4290651c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6.", "mimetype": "text/plain", "start_char_idx": 43726, "end_char_idx": 43728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8dec1b78-c22c-4e3d-85f2-7a451ea2db2f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n", "original_text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82c22c30-d4b3-4501-94c7-6cc13ed9d6d5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 6: A scatter plot where each point represents a real dataset.  The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7.", "original_text": "6."}, "hash": "a6d054452bc678378dc0edca34dae78152fb539a1a03ed48cd9e72ee05c2743b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3851c8f1-a99f-467b-9f18-276e246e6e12", "node_type": "1", "metadata": {"window": "Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4. ", "original_text": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. "}, "hash": "08d01c60387fd2c2bb0e0e00fb7303572c876aff10bd3e6705395a5eb602788f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 43728, "end_char_idx": 43818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3851c8f1-a99f-467b-9f18-276e246e6e12", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4. ", "original_text": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dec1b78-c22c-4e3d-85f2-7a451ea2db2f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The x-axis is \"Average distance from an outlier to the closest inlier\" and the y-axis is \"Average distance from an inlier to the closest inlier.\"  Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n", "original_text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n"}, "hash": "3dcf5871254df5b8ca54474cf30e0e143eaa5755e1c1a95cd043c1c34e16ef2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9928fd1-1103-47c0-8b8f-63ca8046ef84", "node_type": "1", "metadata": {"window": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. ", "original_text": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n"}, "hash": "77a548cb6a37d70da7d318b3aa81579347ce2c793f68cd586981da57ee193cfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. ", "mimetype": "text/plain", "start_char_idx": 43818, "end_char_idx": 43958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9928fd1-1103-47c0-8b8f-63ca8046ef84", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. ", "original_text": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3851c8f1-a99f-467b-9f18-276e246e6e12", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Datasets are labeled.  'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4. ", "original_text": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets. "}, "hash": "5f75bb1d99627cdd6359d7fa75aaed4b7b88cd3aebb9f001fb3ccffe1938d38e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efa9e4d8-1dcb-4157-ba17-b88de3cca777", "node_type": "1", "metadata": {"window": "**Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n", "original_text": "**Fig. "}, "hash": "7be2fae5979f36bc9b08a2f85093319de6c59a586b07271128b5c669bd307818", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n", "mimetype": "text/plain", "start_char_idx": 43958, "end_char_idx": 44172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "efa9e4d8-1dcb-4157-ba17-b88de3cca777", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9928fd1-1103-47c0-8b8f-63ca8046ef84", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "'annthyroid' and 'mammography' are located in the bottom-left corner, indicating small distances for both metrics, meaning outliers are close to inliers and inliers are close to each other.]\n\n **Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. ", "original_text": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n"}, "hash": "a6f17ef7deaae640779dc31f0f6dd35c427e8c6e5aac8642e8ff901bb95f668c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7397998f-9411-4958-a1a1-ec4930011d47", "node_type": "1", "metadata": {"window": "6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5. ", "original_text": "7."}, "hash": "09d2bcebc472e923068392bcb8b245168cf9644f237f175224473d24de25bcea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 44172, "end_char_idx": 44179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7397998f-9411-4958-a1a1-ec4930011d47", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5. ", "original_text": "7."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efa9e4d8-1dcb-4157-ba17-b88de3cca777", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n", "original_text": "**Fig. "}, "hash": "0e92d86a6ab7f5fcbd36fa50a86cc3ad7f61cfa12a8c24891aeb9ba449e00c2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1fdd791-a450-4ff7-a7d1-6dc29c2bae08", "node_type": "1", "metadata": {"window": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. ", "original_text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n"}, "hash": "48e200e3c163f0f071ef997b8e2bd9fa74116813b1e7cdc24210125a429751c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7.", "mimetype": "text/plain", "start_char_idx": 44179, "end_char_idx": 44181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1fdd791-a450-4ff7-a7d1-6dc29c2bae08", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. ", "original_text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7397998f-9411-4958-a1a1-ec4930011d47", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "6. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5. ", "original_text": "7."}, "hash": "fd8b20973b389b16670013707c9d5b827a8749b0baaefc9e5c3bef5b30adf608", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "136b8adc-637e-479b-ae4f-badb0301a302", "node_type": "1", "metadata": {"window": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high. ", "original_text": "---\n\n### 4.4. "}, "hash": "34fcbeba2ab1ede3382ec59b528a747f927540102d0b77fa3ff552aa5504819b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n", "mimetype": "text/plain", "start_char_idx": 44181, "end_char_idx": 44270, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "136b8adc-637e-479b-ae4f-badb0301a302", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high. ", "original_text": "---\n\n### 4.4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1fdd791-a450-4ff7-a7d1-6dc29c2bae08", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n\n ---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. ", "original_text": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n"}, "hash": "c0145b57a00da2c089ce272ba7f2fbc1d0b89b2ff9cbb9bf753f51dc9374f62d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c91c9f4-b0db-4676-a2ed-41e19329b974", "node_type": "1", "metadata": {"window": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. ", "original_text": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. "}, "hash": "d0fdce37b70dd643da37b87c133ab0829674a10947add2daee09ba13b58e73b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n### 4.4. ", "mimetype": "text/plain", "start_char_idx": 44270, "end_char_idx": 44284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c91c9f4-b0db-4676-a2ed-41e19329b974", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. ", "original_text": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "136b8adc-637e-479b-ae4f-badb0301a302", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n[Description of Figure 7: Two scatter plots showing the first two principal components for the 'annthyroid' and 'mammography' datasets.  In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high. ", "original_text": "---\n\n### 4.4. "}, "hash": "946a18e07a66e65f95b412377b3fdf073bb56f282616cf8857649644420bc04c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84eecdc9-353f-4815-9be2-58b5dbfd956c", "node_type": "1", "metadata": {"window": "**Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment. ", "original_text": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n"}, "hash": "0fb0ea3c17f4f40d2f28a37c746442bdabe4480c129d7d724771261381e7edaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. ", "mimetype": "text/plain", "start_char_idx": 44284, "end_char_idx": 44421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84eecdc9-353f-4815-9be2-58b5dbfd956c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment. ", "original_text": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c91c9f4-b0db-4676-a2ed-41e19329b974", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In both plots, outliers (orange dots) are heavily intermingled with inliers (blue dots), and in many cases are located within the main cluster of inliers, making them difficult to distinguish based on isolation.]\n\n **Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. ", "original_text": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function. "}, "hash": "f0727cb29457816a23cf521e62d37ccaa849acbe37462a809f87ce3710e5ccb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb95e40d-1308-4277-b53d-39908db54e69", "node_type": "1", "metadata": {"window": "7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. ", "original_text": "### 4.5. "}, "hash": "81aa99b1ae46a2e7a1cc36896378b2a84dd916a29218b4333d17427ae3348a74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 44421, "end_char_idx": 44530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb95e40d-1308-4277-b53d-39908db54e69", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. ", "original_text": "### 4.5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84eecdc9-353f-4815-9be2-58b5dbfd956c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment. ", "original_text": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n"}, "hash": "b44675d2a5a1b0c00d27301bf12c3eab591b129deb89a24f61268b08218251f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6215f630-8eb6-4203-b416-e17467666ed3", "node_type": "1", "metadata": {"window": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n", "original_text": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. "}, "hash": "eaa50623d3b103a017583931848623533e0b3b7983ade9a28577e23dea951263", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.5. ", "mimetype": "text/plain", "start_char_idx": 44530, "end_char_idx": 44539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6215f630-8eb6-4203-b416-e17467666ed3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n", "original_text": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb95e40d-1308-4277-b53d-39908db54e69", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "7. ** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. ", "original_text": "### 4.5. "}, "hash": "211a89fdaab38b76738487ad0ae69ff22add9cc5066b11f62e051bff0ab7e9b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4a3674f-03fa-4d60-b842-f82a0a78184f", "node_type": "1", "metadata": {"window": "---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. ", "original_text": "Some datasets have N/A in the 1-class SVM column as time complexity is too high. "}, "hash": "88885d5dc51db6244c4b2ce955927603925bcc4dbde560463d307bb0fde08e98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. ", "mimetype": "text/plain", "start_char_idx": 44539, "end_char_idx": 44761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4a3674f-03fa-4d60-b842-f82a0a78184f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. ", "original_text": "Some datasets have N/A in the 1-class SVM column as time complexity is too high. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6215f630-8eb6-4203-b416-e17467666ed3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Average distance between an outlier and the closest inlier for the analyzed datasets.\n ---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n", "original_text": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4. "}, "hash": "765f10cdab415d8afe00bc557ce0adc1f832d35e1b5d3924e43bf1120a0acfac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6973e330-c928-47f7-a809-6f0202d43007", "node_type": "1", "metadata": {"window": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). ", "original_text": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. "}, "hash": "6504f4399036db8a0c24a6bdaa43e7169120714a71a277aab0e8468120ce9158", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some datasets have N/A in the 1-class SVM column as time complexity is too high. ", "mimetype": "text/plain", "start_char_idx": 44761, "end_char_idx": 44842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6973e330-c928-47f7-a809-6f0202d43007", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). ", "original_text": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4a3674f-03fa-4d60-b842-f82a0a78184f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\n### 4.4.  Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. ", "original_text": "Some datasets have N/A in the 1-class SVM column as time complexity is too high. "}, "hash": "c03aa6f631ea932898f524cbb0fccd7a570dca7c7a4e3addcf18a916fdf9466d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38445ead-670f-4ca3-a99b-2fc0bc6fcc67", "node_type": "1", "metadata": {"window": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n", "original_text": "Therefore the AUC values can differ throughout runs of the experiment. "}, "hash": "37c43b3b223fdbe62218e1fc82f4ed0764bf4d3a622dc6d73692d96678f28889", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. ", "mimetype": "text/plain", "start_char_idx": 44842, "end_char_idx": 45017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38445ead-670f-4ca3-a99b-2fc0bc6fcc67", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n", "original_text": "Therefore the AUC values can differ throughout runs of the experiment. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6973e330-c928-47f7-a809-6f0202d43007", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Experiments on real datasets: U-shaped probability density\n\nTable 3 shows the results of applying U-shaped probability density function.  Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). ", "original_text": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests. "}, "hash": "48c5d4032ad53fab43d4b031cb5f35af569a21c16b68088425e08ca4c949b331", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c379a63d-c6e6-449c-81ab-327a3db17540", "node_type": "1", "metadata": {"window": "### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6. ", "original_text": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. "}, "hash": "156df5beb60eef5625e1e1541126f020a15740af578ede1fe92982cc06237197", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore the AUC values can differ throughout runs of the experiment. ", "mimetype": "text/plain", "start_char_idx": 45017, "end_char_idx": 45088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c379a63d-c6e6-449c-81ab-327a3db17540", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6. ", "original_text": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38445ead-670f-4ca3-a99b-2fc0bc6fcc67", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Interestingly, this solution allowed achieving higher result in case of Annthyroid and Ionosphere datasets.\n\n ### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n", "original_text": "Therefore the AUC values can differ throughout runs of the experiment. "}, "hash": "4e9bf5e863fdce212ecda6d1bf5f287f524b6fc8eb78e92e7e49c86c682b05d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cf76d90-8580-475c-84b4-1f0f78db54e8", "node_type": "1", "metadata": {"window": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. ", "original_text": "Due to that fact standard deviation cannot be obtained for these methods.\n\n"}, "hash": "d26a27aa53c0f445321e2156b6206dda4e9aabe787acadc3eec94d8852aa092a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. ", "mimetype": "text/plain", "start_char_idx": 45088, "end_char_idx": 45190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cf76d90-8580-475c-84b4-1f0f78db54e8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. ", "original_text": "Due to that fact standard deviation cannot be obtained for these methods.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c379a63d-c6e6-449c-81ab-327a3db17540", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.5.  Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6. ", "original_text": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset. "}, "hash": "d0a579e7567dd4d0708f360587c5495f26f832e2b6110976fb2db6e1a98166c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6070d5a9-dfc8-4274-a147-0a1b2aa377e2", "node_type": "1", "metadata": {"window": "Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. ", "original_text": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. "}, "hash": "171a93955b360624942787c0c458e017093eb817655bf30ded846400105ee41c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Due to that fact standard deviation cannot be obtained for these methods.\n\n", "mimetype": "text/plain", "start_char_idx": 45190, "end_char_idx": 45265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6070d5a9-dfc8-4274-a147-0a1b2aa377e2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. ", "original_text": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cf76d90-8580-475c-84b4-1f0f78db54e8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Comparison with selected methods of outlier detection\n\nThe comparison of results (values of AUC) obtained with our method (PGIF), original Isolation Forest, and selected outlier detection methods are presented in Table 4.  Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. ", "original_text": "Due to that fact standard deviation cannot be obtained for these methods.\n\n"}, "hash": "3a777d71dfcab570a1a23135abeb4d19330c28216dd1a4c6e715fecc030855b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80db472c-a36b-42ae-942f-8ab9257e6f13", "node_type": "1", "metadata": {"window": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. ", "original_text": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). "}, "hash": "27f074596bbd24fc4f702cac2bf89dcef7b449885136d5165ba129a9a6d1902b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. ", "mimetype": "text/plain", "start_char_idx": 45265, "end_char_idx": 45399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80db472c-a36b-42ae-942f-8ab9257e6f13", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. ", "original_text": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6070d5a9-dfc8-4274-a147-0a1b2aa377e2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Some datasets have N/A in the 1-class SVM column as time complexity is too high.  Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. ", "original_text": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases. "}, "hash": "371118e8efb32203b3f2928c8434cbc61e93eff27404fc0b63c0a273c4e174c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7f824cd-a9bd-4b08-8f41-1008775a5f3a", "node_type": "1", "metadata": {"window": "Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e. ", "original_text": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n"}, "hash": "2c1f1665c313966891aed6094eb3fe5a247ccbf10ec33a20254ea8865f4449fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). ", "mimetype": "text/plain", "start_char_idx": 45399, "end_char_idx": 45581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7f824cd-a9bd-4b08-8f41-1008775a5f3a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e. ", "original_text": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80db472c-a36b-42ae-942f-8ab9257e6f13", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mean and standard deviation of AUC values are shown for Isolation Forest and its generalization due to the fact that random sample of dataset is used for fitting the forests.  Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. ", "original_text": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio). "}, "hash": "20d178208a7105eb3f505dd99cf2cc07cc3cc0d8bb1052ee1ecab493e263f11d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04cada9e-1253-4ae7-b300-d1d3aa2ed2e3", "node_type": "1", "metadata": {"window": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256). ", "original_text": "### 4.6. "}, "hash": "71981e1b5722fae35d3e373e6e6ff14c7300871fdbce2d0063690d82718cef55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n", "mimetype": "text/plain", "start_char_idx": 45581, "end_char_idx": 45816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04cada9e-1253-4ae7-b300-d1d3aa2ed2e3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256). ", "original_text": "### 4.6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7f824cd-a9bd-4b08-8f41-1008775a5f3a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore the AUC values can differ throughout runs of the experiment.  LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e. ", "original_text": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n"}, "hash": "5aa9b52607fef98ae8699982670fd58e7c52b309b27d2ca782416dd4cd478a49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2892fc0-15fa-4538-9e8c-28aa94167c69", "node_type": "1", "metadata": {"window": "Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. ", "original_text": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. "}, "hash": "291978042c030ff67c4bd3f7a2754f5078d719aaeb4cd7d7aa05753753d32369", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.6. ", "mimetype": "text/plain", "start_char_idx": 45816, "end_char_idx": 45825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2892fc0-15fa-4538-9e8c-28aa94167c69", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. ", "original_text": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04cada9e-1253-4ae7-b300-d1d3aa2ed2e3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "LOF and 1-class SVM, on the other hand, are deterministic algorithms, analyzing the complete dataset.  Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256). ", "original_text": "### 4.6. "}, "hash": "7e594b0c9bdb01bdc214254b54198fa2b03bb7393ba947fdc0680114ab578a7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd8f9494-6114-44e4-be21-48be1e9c906b", "node_type": "1", "metadata": {"window": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond. ", "original_text": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. "}, "hash": "540026a1326c56f2c9619c003a40815f4b42a461aa783490d62316707b778e2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. ", "mimetype": "text/plain", "start_char_idx": 45825, "end_char_idx": 45966, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd8f9494-6114-44e4-be21-48be1e9c906b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond. ", "original_text": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2892fc0-15fa-4538-9e8c-28aa94167c69", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Due to that fact standard deviation cannot be obtained for these methods.\n\n The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. ", "original_text": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure. "}, "hash": "2ecf45404657b0190ebb1f40179df6a6bd77fe12c19725df89ff1c38532800f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dbeebe3-c3f9-4315-9674-5cb09b08a54d", "node_type": "1", "metadata": {"window": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. ", "original_text": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. "}, "hash": "be1ecd40bbb3498373179bd7a068f842bfac2c55782dd345ed50a750a2297c11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. ", "mimetype": "text/plain", "start_char_idx": 45966, "end_char_idx": 46108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5dbeebe3-c3f9-4315-9674-5cb09b08a54d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. ", "original_text": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd8f9494-6114-44e4-be21-48be1e9c906b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The comparison between Isolation Forests shows that the generalized model demonstrated superior performance in the majority of cases.  It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond. ", "original_text": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28]. "}, "hash": "1e43c837572c7f07bb7fa7c7f3774edbed1e688133896c7795283c05e506877d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c755b633-9eed-4d2e-beb8-945411d2abe2", "node_type": "1", "metadata": {"window": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n", "original_text": "The hyperparameters of the Forest, i.e. "}, "hash": "3222c2f99581d8ccf0408f786268ad965768f4553fa38516aa6deafbd1a720e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. ", "mimetype": "text/plain", "start_char_idx": 46108, "end_char_idx": 46214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c755b633-9eed-4d2e-beb8-945411d2abe2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n", "original_text": "The hyperparameters of the Forest, i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5dbeebe3-c3f9-4315-9674-5cb09b08a54d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "It allows to improve the outlier detection performance in the cases, where the other methods were better, ensuring at least not worse or even better results (Ionosphere and Cardio).  Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. ", "original_text": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments. "}, "hash": "397e97a1cc8fbe8466e7ff003ce384af8cc7ab14b4ab36e6dd4795057558750c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce46db6e-6299-4bc1-899d-3805e60ff6ee", "node_type": "1", "metadata": {"window": "### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). ", "original_text": "tree number and sample size were the same (100 and 256). "}, "hash": "542d9fde9c4e503ddfb6e94b420696a7e77763a84e1ecb5ac057163a8e07e5b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The hyperparameters of the Forest, i.e. ", "mimetype": "text/plain", "start_char_idx": 46214, "end_char_idx": 46254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ce46db6e-6299-4bc1-899d-3805e60ff6ee", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). ", "original_text": "tree number and sample size were the same (100 and 256). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c755b633-9eed-4d2e-beb8-945411d2abe2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Taking into account the fact that original Isolation Forest approach can be treated as an individual case of the generalized method, it may be noticed that the family of Isolation Forests ensured better results in 11 out of 14 cases.\n\n ### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n", "original_text": "The hyperparameters of the Forest, i.e. "}, "hash": "f19ad7e2147ada0a2f104363aa4c732444ad18867230692956d6376a1dac53fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59d75e2d-c4b7-4d3d-9494-75dcf012f15c", "node_type": "1", "metadata": {"window": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n", "original_text": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. "}, "hash": "3a27231d1ad1c18d58fb9bcbb0534317b867037051ba6d9259fda03213b874ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "tree number and sample size were the same (100 and 256). ", "mimetype": "text/plain", "start_char_idx": 46254, "end_char_idx": 46311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59d75e2d-c4b7-4d3d-9494-75dcf012f15c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n", "original_text": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce46db6e-6299-4bc1-899d-3805e60ff6ee", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.6.  Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). ", "original_text": "tree number and sample size were the same (100 and 256). "}, "hash": "f1adc0c43a0a7392e6ed8c46a1d1adae9d4bc7a87bbdb1e26fb5ff8ade63063b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ffc1c50-6cd6-4d1d-b569-c822a8009a5f", "node_type": "1", "metadata": {"window": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7. ", "original_text": "The columns containing p-values are located between the pairs of columns to which difference they correspond. "}, "hash": "27569dbb0241c4ebe90a357b786c14236ac06b5f00a180bc06422d71966d4693", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. ", "mimetype": "text/plain", "start_char_idx": 46311, "end_char_idx": 46417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8ffc1c50-6cd6-4d1d-b569-c822a8009a5f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7. ", "original_text": "The columns containing p-values are located between the pairs of columns to which difference they correspond. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59d75e2d-c4b7-4d3d-9494-75dcf012f15c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Comparison with existing modifications of Isolation Forest\n\nA series of experiments was run in accordance with previously adopted procedure.  The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n", "original_text": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF. "}, "hash": "21b6ad239f1ffcc01184d9d88e62c196e2c74a71f11fee8f2783177452b792d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c614e768-ff67-4b9a-b251-53ef5edb60b9", "node_type": "1", "metadata": {"window": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. ", "original_text": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. "}, "hash": "dff5ee4ba386fe2dd3f3dbd82b81cd100aa246f65f2c397fccd885fb7bf47944", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The columns containing p-values are located between the pairs of columns to which difference they correspond. ", "mimetype": "text/plain", "start_char_idx": 46417, "end_char_idx": 46527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c614e768-ff67-4b9a-b251-53ef5edb60b9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. ", "original_text": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ffc1c50-6cd6-4d1d-b569-c822a8009a5f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The proposed generalization was compared to one of the newest modifications of Isolation Forest, namely Extended Isolation Forest (EIF) [28].  A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7. ", "original_text": "The columns containing p-values are located between the pairs of columns to which difference they correspond. "}, "hash": "194a18694e038b4b739b292264ebba99ec331db18f1f23fa441a87c0ef30468d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "073538e6-6082-44d5-9f92-bc8057a217cd", "node_type": "1", "metadata": {"window": "The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected. ", "original_text": "This choice was due to lack of normality in the majority of the obtained results.\n\n"}, "hash": "8dde64b46fc4cb4d50822d9e22da2c00095ef085bf757df225fc63a7c5c3cf3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. ", "mimetype": "text/plain", "start_char_idx": 46527, "end_char_idx": 46645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "073538e6-6082-44d5-9f92-bc8057a217cd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected. ", "original_text": "This choice was due to lack of normality in the majority of the obtained results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c614e768-ff67-4b9a-b251-53ef5edb60b9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "A publicly available implementation of EIF (https://github.com/sahandha/eif) was used in the experiments.  The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. ", "original_text": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied. "}, "hash": "90c1460069e51ee3f62b52189865a4d1f5852320f3be3a0ff9847224dc1263ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ae5c2a6-45cb-4e27-a002-51891c30659d", "node_type": "1", "metadata": {"window": "tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. ", "original_text": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). "}, "hash": "738b6ea9b88930d05aa9eb2717291f40def24677fad75ede0cfb97d27e2016ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This choice was due to lack of normality in the majority of the obtained results.\n\n", "mimetype": "text/plain", "start_char_idx": 46645, "end_char_idx": 46728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ae5c2a6-45cb-4e27-a002-51891c30659d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. ", "original_text": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "073538e6-6082-44d5-9f92-bc8057a217cd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The hyperparameters of the Forest, i.e.  tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected. ", "original_text": "This choice was due to lack of normality in the majority of the obtained results.\n\n"}, "hash": "409deb4d032150619f78a8f2a79e96065b9190d81e65b04e7adb90156a36bb78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d215eec-a98b-4749-9612-b33029d5010c", "node_type": "1", "metadata": {"window": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). ", "original_text": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n"}, "hash": "5e3aa412ea40b7dcb40641f55c96e687443b857aec0c138a843e652f4f38c925", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). ", "mimetype": "text/plain", "start_char_idx": 46728, "end_char_idx": 46842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6d215eec-a98b-4749-9612-b33029d5010c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). ", "original_text": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ae5c2a6-45cb-4e27-a002-51891c30659d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "tree number and sample size were the same (100 and 256).  Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. ", "original_text": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14). "}, "hash": "7526187d65d27e25900df3692088f992384b32468f0e3826329910821558185a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7df8d24-980c-4a55-a0f7-41ced5109829", "node_type": "1", "metadata": {"window": "The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. ", "original_text": "### 4.7. "}, "hash": "88f8829f07b936e409cc4f5434e9280115f26138430d5314548cfcc888204aec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n", "mimetype": "text/plain", "start_char_idx": 46842, "end_char_idx": 46929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d7df8d24-980c-4a55-a0f7-41ced5109829", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. ", "original_text": "### 4.7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d215eec-a98b-4749-9612-b33029d5010c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Table 5 presents the comparison of the results obtained with various methods: Original IF, PGIF, and EIF.  The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). ", "original_text": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n"}, "hash": "db05470b5fd138e6d0e2dc696b1191bd244a303dee7b87851ebcbf5f07cb0d0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0fb272f-bf64-4b3a-8282-4268577c3869", "node_type": "1", "metadata": {"window": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n", "original_text": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. "}, "hash": "dd0a53b947304ed0d37da77befa944410fce4eeb4a6d7b0ec5a4935f297077d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.7. ", "mimetype": "text/plain", "start_char_idx": 46929, "end_char_idx": 46938, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0fb272f-bf64-4b3a-8282-4268577c3869", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n", "original_text": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7df8d24-980c-4a55-a0f7-41ced5109829", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The columns containing p-values are located between the pairs of columns to which difference they correspond.  In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. ", "original_text": "### 4.7. "}, "hash": "c7252b73d685858c07cb61296d7518cc028d83d7d27035cd1fb4347897bbe2b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51c3a5ad-eae7-4d73-b316-585445a51441", "node_type": "1", "metadata": {"window": "This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. ", "original_text": "Therefore, as for the time complexity, theoretically only the training process execution time was affected. "}, "hash": "e6bfe700e53aa39a7e8282b0c5d8fd36f7fd3dd4e5a54c52edf042e0d79039d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. ", "mimetype": "text/plain", "start_char_idx": 46938, "end_char_idx": 47131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51c3a5ad-eae7-4d73-b316-585445a51441", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. ", "original_text": "Therefore, as for the time complexity, theoretically only the training process execution time was affected. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0fb272f-bf64-4b3a-8282-4268577c3869", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In order to test statistical significance of the obtained differences non-parametric Mann-Whitney U test was applied.  This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n", "original_text": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified. "}, "hash": "347ff856ab3e4a2a2dc0c02e784c74d88344538b25757224663a329c252a32a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe5e9896-33ca-4523-a9bb-87ded64c5257", "node_type": "1", "metadata": {"window": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. ", "original_text": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. "}, "hash": "699150593cb59c4116d8fdd619ab3fc20c4ab7e2f843116d2d9349d241a575d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, as for the time complexity, theoretically only the training process execution time was affected. ", "mimetype": "text/plain", "start_char_idx": 47131, "end_char_idx": 47239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe5e9896-33ca-4523-a9bb-87ded64c5257", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. ", "original_text": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51c3a5ad-eae7-4d73-b316-585445a51441", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This choice was due to lack of normality in the majority of the obtained results.\n\n The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. ", "original_text": "Therefore, as for the time complexity, theoretically only the training process execution time was affected. "}, "hash": "cd3468d57dfb0231861b3040dc3983f792ed415db8e2ed277b52c3565b1c4fd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c258271-6e8f-4a54-ab92-af245d89ec69", "node_type": "1", "metadata": {"window": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). ", "original_text": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). "}, "hash": "d34c79ae3545e1415195b848d70c9022fffd55c2dbf5c0ad8292257423395e9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. ", "mimetype": "text/plain", "start_char_idx": 47239, "end_char_idx": 47352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c258271-6e8f-4a54-ab92-af245d89ec69", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). ", "original_text": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe5e9896-33ca-4523-a9bb-87ded64c5257", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The results show that the generalization allows to achieve better results in the majority of cases (8 out of 14).  However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. ", "original_text": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer. "}, "hash": "1b7fa665cac942b9acbc1d6c9e75c9f27570b75d7d513e73acebde1d9778e91d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "591c56c9-ecd7-49f2-8800-f820f2594b5f", "node_type": "1", "metadata": {"window": "### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold. ", "original_text": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. "}, "hash": "f07fb27cf6eda66806dd61dae1b3317386ed817440e5d9121194160a3783e22a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). ", "mimetype": "text/plain", "start_char_idx": 47352, "end_char_idx": 47551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "591c56c9-ecd7-49f2-8800-f820f2594b5f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold. ", "original_text": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c258271-6e8f-4a54-ab92-af245d89ec69", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However, in case of only 4 datasets the EIF approach demonstrated better performance.\n\n ### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). ", "original_text": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset). "}, "hash": "675611c3e402385c27695385be658fc069f4dd59d64f605af332b77a2ca04897", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d7807b3-fce5-44af-a458-27f7888b7543", "node_type": "1", "metadata": {"window": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "However it increased only by 15% (from 767.45 to 911.33).\n\n"}, "hash": "68a8453d17914ba83a1dfb2a2cf92dc766f60bc491a024788d5b874c1ef552dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. ", "mimetype": "text/plain", "start_char_idx": 47551, "end_char_idx": 47721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d7807b3-fce5-44af-a458-27f7888b7543", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "However it increased only by 15% (from 767.45 to 911.33).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "591c56c9-ecd7-49f2-8800-f820f2594b5f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### 4.7.  Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold. ", "original_text": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset. "}, "hash": "f226872ba54f31acd0d3bd79a275b6d0727c45eabebcb5975a1e4ae53f6376eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbc4b0c1-5a72-4c72-b462-249801b25cff", "node_type": "1", "metadata": {"window": "Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. ", "original_text": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. "}, "hash": "a28ed2c8019eb05706ffecccc5f077f095a238ae8e971c8cfed2f133245e587e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However it increased only by 15% (from 767.45 to 911.33).\n\n", "mimetype": "text/plain", "start_char_idx": 47721, "end_char_idx": 47780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbc4b0c1-5a72-4c72-b462-249801b25cff", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. ", "original_text": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d7807b3-fce5-44af-a458-27f7888b7543", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Comparison of computation complexity\n\nIt is worth stressing that the presented generalization deals only with training process, while the stage of anomaly score computation stays not modified.  Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "However it increased only by 15% (from 767.45 to 911.33).\n\n"}, "hash": "846f7aa9bbda7bef6eef0fb0312ded10a5e0683ad702cb2935015b1820446205", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43d12fa6-18c6-4bcf-998f-e16a47d53fce", "node_type": "1", "metadata": {"window": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. ", "original_text": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. "}, "hash": "721df66c867b846b5e58aebb92dcc7a490d4dc6396abaa897cfbef74eabfd311", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. ", "mimetype": "text/plain", "start_char_idx": 47780, "end_char_idx": 48017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "43d12fa6-18c6-4bcf-998f-e16a47d53fce", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. ", "original_text": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbc4b0c1-5a72-4c72-b462-249801b25cff", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Therefore, as for the time complexity, theoretically only the training process execution time was affected.  As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. ", "original_text": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure. "}, "hash": "a8a311dab238b2f2d07b86278f3b71168813977421d93954c3bdb9ada397dd5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57096c36-fef4-4bd5-9b53-fbf3ea881fb9", "node_type": "1", "metadata": {"window": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold. ", "original_text": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). "}, "hash": "a0237634d680ec71c3157cd4d0d61caa79b56134671e85e4cb77a57310dbd8aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. ", "mimetype": "text/plain", "start_char_idx": 48017, "end_char_idx": 48182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57096c36-fef4-4bd5-9b53-fbf3ea881fb9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold. ", "original_text": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43d12fa6-18c6-4bcf-998f-e16a47d53fce", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As Table 6 shows, in all the cases the average time of training a modified model was less than two times longer.  Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. ", "original_text": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2. "}, "hash": "7541be9121fbb5d8d29b0436b7c8535e07548f410bed34ca7687b9d5f14cbd6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e178cdc-8dd7-4fce-b611-f3e88700c786", "node_type": "1", "metadata": {"window": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "The best results in the rows are bold. "}, "hash": "ed8d74dc62015b24dde5f9f0ff333a3b9dfde7320c0fbd09acf127df402bcc09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). ", "mimetype": "text/plain", "start_char_idx": 48182, "end_char_idx": 48286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e178cdc-8dd7-4fce-b611-f3e88700c786", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "The best results in the rows are bold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57096c36-fef4-4bd5-9b53-fbf3ea881fb9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Interestingly, in the case of the large datasets (Mulcross, Credit card, Forest cover) training time of the modified model was even notably shorter (up to 7.3 faster in case of Credit card dataset).  As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold. ", "original_text": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC). "}, "hash": "89dd7d6fab3092fc5e38167d06a67ac7082d9fce9c02137e1d28b605514a40d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76e998c7-2d7f-4c8f-baa4-c3d023cda700", "node_type": "1", "metadata": {"window": "However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). ", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "hash": "c9514cacbba5dc4ec8adb869a1e1637dab2a9f9804626b87a088ae2b0d714f5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The best results in the rows are bold. ", "mimetype": "text/plain", "start_char_idx": 48286, "end_char_idx": 48325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76e998c7-2d7f-4c8f-baa4-c3d023cda700", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). ", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e178cdc-8dd7-4fce-b611-f3e88700c786", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for the total elapsed times, they do not differ notably for the original and modified approaches: The highest difference was registered in case of Ionosphere dataset.  However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "The best results in the rows are bold. "}, "hash": "2622629ee452150500b4d6b721fbe7bb1f76367141d38efcdc4d2b316d7ca9d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ba97fe7-eb63-4b1f-b885-b494bfccfd53", "node_type": "1", "metadata": {"window": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. ", "original_text": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. "}, "hash": "a0267ecf53d2a16735a96a1c198d1ca1a8e0abd5d82a92fee12b2e5af162a92a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If two or more models show similarly best results, no value is marked.\n\n", "mimetype": "text/plain", "start_char_idx": 48325, "end_char_idx": 48397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ba97fe7-eb63-4b1f-b885-b494bfccfd53", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. ", "original_text": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76e998c7-2d7f-4c8f-baa4-c3d023cda700", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "However it increased only by 15% (from 767.45 to 911.33).\n\n We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). ", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "hash": "8982adf70f7b3b81d942683b60af5b8273e356f8d3ce4ddc6b326a2cc86fcdda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a28b9f7-1957-4f4e-8a80-470600e9897d", "node_type": "1", "metadata": {"window": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold. ", "original_text": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. "}, "hash": "fde29f96da1162964c7bb1fd08b351cf17fed9d950e2f56043dfb7afd409174c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. ", "mimetype": "text/plain", "start_char_idx": 48397, "end_char_idx": 49781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a28b9f7-1957-4f4e-8a80-470600e9897d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold. ", "original_text": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ba97fe7-eb63-4b1f-b885-b494bfccfd53", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We suppose that such a drop of training time can be potentially related to the fact that in these datasets the outliers are well separated by gaps from the main part of data, so an isolation tree in PGIF has fairly unbalanced structure.  The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. ", "original_text": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification. "}, "hash": "5b88a09b4286ac2f808f053934d8cedad326579bfdecd4549c75e01e031be2ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cd082b2-cd58-4613-a25f-cf22e82b114b", "node_type": "1", "metadata": {"window": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "The best results in the rows are bold. "}, "hash": "73f3597a9c58de1c626d69d4935953a22253f603c9e26d7b1e548ad5e83f3d6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. ", "mimetype": "text/plain", "start_char_idx": 49781, "end_char_idx": 49897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6cd082b2-cd58-4613-a25f-cf22e82b114b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "The best results in the rows are bold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a28b9f7-1957-4f4e-8a80-470600e9897d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The outliers\n\n---\n**Table 3**\nResults obtained with the original method as well as with PGIF applying U-shaped function for random value generation and k = 0, 1, 2.  Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold. ", "original_text": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result. "}, "hash": "816387711e79bdddd07087372bd2d2b1424368bc4b1e9c95a8e314fc711049f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4c9110a-3418-4dce-8aa6-9b5fe0ccc988", "node_type": "1", "metadata": {"window": "The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "hash": "0034cf469e9c7321384f8f743e261e72fc26bc335ddfbaf3f4603e6f98165631", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The best results in the rows are bold. ", "mimetype": "text/plain", "start_char_idx": 49897, "end_char_idx": 49936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4c9110a-3418-4dce-8aa6-9b5fe0ccc988", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cd082b2-cd58-4613-a25f-cf22e82b114b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Mean and standard deviation of the AUC values are provided in the following format: mean AUC (std AUC).  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n", "original_text": "The best results in the rows are bold. "}, "hash": "7dfabcc00bda7c954af3edb875fa0ec62a04a9f2aca18ca55d444db7c4f6b4cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e945d7e5-545d-426d-a6c7-de8491a1b37b", "node_type": "1", "metadata": {"window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. ", "original_text": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). "}, "hash": "66179942853abb12a0cae920aafd3e2853ea7663b3b5dca261961560dc4fbefa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If two or more models show similarly best results, no value is marked.\n\n", "mimetype": "text/plain", "start_char_idx": 49936, "end_char_idx": 50008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e945d7e5-545d-426d-a6c7-de8491a1b37b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. ", "original_text": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4c9110a-3418-4dce-8aa6-9b5fe0ccc988", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "hash": "89ca3c6de0f6c226c2ee18c41d2369223b1c0bd93ecfd2e7056df8269200d627", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bbe3227-400c-4c01-881a-660bd9f9e4d4", "node_type": "1", "metadata": {"window": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. ", "original_text": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. "}, "hash": "ea6784e43964152c351a132a1d648e931fc454dd94c24baa11731bb392eebf3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). ", "mimetype": "text/plain", "start_char_idx": 50008, "end_char_idx": 51379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8bbe3227-400c-4c01-881a-660bd9f9e4d4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. ", "original_text": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e945d7e5-545d-426d-a6c7-de8491a1b37b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. ", "original_text": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF). "}, "hash": "21f8a295a130e234cd7b9088064c5c82522541148a9704280e30244deb9401b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9f4c4a2-fb7d-4755-a249-3330895dae68", "node_type": "1", "metadata": {"window": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n", "original_text": "Significantly best results in the rows are marked as bold. "}, "hash": "e9dbb83718cf067cb911ecb51788d66ed6c990616ee5e1df001065126a7176d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. ", "mimetype": "text/plain", "start_char_idx": 51379, "end_char_idx": 51510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9f4c4a2-fb7d-4755-a249-3330895dae68", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n", "original_text": "Significantly best results in the rows are marked as bold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bbe3227-400c-4c01-881a-660bd9f9e4d4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | IF | PGIF, U-shaped, k=0 | PGIF, U-shaped, k=1 | PGIF, U-shaped, k=2 |\n| :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.993 (0.001) | 0.994 (0.001) | 0.994 (0.001) |\n| ForestCover | 0.873 (0.024) | 0.922 (0.020) | 0.939 (0.019) | **0.943 (0.023)** |\n| Mulcross | 0.958 (0.008) | 0.975 (0.005) | 0.993 (0.002) | **0.995 (0.001)** |\n| Shuttle | 0.997 (0.001) | 0.997 (0.001) | 0.998 (0.000) | 0.998 (0.001) |\n| Mammography | 0.866 (0.009) | 0.866 (0.009) | 0.728 (0.016) | 0.694 (0.018) |\n| Annthyroid | 0.817 (0.016) | **0.831 (0.019)** | 0.796 (0.019) | 0.776 (0.028) |\n| Satellite | **0.704 (0.014)** | 0.642 (0.014) | 0.657 (0.014) | 0.678 (0.014) |\n| Pima | 0.679 (0.013) | 0.644 (0.015) | 0.661 (0.011) | 0.655 (0.011) |\n| Breastw | 0.988 (0.002) | 0.985 (0.003) | 0.986 (0.002) | 0.986 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.794 (0.012) | **0.796 (0.017)** | 0.788 (0.020) |\n| Ionosphere | 0.855 (0.006) | 0.841 (0.008) | **0.881 (0.004)** | 0.888 (0.005) |\n| Wine | 0.791 (0.041) | 0.713 (0.043) | 0.826 (0.027) | **0.828 (0.024)** |\n| Credit card | 0.948 (0.003) | 0.948 (0.003) | 0.949 (0.003) | 0.949 (0.002) |\n| Cardio | 0.926 (0.011) | 0.932 (0.008) | 0.937 (0.009) | **0.939 (0.011)** |\n---\n\n**Table 4**\nComparison of results obtained by selected outlier detection methods, original Isolation Forest, and the proposed modification.  The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. ", "original_text": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns. "}, "hash": "cbda1a4e17ce4e747a998a80831e331dab3e840e776e9735c1bbd2a66d2ff277", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a784cb6-adac-4102-90d2-368574a14e84", "node_type": "1", "metadata": {"window": "The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig. ", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "hash": "1d003ed1892a5660eae2d3f2a3881f8b20f3180ee9b9ef5f74fa5d5c8ca2b2fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Significantly best results in the rows are marked as bold. ", "mimetype": "text/plain", "start_char_idx": 51510, "end_char_idx": 51569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a784cb6-adac-4102-90d2-368574a14e84", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig. ", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9f4c4a2-fb7d-4755-a249-3330895dae68", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The last column contains the notes showing which setup of the proposed method allowed achieving the highest result.  The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n", "original_text": "Significantly best results in the rows are marked as bold. "}, "hash": "83081b48d809190f7cba3dbeb9a71b690c0d6b5d57b8554ed2e5c25191b91d7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ce00114-9090-45ae-ad1d-cf4b6f9bffb5", "node_type": "1", "metadata": {"window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8.", "original_text": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n"}, "hash": "6c868d9a394c727044f9aedf3279c1edf560f6fbf8e17e065b397e5cee5cd40e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If two or more models show similarly best results, no value is marked.\n\n", "mimetype": "text/plain", "start_char_idx": 51569, "end_char_idx": 51641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ce00114-9090-45ae-ad1d-cf4b6f9bffb5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8.", "original_text": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a784cb6-adac-4102-90d2-368574a14e84", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The best results in the rows are bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig. ", "original_text": "If two or more models show similarly best results, no value is marked.\n\n"}, "hash": "ca9b609ba276ca6fe43bb31818fb41e53071d86e7db41b53d838a19905b2faed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b9fe9b9-4616-461f-b80e-74e41b31ec93", "node_type": "1", "metadata": {"window": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). ", "original_text": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. "}, "hash": "247bb51e2483035161a22f948dd9ed0c712bab33325f3e5f87d8fbcc4f85da20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n", "mimetype": "text/plain", "start_char_idx": 51641, "end_char_idx": 53054, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b9fe9b9-4616-461f-b80e-74e41b31ec93", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). ", "original_text": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ce00114-9090-45ae-ad1d-cf4b6f9bffb5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8.", "original_text": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n"}, "hash": "f4952f9982af875e858b5206dd9926023f94e6fa3457dd5445cc73dc6ac524c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bf39ee3-7dc2-44f6-9cc4-e19b6d21e4db", "node_type": "1", "metadata": {"window": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree. ", "original_text": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. "}, "hash": "401be284b713966c81078d3b94839e821580f7b5cac5f014d327a42765e96223", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. ", "mimetype": "text/plain", "start_char_idx": 53054, "end_char_idx": 53954, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2bf39ee3-7dc2-44f6-9cc4-e19b6d21e4db", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree. ", "original_text": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b9fe9b9-4616-461f-b80e-74e41b31ec93", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | 1-class SVM | LOF (Local Outlier Factor) | IF | The best result of PGIF | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | N/A | 0.337 | 0.994 (0.001) | 0.994 (0.001) | \u2013 |\n| ForestCover | N/A | 0.526 | 0.873 (0.024) | **0.943 (0.023)** | Uniform, k = 2 |\n| Mulcross | N/A | 0.603 | 0.958 (0.008) | **0.995 (0.001)** | k = 2 |\n| Shuttle | 0.986 | 0.524 | 0.997 (0.001) | **0.998 (0.000)** | k = 2 |\n| Mammography | 0.84 | 0.74 | **0.866 (0.009)** | 0.866 (0.009) | U-shaped, k = 0 |\n| Annthyroid | 0.61 | 0.708 | 0.817 (0.016) | **0.831 (0.019)** | U-shaped, k = 0 |\n| Satellite | 0.653 | 0.544 | 0.704 (0.014) | **0.711 (0.014)** | Uniform, k = 1 |\n| Pima | 0.61 | 0.598 | 0.679 (0.013) | **0.686 (0.009)** | Uniform, k = 1 |\n| Breastw | 0.95 | 0.366 | 0.988 (0.002) | 0.989 (0.001) | all PGIFs |\n| Arrhythmia | 0.795 | 0.789 | 0.792 (0.015) | **0.796 (0.017)** | U-shaped, k = 1 |\n| Ionosphere | 0.828 | 0.89 | 0.855 (0.006) | **0.888 (0.005)** | U-shaped, k = 2 |\n| Wine | 0.678 | **0.876** | 0.791 (0.041) | 0.843 (0.020) | Uniform, k = 2 |\n| Credit card | N/A | 0.626 | 0.948 (0.003) | **0.951 (0.002)** | Uniform, k = 1 |\n| Cardio | 0.933 | 0.637 | 0.926 (0.011) | **0.994 (0.001)** | Uniform, k = 2 |\n\n---\n**Table 5**\nComparison of results obtained by various modifications of Isolation Forest (original IF, PGIF and Extended IF).  p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). ", "original_text": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees. "}, "hash": "3e83ad48d33b3627fc8beaeda96238b91ecaa09b8727f5cfb2b2aeb057d70ef3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e612173-9377-4716-a6dc-6c6a5c2b2e75", "node_type": "1", "metadata": {"window": "Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "original_text": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n"}, "hash": "686c17f2fabf862960edfa325658bebf430a750d5d94b429ea4cf957b1c2653e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. ", "mimetype": "text/plain", "start_char_idx": 53954, "end_char_idx": 54059, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e612173-9377-4716-a6dc-6c6a5c2b2e75", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "original_text": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bf39ee3-7dc2-44f6-9cc4-e19b6d21e4db", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "p-values characterizing the statistical significance in the pairs (IF-PGIF and PGIF-EIF) are shown between the respective columns.  Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree. ", "original_text": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level. "}, "hash": "1fc0f14cd2f1dfc9f7906de4bd2469c6e57026a9d8ac4839c9cf310f5b0f155f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1beb184-e4cc-470a-a5bb-476c1445173f", "node_type": "1", "metadata": {"window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. ", "original_text": "**Fig. "}, "hash": "91d180c105637a8144d9620aa691c3c005dbecf1b514bff42dd33975ed0b5472", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n", "mimetype": "text/plain", "start_char_idx": 54059, "end_char_idx": 54275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d1beb184-e4cc-470a-a5bb-476c1445173f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e612173-9377-4716-a6dc-6c6a5c2b2e75", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Significantly best results in the rows are marked as bold.  If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "original_text": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n"}, "hash": "68154a20a2a7d33d390f69fa954cdfcb7639042ff0ac2f6d91de37be85d0b5c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b213e3bf-d644-4a22-a208-037676a8baeb", "node_type": "1", "metadata": {"window": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation. ", "original_text": "8."}, "hash": "b2cd1bfc4aa2e6dfd6668a81c3a7cc41ddbb32a2ce625ff2a047d108e830eda4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 54275, "end_char_idx": 54282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b213e3bf-d644-4a22-a208-037676a8baeb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation. ", "original_text": "8."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1beb184-e4cc-470a-a5bb-476c1445173f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If two or more models show similarly best results, no value is marked.\n\n | Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. ", "original_text": "**Fig. "}, "hash": "00bf20cad5402b2d5bc78ed5ae7c2e7d8a53ca001a0bb908567cabfe8d1c3064", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9d16789-1727-4448-a218-8e5fefbc812b", "node_type": "1", "metadata": {"window": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig. ", "original_text": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). "}, "hash": "de394b2554523f91e342e4fb6bd06ac7a995fabc9f4de1a5c84b21706b6ef3d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8.", "mimetype": "text/plain", "start_char_idx": 54282, "end_char_idx": 54284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9d16789-1727-4448-a218-8e5fefbc812b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig. ", "original_text": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b213e3bf-d644-4a22-a208-037676a8baeb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | IF | p-value (IF-PGIF) | PGIF (best result) | p- value (EIF-PGIF) | EIF |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Http KDDCUP99 | 0.994 (0.001) | 0.4 | 0.994 (0.001) | 1.51\u221910\u207b\u00b9\u00b9 | 0.875 (0.012) |\n| ForestCover | 0.873 (0.024) | 2.3\u221910\u207b\u00b9\u2070 | **0.943 (0.023)** | 1.44\u221910\u207b\u00b9\u2070 | 0.867 (0.028) |\n| Mulcross | 0.958 (0.008) | 1.5\u221910\u207b\u00b9\u00b9 | **0.995 (0.001)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.969 (0.010) |\n| Shuttle | 0.997 (0.001) | 3\u221910\u207b\u2078 | **0.998 (0.000)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.986 (0.002) |\n| Mammography | 0.866 (0.009) | 0.4 | 0.866 (0.009) | 1.84\u221910\u207b\u00b9\u00b9 | 0.829 (0.008) |\n| Annthyroid | 0.817 (0.016) | 4.5\u221910\u207b\u2074 | **0.831 (0.019)** | 1.51\u221910\u207b\u00b9\u00b9 | 0.646 (0.010) |\n| Satellite | 0.704 (0.014) | 0.05 | 0.711 (0.014) | 0.015 | **0.717 (0.009)** |\n| Pima | 0.679 (0.013) | 0.03 | 0.686 (0.009) | 1.19\u221910\u207b\u2077 | **0.701 (0.008)** |\n| Breastw | 0.988 (0.002) | 0.01 | **0.989 (0.001)** | 8.46\u221910\u207b\u00b9\u00b9 | 0.985 (0.002) |\n| Arrhythmia | 0.792 (0.015) | 0.09 | 0.796 (0.017) | 0.04 | **0.804 (0.010)** |\n| Ionosphere | 0.855 (0.006) | 1.51\u221910\u207b\u00b9\u00b9 | 0.888 (0.005) | 1.51\u221910\u207b\u00b9\u00b9 | **0.913 (0.006)** |\n| Wine | 0.791 (0.041) | 1\u221910\u207b\u2077 | **0.843 (0.020)** | 0.03 | 0.826 (0.034) |\n| Credit card | 0.948 (0.003) | 1\u221910\u207b\u2074 | **0.951 (0.002)** | 5.5\u221910\u207b\u00b9\u00b9 | 0.943 (0.003) |\n| Cardio | 0.926 (0.011) | 7.4\u221910\u207b\u2078 | **0.943 (0.008)** | 2.8\u221910\u207b\u2074 | 0.935 (0.008) |\n\n---\n**Table 6**\nComparison of training times required by the original IF and PGIF.\n\n | Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation. ", "original_text": "8."}, "hash": "02d2c0b88c5d892ad175f666d7376892773d4576af428683cf328a4e68589ede", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0751319-11b7-470a-bc4e-5b30070d802f", "node_type": "1", "metadata": {"window": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8. ", "original_text": "Red circles represent the nodes of an isolation tree. "}, "hash": "35fa3a91e71b6adb4fb7a616ddbbfddcf19dcd6b0c98d92786286aadc847a9b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). ", "mimetype": "text/plain", "start_char_idx": 54284, "end_char_idx": 54373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0751319-11b7-470a-bc4e-5b30070d802f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8. ", "original_text": "Red circles represent the nodes of an isolation tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9d16789-1727-4448-a218-8e5fefbc812b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "| Dataset | Mean elapsed time: overall (training) (ms) |\n| :--- | :--- |\n| | **IF** | **PGIF** |\n| Http KDDCUP99 | 41310.59 (215.26) | 41881.81 (279.33) |\n| ForestCover | 880048.41 (337.22) | 893662.79 (185.71) |\n| Mulcross | 283355.18 (869.35) | 276837.14 (149.8) |\n| Shuttle | 50234.12 (371.37) | 50752.26 (460.97) |\n| Mammography | 10256.92 (241.86) | 9933.60 (298.05) |\n| Annthyroid | 20240.37 (276.86) | 20144.52 (339) |\n| Satellite | 6621.77 (424.5) | 6786.98 (633) |\n| Pima | 977.14 (299.68) | 966.52 (350.84) |\n| Breastw | 1974.66 (327.07) | 2117.25 (514.61) |\n| Arrhythmia | 1218.10 (300.63) | 1266.45 (351.77) |\n| Ionosphere | 767.45 (391.83) | 911.33 (523.58) |\n| Wine | 326.99 (189.59) | 340.99 (290.08) |\n| Credit card | 257353.19 (926.62) | 260472.13 (127.61) |\n| Cardio | 4490.13 (239.26) | 4548.80 (310.1) |\n---\n[Description of Figure 8: Two diagrams showing example isolation trees.  Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig. ", "original_text": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B). "}, "hash": "8cd57809dbe958f1ad5ea865a786cbff09eb0e3eefeccd7d528d3c32070d5fa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df396f65-9fe0-4f29-9ae0-540583648c53", "node_type": "1", "metadata": {"window": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n", "original_text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n"}, "hash": "7068f1cb91728bbdf500774d0e1c59ac450390465b392ef0e72950ff65668cd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Red circles represent the nodes of an isolation tree. ", "mimetype": "text/plain", "start_char_idx": 54373, "end_char_idx": 54427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df396f65-9fe0-4f29-9ae0-540583648c53", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n", "original_text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0751319-11b7-470a-bc4e-5b30070d802f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tree (A), built by IF for the Credit card dataset, is very wide and bushy with many nodes at each level.  Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8. ", "original_text": "Red circles represent the nodes of an isolation tree. "}, "hash": "cb6c883f520d0d64b58367a3e2ed9943c48a197362fe2a65338dfa64e4be86b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44d9dbe5-58db-4009-81c1-72c65544d876", "node_type": "1", "metadata": {"window": "**Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5. ", "original_text": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. "}, "hash": "920ea1ad564775276e9f511458691f25847d5fd1bfddfe215d7c6748bd6361b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n", "mimetype": "text/plain", "start_char_idx": 54427, "end_char_idx": 54558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44d9dbe5-58db-4009-81c1-72c65544d876", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5. ", "original_text": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df396f65-9fe0-4f29-9ae0-540583648c53", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tree (B), built by PGIF for the same dataset, is much more linear and sparse, with long chains of nodes, suggesting that outliers are being \"stripped off\" at early stages, leading to a less complex tree structure.]\n\n **Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n", "original_text": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n"}, "hash": "c68b9d8df931cc38fcbe0f3a8251c513217e001ed41a94c917bce5a95a078ff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba532a31-85b3-4187-9ec4-8323c2a36163", "node_type": "1", "metadata": {"window": "8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. ", "original_text": "Lower number of nodes leads to less number of time-consuming operations of split value generation. "}, "hash": "c23af00762c8767449a308aea619e417245ee51b23380a01010ab314ffadf17d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. ", "mimetype": "text/plain", "start_char_idx": 54558, "end_char_idx": 54692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba532a31-85b3-4187-9ec4-8323c2a36163", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. ", "original_text": "Lower number of nodes leads to less number of time-consuming operations of split value generation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44d9dbe5-58db-4009-81c1-72c65544d876", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Fig.  8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5. ", "original_text": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes. "}, "hash": "bd04c89e610c465e47a28a3898c83de8ba2301de7411f05b0d2ba5a8e3782225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8499fc96-0680-43e7-9660-e716ca348337", "node_type": "1", "metadata": {"window": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. ", "original_text": "This situation is shown in Fig. "}, "hash": "641293710ab36b8e1a95edd0ae405ab7117b907471a5287ad498b1503aeeb8d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lower number of nodes leads to less number of time-consuming operations of split value generation. ", "mimetype": "text/plain", "start_char_idx": 54692, "end_char_idx": 54791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8499fc96-0680-43e7-9660-e716ca348337", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. ", "original_text": "This situation is shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba532a31-85b3-4187-9ec4-8323c2a36163", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "8. ** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. ", "original_text": "Lower number of nodes leads to less number of time-consuming operations of split value generation. "}, "hash": "2d1b33f1f92148957ef6d4bbe4fb1a04ff638e900a4c6b4fd9f13cb8aa95151d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bbe4f73-cca5-4968-9f48-d6c85c80d831", "node_type": "1", "metadata": {"window": "Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. ", "original_text": "8. "}, "hash": "0e04154b43557734018c12f0e76b632355143aaf54a0f0eb72acb95a0ada3864", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This situation is shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 54791, "end_char_idx": 54823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2bbe4f73-cca5-4968-9f48-d6c85c80d831", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. ", "original_text": "8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8499fc96-0680-43e7-9660-e716ca348337", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "** Example Isolation trees built for the Credit card dataset by the IF (A) and PGIF (B).  Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. ", "original_text": "This situation is shown in Fig. "}, "hash": "b1115edef657e474b0b32af230e7848362c58465ce22b16fadd6075bdf6f0db5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23bfd812-b6dd-4b68-a747-3e1fe803a441", "node_type": "1", "metadata": {"window": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. ", "original_text": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n"}, "hash": "c3d0665a2364b3cad65663fc04c5f7ba9788130e8245146a25690c1bdae2fb04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8. ", "mimetype": "text/plain", "start_char_idx": 54823, "end_char_idx": 54826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23bfd812-b6dd-4b68-a747-3e1fe803a441", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. ", "original_text": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bbe4f73-cca5-4968-9f48-d6c85c80d831", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Red circles represent the nodes of an isolation tree.  (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. ", "original_text": "8. "}, "hash": "731d4078e05344e3884e6bc831ccb0f284d1eb4aa0bafddb404d24c45e0786f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53e0d8e3-2d31-4947-b8e4-f43b397a789e", "node_type": "1", "metadata": {"window": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. ", "original_text": "## 5. "}, "hash": "f16a4288ea6dbc3029796e87f4de74283ec3a54c92b65b9bb12329e3011d07ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n", "mimetype": "text/plain", "start_char_idx": 54826, "end_char_idx": 55082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "53e0d8e3-2d31-4947-b8e4-f43b397a789e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. ", "original_text": "## 5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23bfd812-b6dd-4b68-a747-3e1fe803a441", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)\n ---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. ", "original_text": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n"}, "hash": "6764ed288fd9f3e3bb39f9ba5807d7c69b5d048d939209e1704e5b0b0fd5ec8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf508430-118e-40ee-b0fe-c7c7bbcb95c4", "node_type": "1", "metadata": {"window": "Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n", "original_text": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. "}, "hash": "6d1d9f521f3cb2eb243058c479d251a5ad24835c56bb5e610fa8b7a3f31c1daa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 5. ", "mimetype": "text/plain", "start_char_idx": 55082, "end_char_idx": 55088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf508430-118e-40ee-b0fe-c7c7bbcb95c4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n", "original_text": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53e0d8e3-2d31-4947-b8e4-f43b397a789e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n\nare isolated at high levels of the tree, so that the tree is formed in rather linear structure, producing lower number of nodes.  Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. ", "original_text": "## 5. "}, "hash": "84f2175551168374adac9ab66496c56e487725da7764545d5d1d7ab117d9f721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "050e63c9-993a-4976-9e70-b7ee75f45b9e", "node_type": "1", "metadata": {"window": "This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. ", "original_text": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. "}, "hash": "1891122d07ab737a2f6198757fdf7937b5501ce38f85b57f6639c8d521bfdecf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. ", "mimetype": "text/plain", "start_char_idx": 55088, "end_char_idx": 55208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "050e63c9-993a-4976-9e70-b7ee75f45b9e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. ", "original_text": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf508430-118e-40ee-b0fe-c7c7bbcb95c4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Lower number of nodes leads to less number of time-consuming operations of split value generation.  This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n", "original_text": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm. "}, "hash": "fa1e4d1030dacf898cef5d29edaf1d180178711a09d122e37e4f5e5a555cac58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "285acc59-23cb-4a7a-9807-148bf769bbf3", "node_type": "1", "metadata": {"window": "8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. ", "original_text": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. "}, "hash": "f1a710445b4f3fe84c3a7ec5918f0685336c89972b02abd6da1e584169c97802", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. ", "mimetype": "text/plain", "start_char_idx": 55208, "end_char_idx": 55427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "285acc59-23cb-4a7a-9807-148bf769bbf3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. ", "original_text": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "050e63c9-993a-4976-9e70-b7ee75f45b9e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "This situation is shown in Fig.  8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. ", "original_text": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods. "}, "hash": "4a080cb86ca129a3c82d2ecb0f24d37ec3e7c404972f40af0f614669d5d8d225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be8681f9-0214-477c-b270-67fe73cdf7e3", "node_type": "1", "metadata": {"window": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. ", "original_text": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. "}, "hash": "6d2482327467ebdbc5292cbb62af37ee85f4b770cee00c2c2feb2611e5e3a260", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. ", "mimetype": "text/plain", "start_char_idx": 55427, "end_char_idx": 55643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be8681f9-0214-477c-b270-67fe73cdf7e3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. ", "original_text": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "285acc59-23cb-4a7a-9807-148bf769bbf3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "8.  The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. ", "original_text": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency. "}, "hash": "4a4cb8ba22f6239de2a32de91ac8172f41f29d09b0fba685cf06680344bbebf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b4433b9-e6ab-468e-b831-adef8a31bb87", "node_type": "1", "metadata": {"window": "## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n", "original_text": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. "}, "hash": "1a2d8f55d0c83354ebc6cfd631af2bb1f565de7c6f93943b941f660206b69f43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. ", "mimetype": "text/plain", "start_char_idx": 55643, "end_char_idx": 55799, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b4433b9-e6ab-468e-b831-adef8a31bb87", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n", "original_text": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be8681f9-0214-477c-b270-67fe73cdf7e3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The figure can be interpreted in such a way that the peripheral less densely populated regions of the dataset are \u2018\u2018stripped off\u201d by early splits resulting in the situation when the tree achieves maximal depth faster thus lowering the computational cost.\n\n ## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. ", "original_text": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection. "}, "hash": "3678ad3f5586846c7f5831d179b1a0d5e633e78c54f23ca515db0d6c7ae034f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44f6b6ba-de0b-4d80-b2dc-dd190023cc00", "node_type": "1", "metadata": {"window": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). ", "original_text": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n"}, "hash": "3567d941ce7cb672f1f77793f4e2d9133cfb480405b17532b61296ae602d6b19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. ", "mimetype": "text/plain", "start_char_idx": 55799, "end_char_idx": 56046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44f6b6ba-de0b-4d80-b2dc-dd190023cc00", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). ", "original_text": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b4433b9-e6ab-468e-b831-adef8a31bb87", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "## 5.  Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n", "original_text": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas. "}, "hash": "fc747ce654bdded4769859fd3dba8fd6de4ac96fd2697fc467ef4c012c5f7e32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c88fd15-dc6b-4dcf-b588-fb3f64359ac2", "node_type": "1", "metadata": {"window": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research. ", "original_text": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. "}, "hash": "9a3535df3891307ff434113467df748a266aa4748e25b1f130877a15fff11787", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n", "mimetype": "text/plain", "start_char_idx": 56046, "end_char_idx": 56138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c88fd15-dc6b-4dcf-b588-fb3f64359ac2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research. ", "original_text": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44f6b6ba-de0b-4d80-b2dc-dd190023cc00", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Conclusions and future work\n\nIn this study, we have presented a novel generalization of the Isolation Forest algorithm.  As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). ", "original_text": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n"}, "hash": "7f9c7860a388c5f291a776aaf0efcb84bd8764d4c4ad3f9db3251c18fac9f736", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60e6222d-2e74-4e39-940d-f783557a527a", "node_type": "1", "metadata": {"window": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n", "original_text": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. "}, "hash": "7ba961f18dddab4cfff1e18e47c3dcdb5ad8fc92f5591c6ca215908aad017fe5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. ", "mimetype": "text/plain", "start_char_idx": 56138, "end_char_idx": 56346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60e6222d-2e74-4e39-940d-f783557a527a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n", "original_text": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c88fd15-dc6b-4dcf-b588-fb3f64359ac2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As for the original Isolation Forest, it is the conceptual simplicity and low time complexity together with high efficiency that are the main advantages of our approach in comparison to other outlier detection methods.  To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research. ", "original_text": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training. "}, "hash": "d70c191c730f91d18e412c9c2fcb4a9e5fc5e01cdac1cee8502a1d6c85719381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ce9f124-13cf-435c-b785-ced69191ec02", "node_type": "1", "metadata": {"window": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. ", "original_text": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. "}, "hash": "4b0829870b6f48fedbb84f4cd5f047251f594acc35dfbe48f03a17ecb4312c30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. ", "mimetype": "text/plain", "start_char_idx": 56346, "end_char_idx": 56459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ce9f124-13cf-435c-b785-ced69191ec02", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. ", "original_text": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60e6222d-2e74-4e39-940d-f783557a527a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "To high extent, the generalization that we have introduced, preserves the strong sides of Isolation Forest related to intuitiveness and time complexity along with further improvement of anomaly detection efficiency.  We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n", "original_text": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets. "}, "hash": "f80bf500cdf68dd10cd5b1899e510d3c2658c32acfdaa244262fffd3777fa345", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e21e109a-577f-4446-9035-e8f41eb4a5c2", "node_type": "1", "metadata": {"window": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n", "original_text": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n"}, "hash": "3f1b2ed73ec3f06288ffd558bf15cf58296ddf47b89215e1423cfccb81a0fa3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. ", "mimetype": "text/plain", "start_char_idx": 56459, "end_char_idx": 56653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e21e109a-577f-4446-9035-e8f41eb4a5c2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n", "original_text": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ce9f124-13cf-435c-b785-ced69191ec02", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We also propose the idea of the U-shaped probability density function used for generating random values specifically for applications in outlier detection.  In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. ", "original_text": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods. "}, "hash": "36e39185dccb467490732fd6a260ba0f9b118acb0155953c6e09bd0d3c8dde7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db0496c3-eecb-48db-9f43-3672c180726a", "node_type": "1", "metadata": {"window": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. ", "original_text": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). "}, "hash": "ce274ccb447c67d7ddd11d70339a5391623811645be44560921ca5db1c86e3b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n", "mimetype": "text/plain", "start_char_idx": 56653, "end_char_idx": 56735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db0496c3-eecb-48db-9f43-3672c180726a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. ", "original_text": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e21e109a-577f-4446-9035-e8f41eb4a5c2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In the series of numerical experiments our model called Probabilistic Generalization of Isolation Forest (PGIF) has proven its usefulness in detection of anomalies located not only far from other groups of records but also in intra-cluster areas.  Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n", "original_text": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n"}, "hash": "7b84f9ed02b9a743a52ce4f417593946a230367d597b9583fef9bd67ac2b54cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50e3d9d3-3809-434d-8a21-a7f69993ae03", "node_type": "1", "metadata": {"window": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n", "original_text": "The fact that EIF in some cases may be the preferable solution shows the path of further research. "}, "hash": "d63724e38bf0eb32de54d15b1479120764b3540d4db8936b0e8416e05f174091", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). ", "mimetype": "text/plain", "start_char_idx": 56735, "end_char_idx": 56936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50e3d9d3-3809-434d-8a21-a7f69993ae03", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n", "original_text": "The fact that EIF in some cases may be the preferable solution shows the path of further research. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db0496c3-eecb-48db-9f43-3672c180726a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Also PGIF easily finds distinct clusters when they are located fairly close to each other.\n\n Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. ", "original_text": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF). "}, "hash": "5a19bedbff5bd5f83a6872f7eabde1eacdd796b459af0dbb44dd41954ad0b1f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d331dd3d-9a62-40cd-9142-c9d69aae6ff4", "node_type": "1", "metadata": {"window": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n", "original_text": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n"}, "hash": "c8914ceb95dd472198c9969ece0557f25cb3428486b35a9bcac8f3d1f2de9f7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The fact that EIF in some cases may be the preferable solution shows the path of further research. ", "mimetype": "text/plain", "start_char_idx": 56936, "end_char_idx": 57035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d331dd3d-9a62-40cd-9142-c9d69aae6ff4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n", "original_text": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50e3d9d3-3809-434d-8a21-a7f69993ae03", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Another notable achievement is the fact that PGIF performs favorably in the case of extensive datasets (Mulcross, Creditcard, Forest cover) both in terms of outlier detection and time complexity of training.  In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n", "original_text": "The fact that EIF in some cases may be the preferable solution shows the path of further research. "}, "hash": "887ab5a83b6ef19fb66a9ba6be0681564492c4daaa03e900323744d9b6736af5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40dac825-3298-42fc-9ab4-be077accd0cb", "node_type": "1", "metadata": {"window": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "original_text": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. "}, "hash": "9242c5c12da58163d859b9f742b882da1056530da9b5c2ad464c0167e4fe4bfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n", "mimetype": "text/plain", "start_char_idx": 57035, "end_char_idx": 57275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40dac825-3298-42fc-9ab4-be077accd0cb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "original_text": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d331dd3d-9a62-40cd-9142-c9d69aae6ff4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In comparison with other algorithms our model demonstrated superior performance for 10 of 14 benchmark datasets.  If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n", "original_text": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n"}, "hash": "0b6bcc45e055ae520164584f4751a49086b3af677390a6da3d9cb712e9439fee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99903c70-abd3-4a05-8a2a-d1ed27300ec4", "node_type": "1", "metadata": {"window": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n", "original_text": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n"}, "hash": "851e09cb4bb97b106c472bdca0586aec1eeae10fb498b5e59fa216dbd268eff5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 57275, "end_char_idx": 57547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99903c70-abd3-4a05-8a2a-d1ed27300ec4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n", "original_text": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40dac825-3298-42fc-9ab4-be077accd0cb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "If we consider the original IF as a special case of our generalization, the obtained results are superior for the Isolation Forest family in 11 out of 14 datasets compared to the other methods.  Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "original_text": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest. "}, "hash": "8289877085d6633b21294c1cf8ebc82629fc8b6eec14776d7030a9d9ae4477c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3142e184-dc73-48bf-b968-ce583ce957e9", "node_type": "1", "metadata": {"window": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C. ", "original_text": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. "}, "hash": "ed451a799801a5296efad8a5e070b956fa83031199700635e118b6280a0b3187", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n", "mimetype": "text/plain", "start_char_idx": 57547, "end_char_idx": 57801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3142e184-dc73-48bf-b968-ce583ce957e9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C. ", "original_text": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99903c70-abd3-4a05-8a2a-d1ed27300ec4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Only for one small dataset (Wine) a competing model (LOF) showed better results.\n\n We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n", "original_text": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n"}, "hash": "be6df28d1c708daad343b3e267727c4dda260273a0406be439094330a2f0789a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57ab6c18-f32b-4920-b386-f0057b496215", "node_type": "1", "metadata": {"window": "The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J. ", "original_text": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n"}, "hash": "4e553d01eb8ca0e9ea1b89fd723445fb4a7e896b9eb22c35e265e972140f9013", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. ", "mimetype": "text/plain", "start_char_idx": 57801, "end_char_idx": 58019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57ab6c18-f32b-4920-b386-f0057b496215", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J. ", "original_text": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3142e184-dc73-48bf-b968-ce583ce957e9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "We conclude that the proposed generalization allows to achieve higher performance in many cases also in comparison with other modifications of Isolation Forest such as Extended Isolation Forest (EIF).  The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C. ", "original_text": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration. "}, "hash": "c599a76ba8eaa26a8b7c81228b58cd4003682a83ec682c0c0b7898306a927f22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7f58eac-cee6-46e4-804c-d0b11e90c249", "node_type": "1", "metadata": {"window": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C. ", "original_text": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n"}, "hash": "e1f7de607f9bf3d5e8eac509765adbdc6bf45fc320157bdab668c05a04997b65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n", "mimetype": "text/plain", "start_char_idx": 58019, "end_char_idx": 58201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7f58eac-cee6-46e4-804c-d0b11e90c249", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C. ", "original_text": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57ab6c18-f32b-4920-b386-f0057b496215", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "The fact that EIF in some cases may be the preferable solution shows the path of further research.  As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J. ", "original_text": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n"}, "hash": "ed910fd2e9918256890257830712075d73bbcf7ccfca770299ceb2a79518935d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0480022b-167d-4dc0-b009-390fcd379e82", "node_type": "1", "metadata": {"window": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n", "original_text": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. "}, "hash": "8d81b778bfdb339aa1a4bfc9e6c7393ac152c0289521edd095c8afd65fb8c499", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n", "mimetype": "text/plain", "start_char_idx": 58201, "end_char_idx": 58416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0480022b-167d-4dc0-b009-390fcd379e82", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n", "original_text": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7f58eac-cee6-46e4-804c-d0b11e90c249", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "As PGIF, EIF and IF are based on slightly different principles and provide different results, one can use them in an aggregated ensemble where various techniques of fuzzy logic can be applied in order to obtain yet more promising results.\n\n Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C. ", "original_text": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n"}, "hash": "bf5c682f05dce9100e6ac2fe721ffcbcda310ba5dc544a94fd2c820c41a628b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d0b0cde-52ae-4cb4-a447-82f90535836f", "node_type": "1", "metadata": {"window": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S. ", "original_text": "2018/28/Z/ST6/00563).\n\n"}, "hash": "3852b8fbea41e3a9d5328c377f41faaf4e9fa4cb44aef0da8a9beea30acabf85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "mimetype": "text/plain", "start_char_idx": 58416, "end_char_idx": 58521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d0b0cde-52ae-4cb4-a447-82f90535836f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S. ", "original_text": "2018/28/Z/ST6/00563).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0480022b-167d-4dc0-b009-390fcd379e82", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Future work would also include analysis of the proposed model in various applications and in the framework of Granular Computing and fuzzy techniques related to modifications of relatively non-intuitive measure of anomaly proposed in original version of Isolation Forest.  Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n", "original_text": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. "}, "hash": "7aa87989ac0d26075affcfa9a5617fd14d14f6a1f94ae19b2a271c70bab61dc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b787c59b-823f-485e-89b8-56c011107e35", "node_type": "1", "metadata": {"window": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S. ", "original_text": "### References\n\n[1] B. Sch\u00f6lkopf, J.C. "}, "hash": "1e85105211cc2fef044e8d612d9290c66926a9edb8de24c583ec972941ab28fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2018/28/Z/ST6/00563).\n\n", "mimetype": "text/plain", "start_char_idx": 58521, "end_char_idx": 58544, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b787c59b-823f-485e-89b8-56c011107e35", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S. ", "original_text": "### References\n\n[1] B. Sch\u00f6lkopf, J.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d0b0cde-52ae-4cb4-a447-82f90535836f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Moreover, it is possible to utilize the aggregation methods and replace the process of summing the anomaly scores resulting from each of the search trees by other operators and to apply weights related to measures of quality of particular search trees.\n\n **CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S. ", "original_text": "2018/28/Z/ST6/00563).\n\n"}, "hash": "5ec3dca03daabfc4f97096308bebad413550d99ddb5b578fb7607bfda1f70b91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be39fe99-c556-428a-88a2-00a361b04a76", "node_type": "1", "metadata": {"window": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f. ", "original_text": "Platt, J. Shawe-Taylor, A.J. "}, "hash": "a6baa49760363d2fbd6aee2af6f8afde02679fa60ef23b5a39f7ba1e9eee655d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### References\n\n[1] B. Sch\u00f6lkopf, J.C. ", "mimetype": "text/plain", "start_char_idx": 58544, "end_char_idx": 58583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be39fe99-c556-428a-88a2-00a361b04a76", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f. ", "original_text": "Platt, J. Shawe-Taylor, A.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b787c59b-823f-485e-89b8-56c011107e35", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**CRediT authorship contribution statement**\n\n**Mikhail Tokovarov:** Conceptualization, Methodology, Validation, Software, Formal analysis, Writing \u2013 original draft, Visualization, Supervision, Project administration.  **Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S. ", "original_text": "### References\n\n[1] B. Sch\u00f6lkopf, J.C. "}, "hash": "bb09f082af9169490bb7a8a39b2fae6ef35e5ae5410e7ead558de45178628318", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25aca80b-36b9-4af8-aff6-cb0d56c0b3bf", "node_type": "1", "metadata": {"window": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n", "original_text": "Smola, R.C. "}, "hash": "23080023accc4a744b0ae6dd623d7eab6b5c0fbecde0059c78609a98b992392e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Platt, J. Shawe-Taylor, A.J. ", "mimetype": "text/plain", "start_char_idx": 58583, "end_char_idx": 58612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "25aca80b-36b9-4af8-aff6-cb0d56c0b3bf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n", "original_text": "Smola, R.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be39fe99-c556-428a-88a2-00a361b04a76", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "**Pawe\u0142 Karczmarek:** Conceptualization, Formal analysis, Resources, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition.\n\n ---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f. ", "original_text": "Platt, J. Shawe-Taylor, A.J. "}, "hash": "7cb2485d2cb4471e375c4f2d7374bc325b6db695992312e1f1941d1201787c95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0286daf1-971f-4f35-84ca-dfac2f726468", "node_type": "1", "metadata": {"window": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n", "original_text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n"}, "hash": "f31b8fa21bf820ae2a307b841453cc9eb6dce551893e96e7ad314f73690fe25d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Smola, R.C. ", "mimetype": "text/plain", "start_char_idx": 58612, "end_char_idx": 58624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0286daf1-971f-4f35-84ca-dfac2f726468", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n", "original_text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25aca80b-36b9-4af8-aff6-cb0d56c0b3bf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "---\n### Declaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\n ### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n", "original_text": "Smola, R.C. "}, "hash": "91143e2e2854d21343449bbc2b12bff8f486e5f9dc9617da9a17bd85a7964bf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bad937ba-70d8-46c6-a1a1-85fdbc46c8d0", "node_type": "1", "metadata": {"window": "2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M. ", "original_text": "[2] X.-S. "}, "hash": "9999e0e2049c90a9ebe215af6a4f64c7a69942ecd8416f029d9913d3bee419c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n", "mimetype": "text/plain", "start_char_idx": 58624, "end_char_idx": 58739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bad937ba-70d8-46c6-a1a1-85fdbc46c8d0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M. ", "original_text": "[2] X.-S. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0286daf1-971f-4f35-84ca-dfac2f726468", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### Acknowledgements\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n", "original_text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n"}, "hash": "f5ed4b6de305911b393ddfbc888206c845d6c51a0b1b11979f5903321004ffdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca0639e5-1077-4d23-b702-891b6870e7cd", "node_type": "1", "metadata": {"window": "### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. ", "original_text": "Gan, J.-S. "}, "hash": "0c7e0c9890c15d8ba33519a93a18b35c2b291fe62a38a189629fbf712025e9ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] X.-S. ", "mimetype": "text/plain", "start_char_idx": 58739, "end_char_idx": 58749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca0639e5-1077-4d23-b702-891b6870e7cd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. ", "original_text": "Gan, J.-S. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bad937ba-70d8-46c6-a1a1-85fdbc46c8d0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2018/28/Z/ST6/00563).\n\n ### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M. ", "original_text": "[2] X.-S. "}, "hash": "fc548173e0451c98a5e1f9675b20c55531fdba1485ca246da07d92f3d557844b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e545d638-f116-4f4e-881b-ff1e39a2ebd2", "node_type": "1", "metadata": {"window": "Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n", "original_text": "Duanmu, J.-f. "}, "hash": "a106024eedbd81b450c4e560a27471c32c44ce7eab7ad9ab18ddaf07a123d0df", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Gan, J.-S. ", "mimetype": "text/plain", "start_char_idx": 58749, "end_char_idx": 58760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e545d638-f116-4f4e-881b-ff1e39a2ebd2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n", "original_text": "Duanmu, J.-f. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca0639e5-1077-4d23-b702-891b6870e7cd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "### References\n\n[1] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. ", "original_text": "Gan, J.-S. "}, "hash": "976148f316835874b2e7f957711b0a785d3cc559ecd14575057d6fa8120d6b9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "988c396c-f6bd-49eb-b409-82f1e71c70f9", "node_type": "1", "metadata": {"window": "Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "original_text": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n"}, "hash": "f30f9643b0fd5dea86c7971b12900157f6c0f9a43150a2568c725488ebba91f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Duanmu, J.-f. ", "mimetype": "text/plain", "start_char_idx": 58760, "end_char_idx": 58774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "988c396c-f6bd-49eb-b409-82f1e71c70f9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "original_text": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e545d638-f116-4f4e-881b-ff1e39a2ebd2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n", "original_text": "Duanmu, J.-f. "}, "hash": "de27f6b376b07c78fc31ca0ae7185e11a0c7a3c3d87a6a63f2ed2c99e55db748", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5917e000-8849-427f-a42b-9b300239339d", "node_type": "1", "metadata": {"window": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n", "original_text": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n"}, "hash": "eb253c5859cdba3d4adec84cca360007881d424024755d3cae289f723bf750c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n", "mimetype": "text/plain", "start_char_idx": 58774, "end_char_idx": 58913, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5917e000-8849-427f-a42b-9b300239339d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n", "original_text": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "988c396c-f6bd-49eb-b409-82f1e71c70f9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "original_text": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n"}, "hash": "dd60143c7fbab9753dfc6c90df895ece38e2bd7a1078155cab352591a15b2543", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ef70100-b8aa-46e2-a091-e937a2c4e2e4", "node_type": "1", "metadata": {"window": "[2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C. ", "original_text": "[4] S.M. "}, "hash": "0d55674d2691e3dd0437940f1b58e18426261e714164f16afa17c92fe60a8072", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n", "mimetype": "text/plain", "start_char_idx": 58913, "end_char_idx": 59160, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ef70100-b8aa-46e2-a091-e937a2c4e2e4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C. ", "original_text": "[4] S.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5917e000-8849-427f-a42b-9b300239339d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Williamson, Estimating the support of a high-dimensional distribution, Neural Computation 13 (7) (2001) 1443\u20131471.\n [2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n", "original_text": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n"}, "hash": "b4617f2b14351cb9fcca49c3811e15790f6400ecc5af14c3fd997fe851ad80fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "114f3598-e5cd-4804-a0d7-884d8b2433e9", "node_type": "1", "metadata": {"window": "Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "original_text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. "}, "hash": "28f40b4589d1536f756593bf3f75ffd8b4bbdc0c9904a1217e2cd8eb8f88807e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] S.M. ", "mimetype": "text/plain", "start_char_idx": 59160, "end_char_idx": 59169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "114f3598-e5cd-4804-a0d7-884d8b2433e9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "original_text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ef70100-b8aa-46e2-a091-e937a2c4e2e4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[2] X.-S.  Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C. ", "original_text": "[4] S.M. "}, "hash": "9719f3412deffa52e549b628b5b5dde4ed17bcdba9d3e3864c9f2a537966172e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "761d572a-17ee-4389-88fd-db3dbb4f23e3", "node_type": "1", "metadata": {"window": "Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n", "original_text": "58 (2016) 121\u2013134.\n"}, "hash": "ead83ef22922a691c16297c35023a312e2779eab77aaee3b17108e7e6de4626f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. ", "mimetype": "text/plain", "start_char_idx": 59169, "end_char_idx": 59342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "761d572a-17ee-4389-88fd-db3dbb4f23e3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n", "original_text": "58 (2016) 121\u2013134.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "114f3598-e5cd-4804-a0d7-884d8b2433e9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Gan, J.-S.  Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "original_text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition. "}, "hash": "0385bb5266f659af7c7f4e0dd1fae62a9f1361a989b107d4e6229ac3bd022d01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81afd51c-8582-4e45-8320-333954b3e8fc", "node_type": "1", "metadata": {"window": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B. ", "original_text": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. "}, "hash": "4062d0fc28dd001c2ca155426e71fbff8c494b8032042bea17de8e4538453d91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "58 (2016) 121\u2013134.\n", "mimetype": "text/plain", "start_char_idx": 59342, "end_char_idx": 59361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "81afd51c-8582-4e45-8320-333954b3e8fc", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B. ", "original_text": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "761d572a-17ee-4389-88fd-db3dbb4f23e3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Duanmu, J.-f.  Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n", "original_text": "58 (2016) 121\u2013134.\n"}, "hash": "dbbbab89fa942a75fae7370ee78b5a266814737db5c18510b8bec3a1b110aac8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed13c564-439c-4ee6-9e16-1304903fbb94", "node_type": "1", "metadata": {"window": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. ", "original_text": "89\u201394.\n"}, "hash": "e9351a7aafb3bf62acf351653635a3084bdf126c8d22262fb2013485137ad1ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "mimetype": "text/plain", "start_char_idx": 59361, "end_char_idx": 59596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed13c564-439c-4ee6-9e16-1304903fbb94", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. ", "original_text": "89\u201394.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81afd51c-8582-4e45-8320-333954b3e8fc", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowledge-Based Systems 40 (2013) 1\u20136.\n [3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B. ", "original_text": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. "}, "hash": "a7636fa6a749465e9b91feaaa321fe373cc7e06f73e376ccb34002e79d76dfda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fdf852c-3422-4c13-9076-f6c7d66749c7", "node_type": "1", "metadata": {"window": "[4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n", "original_text": "[6] C. Zhou, R.C. "}, "hash": "60f2c25e97b628fe35416ae9682128f68255be3fb28f2e270cd93e0567e1c441", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "89\u201394.\n", "mimetype": "text/plain", "start_char_idx": 59596, "end_char_idx": 59603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3fdf852c-3422-4c13-9076-f6c7d66749c7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n", "original_text": "[6] C. Zhou, R.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed13c564-439c-4ee6-9e16-1304903fbb94", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[3] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowledge-Based Systems 71 (2014) 322\u2013338.\n [4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. ", "original_text": "89\u201394.\n"}, "hash": "83428acd0d20bcbe8db2dc93c4edea43bfff2875aef9ac0df783e10e7ecd4262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf055962-5091-4bac-aa72-139b0d0b2eb1", "node_type": "1", "metadata": {"window": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n", "original_text": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. "}, "hash": "53724ccd14a41bae9a1560eaa0e41fb725d0579c711982bf83e49611464395a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] C. Zhou, R.C. ", "mimetype": "text/plain", "start_char_idx": 59603, "end_char_idx": 59621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf055962-5091-4bac-aa72-139b0d0b2eb1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n", "original_text": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fdf852c-3422-4c13-9076-f6c7d66749c7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[4] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n", "original_text": "[6] C. Zhou, R.C. "}, "hash": "916bee58da93cc500c354dba6c7ce5c9b2280412f0ecb90a3695599c25e25725", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05150551-6c1e-4fe4-883d-54252d5615a3", "node_type": "1", "metadata": {"window": "58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H. ", "original_text": "665\u2013674.\n"}, "hash": "0c69dc38d2ae1d8031d66e0e584402bf9e2f8119564a7b12bdfe4c3dec76c07b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "mimetype": "text/plain", "start_char_idx": 59621, "end_char_idx": 59809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05150551-6c1e-4fe4-883d-54252d5615a3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H. ", "original_text": "665\u2013674.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf055962-5091-4bac-aa72-139b0d0b2eb1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognition.  58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n", "original_text": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. "}, "hash": "1f8723a6c36ce61310222ee91b7986b35beb001e6b0986b4a731f2ff39f21945", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb8671a7-a23d-4f93-b3ab-9761c1e5b1b0", "node_type": "1", "metadata": {"window": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M. ", "original_text": "[7] R.J.G.B. "}, "hash": "3bd78e126d4177073f30eff0bc7d3323005e56de5a9c96686955ecd21c207bf7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "665\u2013674.\n", "mimetype": "text/plain", "start_char_idx": 59809, "end_char_idx": 59818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb8671a7-a23d-4f93-b3ab-9761c1e5b1b0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M. ", "original_text": "[7] R.J.G.B. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05150551-6c1e-4fe4-883d-54252d5615a3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "58 (2016) 121\u2013134.\n [5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H. ", "original_text": "665\u2013674.\n"}, "hash": "e474fb203602304aa8f76d31a160fb7ffd5dc1cc5a698bde21e550535867be12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89165378-f142-4dfe-9616-bb4fee56042d", "node_type": "1", "metadata": {"window": "89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. ", "original_text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. "}, "hash": "3f4c30f9337a28eee40d6885ed54d05af126516a6d7d7ec18a802d363aec8a5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[7] R.J.G.B. ", "mimetype": "text/plain", "start_char_idx": 59818, "end_char_idx": 59831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "89165378-f142-4dfe-9616-bb4fee56042d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. ", "original_text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb8671a7-a23d-4f93-b3ab-9761c1e5b1b0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[5] P. Malhotra, L. Vig, G. Shroff, G., P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M. ", "original_text": "[7] R.J.G.B. "}, "hash": "12f704f424f38ac9601de6205be2ee9b7b6ad68e3f6b0ff7717ec67ae5aecde1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "508344cc-9525-4ec6-a33d-6e7e2dd050c2", "node_type": "1", "metadata": {"window": "[6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n", "original_text": "5.\n"}, "hash": "bd5227316644e61c471750159bcaea4ca9131b84ce0e64ffe9c188b7c6199c1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. ", "mimetype": "text/plain", "start_char_idx": 59831, "end_char_idx": 60040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "508344cc-9525-4ec6-a33d-6e7e2dd050c2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n", "original_text": "5.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89165378-f142-4dfe-9616-bb4fee56042d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "89\u201394.\n [6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. ", "original_text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no. "}, "hash": "f11b6ae2530a4fe7099cd15247e35398891b1baa7c742e4e019ff2826f41b9b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a2abb96-f44d-48e4-b371-1d9ab455f0af", "node_type": "1", "metadata": {"window": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. ", "original_text": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n"}, "hash": "9398ab61cecbeeb09810e37bd385037fa50e70bf174d71de485e2c9d43359e89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5.\n", "mimetype": "text/plain", "start_char_idx": 60040, "end_char_idx": 60043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a2abb96-f44d-48e4-b371-1d9ab455f0af", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. ", "original_text": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "508344cc-9525-4ec6-a33d-6e7e2dd050c2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[6] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n", "original_text": "5.\n"}, "hash": "f97f8f0033464588ee3a42b27f01b82151c578bd74907c94294dd864bec8aaaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be1b2bee-b9dd-422c-936b-69069e9579f8", "node_type": "1", "metadata": {"window": "665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2. ", "original_text": "[9] W. Chimphlee, A.H. "}, "hash": "493152581f35e263faddb92507d8eb8ce64d655aa288e55ec95ec9265bd1ad29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n", "mimetype": "text/plain", "start_char_idx": 60043, "end_char_idx": 60190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be1b2bee-b9dd-422c-936b-69069e9579f8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2. ", "original_text": "[9] W. Chimphlee, A.H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a2abb96-f44d-48e4-b371-1d9ab455f0af", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Paffenroth, Anomaly detection with robust deep autoencoders, KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. ", "original_text": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n"}, "hash": "94a3b5cb2862bc711176cb1a1fb916d59edf78273b475402547ddc6f81fdb961", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "432184fd-45db-4739-9741-4b29c0defd58", "node_type": "1", "metadata": {"window": "[7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp. ", "original_text": "Abdullah, M.N.M. "}, "hash": "1de0684bbe6e16f92ad0ef3990b1faed01353771c2eefaf68715ded2a9ef33ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] W. Chimphlee, A.H. ", "mimetype": "text/plain", "start_char_idx": 60190, "end_char_idx": 60213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "432184fd-45db-4739-9741-4b29c0defd58", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp. ", "original_text": "Abdullah, M.N.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be1b2bee-b9dd-422c-936b-69069e9579f8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "665\u2013674.\n [7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2. ", "original_text": "[9] W. Chimphlee, A.H. "}, "hash": "6eb229794f380dc5009ed24cb6963ff11e9045d82b419d6cfcf72c807d9a4116", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9cde574-a8a7-41bf-8e99-b8b6ded8c57d", "node_type": "1", "metadata": {"window": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n", "original_text": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. "}, "hash": "434d7ec284d2e424afc11b40df9bccbedb31edbea6635a68bc393024d13b8dc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Abdullah, M.N.M. ", "mimetype": "text/plain", "start_char_idx": 60213, "end_char_idx": 60230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9cde574-a8a7-41bf-8e99-b8b6ded8c57d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n", "original_text": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "432184fd-45db-4739-9741-4b29c0defd58", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[7] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp. ", "original_text": "Abdullah, M.N.M. "}, "hash": "9efea1e1fab86f6f82f8b9c964960fe4efa5dc3ae2f3c75ad4e9e4795973752d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14434a9b-e065-4e0e-8131-56017c7be2cf", "node_type": "1", "metadata": {"window": "5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D. ", "original_text": "329\u2013334.\n"}, "hash": "23a3f145c5a4e284e502cab4d173adeccbfa9ccd2a714d8d8cb60974c5fc71a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. ", "mimetype": "text/plain", "start_char_idx": 60230, "end_char_idx": 60376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14434a9b-e065-4e0e-8131-56017c7be2cf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D. ", "original_text": "329\u2013334.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9cde574-a8a7-41bf-8e99-b8b6ded8c57d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Transactions on Knowledge Discovery from Data 10 (1) (2015) article no.  5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n", "original_text": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp. "}, "hash": "f4e16b5b8d25325f090a3f8507eab13b4049ee3575e96516900ccd9767258f71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9da97659-90e2-485b-9252-dddf49669436", "node_type": "1", "metadata": {"window": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n", "original_text": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. "}, "hash": "e2dea29b272ba4251f461ddfd68de3821b5221072dc9af3ee05723332b46a1d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "329\u2013334.\n", "mimetype": "text/plain", "start_char_idx": 60376, "end_char_idx": 60385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9da97659-90e2-485b-9252-dddf49669436", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n", "original_text": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14434a9b-e065-4e0e-8131-56017c7be2cf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "5.\n [8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D. ", "original_text": "329\u2013334.\n"}, "hash": "d4d9bb243dc9f1a91371176e0393e5273ed64eb78f6ce4c7bb58b0ec72e3c56c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dd4d376-c95f-4064-9522-a1a5fbc5b435", "node_type": "1", "metadata": {"window": "[9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H. ", "original_text": "2. "}, "hash": "0d6a0716e8989b95b63121021d799869a184461e74e50a28fbac4efaccecfa5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. ", "mimetype": "text/plain", "start_char_idx": 60385, "end_char_idx": 60549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0dd4d376-c95f-4064-9522-a1a5fbc5b435", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H. ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9da97659-90e2-485b-9252-dddf49669436", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[8] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Journal of Network Security, Computer Networks 8 (2007) 43\u201346.\n [9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n", "original_text": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol. "}, "hash": "41827372b7cad42fd7fe64b85227cf584bb8d83d4e2ec52ad08cb7d8e4b44109", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c130b5de-47f8-412b-9200-9f50f4c79ada", "node_type": "1", "metadata": {"window": "Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n", "original_text": "St Louis, 2003, pp. "}, "hash": "fce57cedc2fa582ce2496aa3d5519521c2cb2beeac979efe526b75aa04821286", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 60549, "end_char_idx": 60552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c130b5de-47f8-412b-9200-9f50f4c79ada", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n", "original_text": "St Louis, 2003, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dd4d376-c95f-4064-9522-a1a5fbc5b435", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[9] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H. ", "original_text": "2. "}, "hash": "ec5c6c7a2d9cc7672fdb2cd33cc0217778b68d52adb74624c0ba2a8f3b4779be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63c5e036-33e2-4805-91a3-111a80e6ea91", "node_type": "1", "metadata": {"window": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n", "original_text": "1219\u20131224.\n"}, "hash": "0ab0a7d4e0a18e49fe99c758a526eae7a2b3160bc4d656e426f5d0bd9a4cae05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "St Louis, 2003, pp. ", "mimetype": "text/plain", "start_char_idx": 60552, "end_char_idx": 60572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63c5e036-33e2-4805-91a3-111a80e6ea91", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n", "original_text": "1219\u20131224.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c130b5de-47f8-412b-9200-9f50f4c79ada", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n", "original_text": "St Louis, 2003, pp. "}, "hash": "083cf31296668c07a1e81541588803fef5c94f9a7fc412f16b40129a4288c116", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3daea43-ed86-461b-89f5-1e02b563d028", "node_type": "1", "metadata": {"window": "329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "original_text": "[11] X.D. "}, "hash": "afc3f683dcbb21b6cf1a5c01488205ded4f1f88504164f32f496c5ad8df50bf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1219\u20131224.\n", "mimetype": "text/plain", "start_char_idx": 60572, "end_char_idx": 60583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3daea43-ed86-461b-89f5-1e02b563d028", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "original_text": "[11] X.D. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63c5e036-33e2-4805-91a3-111a80e6ea91", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Sap, S. Srinoy, S. Chimphlee, in: Anomaly-based intrusion detection using fuzzy rough clustering, Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n", "original_text": "1219\u20131224.\n"}, "hash": "9b5ac70cff72faad95bbd4c98481c9d6b42bf5ee3a40d30b481fd8c9e489a0f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5adb4f8-519a-42ca-8f6f-d8530b30f12b", "node_type": "1", "metadata": {"window": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n", "original_text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n"}, "hash": "bb9d1332fc037bf9887c48095fa653b7fe822ff32d6a28bc2cc409f78b3e7ef7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] X.D. ", "mimetype": "text/plain", "start_char_idx": 60583, "end_char_idx": 60593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c5adb4f8-519a-42ca-8f6f-d8530b30f12b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n", "original_text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3daea43-ed86-461b-89f5-1e02b563d028", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "329\u2013334.\n [10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "original_text": "[11] X.D. "}, "hash": "a693fac2ff0fe764c171cbce1dfd2ab04f55432c81dc9fe4314c4f0e02891a92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37249481-37c9-459b-ae1d-b4410f9fa005", "node_type": "1", "metadata": {"window": "2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n", "original_text": "[12] C.-H. "}, "hash": "d6684730da7b8e58ca89e4440065850fd638f9c82c6a9053fd10dc0e630f8947", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n", "mimetype": "text/plain", "start_char_idx": 60593, "end_char_idx": 60793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37249481-37c9-459b-ae1d-b4410f9fa005", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n", "original_text": "[12] C.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5adb4f8-519a-42ca-8f6f-d8530b30f12b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[10] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903.,vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n", "original_text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n"}, "hash": "5bff32fd403faf2cace48787011ee79e96ba3275b3566d7540d56990f6a255b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c9b143b-348d-40d5-9b0e-9324a234c284", "node_type": "1", "metadata": {"window": "St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M. ", "original_text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n"}, "hash": "4a7e7fb8a7bdec52488cec23480b6076055b0433676a770f032fe5de98649849", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] C.-H. ", "mimetype": "text/plain", "start_char_idx": 60793, "end_char_idx": 60804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c9b143b-348d-40d5-9b0e-9324a234c284", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M. ", "original_text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37249481-37c9-459b-ae1d-b4410f9fa005", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "2.  St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n", "original_text": "[12] C.-H. "}, "hash": "cdd5761a5f1577001de7b4f2694a3ab80684f0823c8afbcea08a82355e6fd952", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dffd7d8-f7f2-472e-a9b0-35d80a2ac3f6", "node_type": "1", "metadata": {"window": "1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A. ", "original_text": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n"}, "hash": "a0c0f32ff74a26c1f1da9674a25a7de272d4893a89208095b82135185057de12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n", "mimetype": "text/plain", "start_char_idx": 60804, "end_char_idx": 60990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4dffd7d8-f7f2-472e-a9b0-35d80a2ac3f6", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A. ", "original_text": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c9b143b-348d-40d5-9b0e-9324a234c284", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "St Louis, 2003, pp.  1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M. ", "original_text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n"}, "hash": "b90bb7bd093c113a1eb8ebf0772a524f99d983c2d52c2031d8fb649ee400263e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e59ea5e6-45b4-49ad-a358-f71b0fc7b1bb", "node_type": "1", "metadata": {"window": "[11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A. ", "original_text": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. "}, "hash": "dd9ab1c67c6f2467a9da739bbe1080693c9d59c8822a8dedacb4e2716b3d0454", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n", "mimetype": "text/plain", "start_char_idx": 60990, "end_char_idx": 61192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e59ea5e6-45b4-49ad-a358-f71b0fc7b1bb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A. ", "original_text": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dffd7d8-f7f2-472e-a9b0-35d80a2ac3f6", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1219\u20131224.\n [11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A. ", "original_text": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n"}, "hash": "4f91df4379be73721578c37398edec81a6059dd4bd64966e1af50543db93945d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65545e5b-b95e-4ceb-add3-c358fe73de17", "node_type": "1", "metadata": {"window": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n", "original_text": "1513\u20131518.\n"}, "hash": "fd03c9a48d1d384b149631527866421b5e5431a8178f0adae96dbc5d47ca7569", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "mimetype": "text/plain", "start_char_idx": 61192, "end_char_idx": 61393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65545e5b-b95e-4ceb-add3-c358fe73de17", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n", "original_text": "1513\u20131518.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e59ea5e6-45b4-49ad-a358-f71b0fc7b1bb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[11] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A. ", "original_text": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. "}, "hash": "373c3a6cc840c34abba436f40adf620ab6c2e741b07eeaf7a77f6ce113b51ea2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cc9d4a6-9cb8-44bb-ba53-4fe4a4a5f2c8", "node_type": "1", "metadata": {"window": "[12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "original_text": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n"}, "hash": "076c53e22d3e2f5ae85ff073f2b3ae8e0bb2decc002c22877d90ad945e1ad355", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1513\u20131518.\n", "mimetype": "text/plain", "start_char_idx": 61393, "end_char_idx": 61404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6cc9d4a6-9cb8-44bb-ba53-4fe4a4a5f2c8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "original_text": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65545e5b-b95e-4ceb-add3-c358fe73de17", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, Journal of Network and Computer Applications 32 (6) (2009) 1219\u20131228.\n [12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n", "original_text": "1513\u20131518.\n"}, "hash": "84c34c56d056d720b8916c2b1e944cc4d3421659d9c097dc92ab963cdf89c289", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271b8640-42ea-4bd6-8e24-ea83e5a09113", "node_type": "1", "metadata": {"window": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf. ", "original_text": "[16] H. Faris, A.M. "}, "hash": "a5376a0ef15b9446c0c21c26cd9c631f300d50e949c3dbf79409ac012bbc6843", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n", "mimetype": "text/plain", "start_char_idx": 61404, "end_char_idx": 61589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "271b8640-42ea-4bd6-8e24-ea83e5a09113", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf. ", "original_text": "[16] H. Faris, A.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cc9d4a6-9cb8-44bb-ba53-4fe4a4a5f2c8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[12] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "original_text": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n"}, "hash": "ae1b24f2eb0a5b29268a13f79df9c391cab3d1fcfcef168d09e5b25fea7e1f01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21ce1cf3-5a24-4dd8-99be-a73c0c3abb37", "node_type": "1", "metadata": {"window": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n", "original_text": "Al-Zoubi, A.A. "}, "hash": "bce5e83abe831a66abc20a1ed1059343cbee9f4eb4af391ec5834d5b206e1408", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] H. Faris, A.M. ", "mimetype": "text/plain", "start_char_idx": 61589, "end_char_idx": 61609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21ce1cf3-5a24-4dd8-99be-a73c0c3abb37", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n", "original_text": "Al-Zoubi, A.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271b8640-42ea-4bd6-8e24-ea83e5a09113", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognition 40 (9) (2007) 2373\u20132391.\n [13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf. ", "original_text": "[16] H. Faris, A.M. "}, "hash": "e4058f45e05a8aa881bfe10533543b3ac3db9191367b0dbbf94d9253cbaf899b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e9696ec-de5d-4103-96d0-e798993973ca", "node_type": "1", "metadata": {"window": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. ", "original_text": "Heidari, I. Aljarah, M. Mafarja, M.A. "}, "hash": "b73bdb859af0bc20553fcaba4831cfe5c5dab4b83f647c6713aa8213ecae3469", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Al-Zoubi, A.A. ", "mimetype": "text/plain", "start_char_idx": 61609, "end_char_idx": 61624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e9696ec-de5d-4103-96d0-e798993973ca", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. ", "original_text": "Heidari, I. Aljarah, M. Mafarja, M.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21ce1cf3-5a24-4dd8-99be-a73c0c3abb37", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[13] A. Kiersztyn, P. Karczmarek, K. Kiersztyn, W. Pedrycz, Detection and Classification of Anomalies in Large Data Sets on the Basis of Information Granules, IEEE Transactions on Fuzzy Systems (2021).\n [14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n", "original_text": "Al-Zoubi, A.A. "}, "hash": "17fb8188a7f9451ad35b0a45f7a57de33375b93a5c0e8d3e87a28aa9f4a0cf8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "623448dc-62eb-4b1e-91ba-797e574607a0", "node_type": "1", "metadata": {"window": "1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int. ", "original_text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n"}, "hash": "b433c7e911955d9f5fd07ab57d51b9d0fc3982e51e4410427c35fb1a002e4da3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Heidari, I. Aljarah, M. Mafarja, M.A. ", "mimetype": "text/plain", "start_char_idx": 61624, "end_char_idx": 61662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "623448dc-62eb-4b1e-91ba-797e574607a0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int. ", "original_text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e9696ec-de5d-4103-96d0-e798993973ca", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[14] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. ", "original_text": "Heidari, I. Aljarah, M. Mafarja, M.A. "}, "hash": "85f640868791479b6eef55ca7670d5a1655b0c53aff06a8572038f4375d62841", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5012737c-7d8b-4388-8bdd-d8dbb0821f06", "node_type": "1", "metadata": {"window": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf. ", "original_text": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. "}, "hash": "cd8941d464d3842d5a3d055d4a0420e4199ee7492d347f6e3bb281d47558b5c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n", "mimetype": "text/plain", "start_char_idx": 61662, "end_char_idx": 61855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5012737c-7d8b-4388-8bdd-d8dbb0821f06", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf. ", "original_text": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "623448dc-62eb-4b1e-91ba-797e574607a0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1513\u20131518.\n [15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int. ", "original_text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n"}, "hash": "e5132022015f6a5c7750e56e5ac77731aa312a150084fe017df16390402860cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe58c736-a11a-490a-81af-f77a6e184e3d", "node_type": "1", "metadata": {"window": "[16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp. ", "original_text": "Conf. "}, "hash": "c3786a6479ed05ba664fd12f2794c953444b3ba2e28e142e2219cd4decb10bce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "mimetype": "text/plain", "start_char_idx": 61855, "end_char_idx": 61965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe58c736-a11a-490a-81af-f77a6e184e3d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5012737c-7d8b-4388-8bdd-d8dbb0821f06", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[15] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Transactions on Fuzzy Systems 22 (6) (2014) 1612\u20131624.\n [16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf. ", "original_text": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. "}, "hash": "16a4f5e46c669fd7b394194663978b4cb94daa9fe3059f69982ca60195ea8b93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d563439-73e2-4b5c-b576-655886546c71", "node_type": "1", "metadata": {"window": "Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n", "original_text": "on Intelligent Syst., 1996.\n"}, "hash": "8b0591109ec181aaa0502868350120db0196f955fcf4f61019ae8239067733f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 61965, "end_char_idx": 61971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d563439-73e2-4b5c-b576-655886546c71", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n", "original_text": "on Intelligent Syst., 1996.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe58c736-a11a-490a-81af-f77a6e184e3d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[16] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp. ", "original_text": "Conf. "}, "hash": "1b1ea937b6a6c6f9eb81104f83989e6f684d74d7c76a04c85dcc5959133c0839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "260a1c7a-3a22-440f-a24d-d4e5c1cfcf49", "node_type": "1", "metadata": {"window": "Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W. ", "original_text": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. "}, "hash": "2bd469fb3703cb175c60f44f46d2c73ca4b376f925048cd35bfd002929cd3bef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Intelligent Syst., 1996.\n", "mimetype": "text/plain", "start_char_idx": 61971, "end_char_idx": 61999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "260a1c7a-3a22-440f-a24d-d4e5c1cfcf49", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W. ", "original_text": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d563439-73e2-4b5c-b576-655886546c71", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n", "original_text": "on Intelligent Syst., 1996.\n"}, "hash": "a2a0fc89a597bbea52a9715e0f344fb1dd9a203d76880be1f7dceafe18cab3c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc7d9fe0-6b5a-436c-b602-9a68bf40b26b", "node_type": "1", "metadata": {"window": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G. ", "original_text": "IEEE Int. "}, "hash": "e8fe18d355b07c7aa8fdf340083a4f96a56146d6af728c6a391194465d5b6ad9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. ", "mimetype": "text/plain", "start_char_idx": 61999, "end_char_idx": 62166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc7d9fe0-6b5a-436c-b602-9a68bf40b26b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G. ", "original_text": "IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "260a1c7a-3a22-440f-a24d-d4e5c1cfcf49", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W. ", "original_text": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc. "}, "hash": "f33b3626d641c31d47c65e5cce9b9e8382b69cd1655d080e80fe46fb90a7b30b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89e46e2b-03fc-4453-beee-6b7d203703c3", "node_type": "1", "metadata": {"window": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. ", "original_text": "Conf. "}, "hash": "9c4adf0064ee34bf9d18f7d17df7ea5de00f0f9c3109a1922017cc9e311e4bac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 62166, "end_char_idx": 62176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "89e46e2b-03fc-4453-beee-6b7d203703c3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc7d9fe0-6b5a-436c-b602-9a68bf40b26b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary Random Weight Networks, Information Fusion 48 (2019) 67\u201383.\n [17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G. ", "original_text": "IEEE Int. "}, "hash": "a7c55206c233474b5fbfc950c76907fd1b7f01c47439d964130ef717f34addd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c35407ed-26bc-44b6-bae4-2851bf157a9c", "node_type": "1", "metadata": {"window": "Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n", "original_text": "Data Mining Workshops, Pisa, Italy, 2008, pp. "}, "hash": "526d321572d69ba6fd09991c81b2a4588a264aa1d30ddc4523fc40402dfc5b00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 62176, "end_char_idx": 62182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c35407ed-26bc-44b6-bae4-2851bf157a9c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n", "original_text": "Data Mining Workshops, Pisa, Italy, 2008, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89e46e2b-03fc-4453-beee-6b7d203703c3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[17] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. ", "original_text": "Conf. "}, "hash": "6aac9183e405c713c6431b90061eedd1770f3ba5da8d9354617b3754676e8a68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8a445f8-ac61-4e01-b880-4d48031f2d77", "node_type": "1", "metadata": {"window": "on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J. ", "original_text": "349\u2013358.\n"}, "hash": "3a28a4328584c8679e9c439dd5bfec18b365106ee1b4a1b7c8f3e57f3a68e30e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining Workshops, Pisa, Italy, 2008, pp. ", "mimetype": "text/plain", "start_char_idx": 62182, "end_char_idx": 62228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8a445f8-ac61-4e01-b880-4d48031f2d77", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J. ", "original_text": "349\u2013358.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c35407ed-26bc-44b6-bae4-2851bf157a9c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Conf.  on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n", "original_text": "Data Mining Workshops, Pisa, Italy, 2008, pp. "}, "hash": "f42a609891f804ac6a14d5f3863baae242f7b6e480dec5c4d708bd9e37836907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5757ad47-a190-4cec-ad5e-04131c2514c1", "node_type": "1", "metadata": {"window": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S. ", "original_text": "[19] E.W. "}, "hash": "60f0eae2c5689108af80334da5b9e1ee5f3c459a87263d05239a4a900e03ee85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "349\u2013358.\n", "mimetype": "text/plain", "start_char_idx": 62228, "end_char_idx": 62237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5757ad47-a190-4cec-ad5e-04131c2514c1", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S. ", "original_text": "[19] E.W. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8a445f8-ac61-4e01-b880-4d48031f2d77", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "on Intelligent Syst., 1996.\n [18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J. ", "original_text": "349\u2013358.\n"}, "hash": "43dfdcea884afa3c71eece1612f8862c229f0a6ebf3dd5b05f465ab3ea6685f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78151892-a163-467c-95f2-facd32badc66", "node_type": "1", "metadata": {"window": "IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "original_text": "Dereszynski, T.G. "}, "hash": "321f526a3f45bac7844b754eb6bb577d56cd36918f750b212db670c772a27c08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[19] E.W. ", "mimetype": "text/plain", "start_char_idx": 62237, "end_char_idx": 62247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "78151892-a163-467c-95f2-facd32badc66", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "original_text": "Dereszynski, T.G. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5757ad47-a190-4cec-ad5e-04131c2514c1", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[18] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S. ", "original_text": "[19] E.W. "}, "hash": "fac79f9f6f754597c2521d115abdf6a2b4a1cd41d997a0435849db458e379b08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99b0f125-63f4-4d9d-848c-7389844cd990", "node_type": "1", "metadata": {"window": "Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int. ", "original_text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. "}, "hash": "8266e53bd8d1eedbb7bca023bf52ea543e9109098091002a647899a2baf867d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dereszynski, T.G. ", "mimetype": "text/plain", "start_char_idx": 62247, "end_char_idx": 62265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99b0f125-63f4-4d9d-848c-7389844cd990", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int. ", "original_text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78151892-a163-467c-95f2-facd32badc66", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "original_text": "Dereszynski, T.G. "}, "hash": "e6652210f54c7b79586583dcf3bca45a0dc6338b0fa8520d66b0e0f66cbb69eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b75fb6b-8f20-4002-a9bb-baedbd8fc3c8", "node_type": "1", "metadata": {"window": "Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc. ", "original_text": "3.\n"}, "hash": "c716a25d9916f850571b193fb52a14412acc5998cdc90a90ea8c152d4559bd69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. ", "mimetype": "text/plain", "start_char_idx": 62265, "end_char_idx": 62435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b75fb6b-8f20-4002-a9bb-baedbd8fc3c8", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc. ", "original_text": "3.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99b0f125-63f4-4d9d-848c-7389844cd990", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int. ", "original_text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no. "}, "hash": "6470e77d69b3136ce07f71ae41e885b11db92687f2cc2abf668dff11f19fa0bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4ed15cc-50ec-4842-a5db-31cbe71c89fb", "node_type": "1", "metadata": {"window": "349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng. ", "original_text": "[20] D.J. "}, "hash": "1132f4e301ea0996a6c8dc5ebb2f1dc5cccaa9be2786ad74c3e24e43a82d0554", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3.\n", "mimetype": "text/plain", "start_char_idx": 62435, "end_char_idx": 62438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4ed15cc-50ec-4842-a5db-31cbe71c89fb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng. ", "original_text": "[20] D.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b75fb6b-8f20-4002-a9bb-baedbd8fc3c8", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc. ", "original_text": "3.\n"}, "hash": "05bd04bd1e7192db1a2042e44447bd67851a24887b8a5b4b07503775d97a2f9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad5fb894-91f6-4248-a081-fc892b64a314", "node_type": "1", "metadata": {"window": "[19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n", "original_text": "Hill, B.S. "}, "hash": "d866ade5b319b4135488c7de4c7fbd0e9a52391a9bb7f26be72323728584bafd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[20] D.J. ", "mimetype": "text/plain", "start_char_idx": 62438, "end_char_idx": 62448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad5fb894-91f6-4248-a081-fc892b64a314", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n", "original_text": "Hill, B.S. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4ed15cc-50ec-4842-a5db-31cbe71c89fb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "349\u2013358.\n [19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng. ", "original_text": "[20] D.J. "}, "hash": "4b2f56bfa12ae04f48c8aa2cf69a6834d2b061be17951c99710fd427a22ed159", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62ed329d-5d05-4b8b-842c-cccdf3958fed", "node_type": "1", "metadata": {"window": "Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B. ", "original_text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. "}, "hash": "e669ea7f819ef6db9a638cd7df009092b14c95024e9640730027f07ef372f2bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hill, B.S. ", "mimetype": "text/plain", "start_char_idx": 62448, "end_char_idx": 62459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62ed329d-5d05-4b8b-842c-cccdf3958fed", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B. ", "original_text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad5fb894-91f6-4248-a081-fc892b64a314", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[19] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n", "original_text": "Hill, B.S. "}, "hash": "4d7d6f92217294bbebcb01c40de3a29d40e97c25b9efd2072e1da36cf48d8960", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86953109-7e54-4f05-90f8-fcc25dd30d63", "node_type": "1", "metadata": {"window": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n", "original_text": "32nd Congress of the Int. "}, "hash": "94b05e2e012fc62f66090a142eed0c248bb07f30e8caea2175ed615878904042", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 62459, "end_char_idx": 62555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86953109-7e54-4f05-90f8-fcc25dd30d63", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n", "original_text": "32nd Congress of the Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62ed329d-5d05-4b8b-842c-cccdf3958fed", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B. ", "original_text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. "}, "hash": "b36d095cf0513d601ce8d53ef9d6f88cc77762ccf535ab1f5e1fffe0141bce84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5cbeae4-604f-4851-b304-0b338e70bbef", "node_type": "1", "metadata": {"window": "3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. ", "original_text": "Assoc. "}, "hash": "858792fbad4e5bf82bbfcb61331e432954019d8243f0569c8de9db8298bfa31f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "32nd Congress of the Int. ", "mimetype": "text/plain", "start_char_idx": 62555, "end_char_idx": 62581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5cbeae4-604f-4851-b304-0b338e70bbef", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. ", "original_text": "Assoc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86953109-7e54-4f05-90f8-fcc25dd30d63", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Transactions on Sensor Networks 8 (1) (2011) article no.  3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n", "original_text": "32nd Congress of the Int. "}, "hash": "77049a0d7be8b943a6754135709023336b5eb9d4374fd25b88164e7dd8089395", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0f3cb3c-e9f4-4111-8ef8-a2658be1a471", "node_type": "1", "metadata": {"window": "[20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp. ", "original_text": "of Hydraulic Eng. "}, "hash": "487c8edbc66af3fb75a963352652f1c86ef017a35861ddf9ce956f5d7b5fc318", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assoc. ", "mimetype": "text/plain", "start_char_idx": 62581, "end_char_idx": 62588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0f3cb3c-e9f4-4111-8ef8-a2658be1a471", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp. ", "original_text": "of Hydraulic Eng. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5cbeae4-604f-4851-b304-0b338e70bbef", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "3.\n [20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. ", "original_text": "Assoc. "}, "hash": "bcc150339a62a1128b28471baeb7a12f2b648588f419a7819fb68031d92eb5d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db9ac175-7814-40ba-9024-35ab1b8ec05e", "node_type": "1", "metadata": {"window": "Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n", "original_text": "and Research, 2007.\n"}, "hash": "9ef35f584fd77cc00883db965980bfc0692b25d5c1dacfcec109da29f3ee8f8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of Hydraulic Eng. ", "mimetype": "text/plain", "start_char_idx": 62588, "end_char_idx": 62606, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db9ac175-7814-40ba-9024-35ab1b8ec05e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n", "original_text": "and Research, 2007.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0f3cb3c-e9f4-4111-8ef8-a2658be1a471", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[20] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp. ", "original_text": "of Hydraulic Eng. "}, "hash": "4f0ef7764a4843c0e0207cbccc2f13aa0e01afc089104943f300b6f8a23e41a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "946f6076-fcff-43c6-a954-a1077c128fee", "node_type": "1", "metadata": {"window": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T. ", "original_text": "[21] D.B. "}, "hash": "764bf8ea0863c63e6a3af4bbacb322e3305d047d9a08c50605be8d9283d1a2ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and Research, 2007.\n", "mimetype": "text/plain", "start_char_idx": 62606, "end_char_idx": 62626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "946f6076-fcff-43c6-a954-a1077c128fee", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T. ", "original_text": "[21] D.B. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db9ac175-7814-40ba-9024-35ab1b8ec05e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n", "original_text": "and Research, 2007.\n"}, "hash": "51451560fc5d42baa26e4b2d621307f59050a7dbf978d8f8cd483fa024b9ea15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "deae35af-f9f2-462f-8993-5eb8de6395ce", "node_type": "1", "metadata": {"window": "32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M. ", "original_text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n"}, "hash": "8bef4a432b31d1705d349c970c9b98a48a59a8a27fce255a7f8c89e6b52faf80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[21] D.B. ", "mimetype": "text/plain", "start_char_idx": 62626, "end_char_idx": 62636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "deae35af-f9f2-462f-8993-5eb8de6395ce", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M. ", "original_text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "946f6076-fcff-43c6-a954-a1077c128fee", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T. ", "original_text": "[21] D.B. "}, "hash": "89cbbb7244f4ae070744afa0a441eac695a737ca8b4d57fe55d10587fad05687", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a550d0f5-b045-4030-a3f7-6b4a29d1d571", "node_type": "1", "metadata": {"window": "Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. "}, "hash": "a3540b695de3ab0c955fe93c5546a9cac1762d883b4a88c086a7e580a5bd06a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n", "mimetype": "text/plain", "start_char_idx": 62636, "end_char_idx": 62778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a550d0f5-b045-4030-a3f7-6b4a29d1d571", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deae35af-f9f2-462f-8993-5eb8de6395ce", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M. ", "original_text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n"}, "hash": "1623c2430eb523e7cf41cafadd5532b60a5d1bbc860969490af152a7cd684da6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e997379a-f009-4311-aa9d-6211994fe4d2", "node_type": "1", "metadata": {"window": "of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n", "original_text": "Lecture Notes in Computer Science 2431, 2002, pp. "}, "hash": "a9daa99e883c95d733a997f53b8da43c5b0e72c11611c256c80a15f50dda94e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. ", "mimetype": "text/plain", "start_char_idx": 62778, "end_char_idx": 62914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e997379a-f009-4311-aa9d-6211994fe4d2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n", "original_text": "Lecture Notes in Computer Science 2431, 2002, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a550d0f5-b045-4030-a3f7-6b4a29d1d571", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Assoc.  of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery. "}, "hash": "29241e46ed2e45dd584ae4be6cf2ec223f0e38f1e08a77a896e06e1cef4a81dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30647ff2-b020-4e94-945b-b69ae0559ddd", "node_type": "1", "metadata": {"window": "and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T. ", "original_text": "15\u201326.\n"}, "hash": "ff616083d92d0d18e08fe49def415eef14f83b2197185a2d49954dfd67b0985d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lecture Notes in Computer Science 2431, 2002, pp. ", "mimetype": "text/plain", "start_char_idx": 62914, "end_char_idx": 62964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30647ff2-b020-4e94-945b-b69ae0559ddd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T. ", "original_text": "15\u201326.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e997379a-f009-4311-aa9d-6211994fe4d2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "of Hydraulic Eng.  and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n", "original_text": "Lecture Notes in Computer Science 2431, 2002, pp. "}, "hash": "238e023d81322cab4e0f3e6d9b51dd5ecc57c6f031d44140fc8f67f74920c14a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6892db22-3e69-488c-a9d6-c03c73e59a22", "node_type": "1", "metadata": {"window": "[21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M. ", "original_text": "[23] F.T. "}, "hash": "2800c76ec569441775c3c697022c038323369910799534d47e48e3e1179edbf9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15\u201326.\n", "mimetype": "text/plain", "start_char_idx": 62964, "end_char_idx": 62971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6892db22-3e69-488c-a9d6-c03c73e59a22", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M. ", "original_text": "[23] F.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30647ff2-b020-4e94-945b-b69ae0559ddd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "and Research, 2007.\n [21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T. ", "original_text": "15\u201326.\n"}, "hash": "57d85af1ef6e93991c18cf6c9553b15a5ac7bcd57174082052dc3cd6ac40b34a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3883ffc4-6a1d-49e0-9d03-58e51a827045", "node_type": "1", "metadata": {"window": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "Liu, K.M. "}, "hash": "e006fee20e1679365202458f86eada24d0a4ff322aefc30c5a641efb4ef882c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[23] F.T. ", "mimetype": "text/plain", "start_char_idx": 62971, "end_char_idx": 62981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3883ffc4-6a1d-49e0-9d03-58e51a827045", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "Liu, K.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6892db22-3e69-488c-a9d6-c03c73e59a22", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[21] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M. ", "original_text": "[23] F.T. "}, "hash": "e9a24b03288113c75c74dd273690a14055bb8343ca45cecb42a03c7271826cd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45d62c9f-6a14-4f81-b99f-a645388d162a", "node_type": "1", "metadata": {"window": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n", "original_text": "Ting, Z.-H. "}, "hash": "e3ea497731904d79bc26dd1fc0bc2ea9e42c01b46be7fe242d28d1a17d22a708", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu, K.M. ", "mimetype": "text/plain", "start_char_idx": 62981, "end_char_idx": 62991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45d62c9f-6a14-4f81-b99f-a645388d162a", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n", "original_text": "Ting, Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3883ffc4-6a1d-49e0-9d03-58e51a827045", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Neill, Expectation-based scan statistics for monitoring spatial time series data, International Journal of Forecasting 25 (3) (2009) 498\u2013517.\n [22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "Liu, K.M. "}, "hash": "fab68acb3e7c48711d2b3fbcb535ad283f344c40c71c06619db392056d266043", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d18054b-7204-436d-9364-09e5434cc3ec", "node_type": "1", "metadata": {"window": "Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n", "original_text": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n"}, "hash": "a6a88b2a81291657933ca709eb7ecd67ebb225809a9b3ca8790f66f4a424821a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ting, Z.-H. ", "mimetype": "text/plain", "start_char_idx": 62991, "end_char_idx": 63003, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d18054b-7204-436d-9364-09e5434cc3ec", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n", "original_text": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45d62c9f-6a14-4f81-b99f-a645388d162a", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[22] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery.  Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n", "original_text": "Ting, Z.-H. "}, "hash": "08893fc1ea4f78a40a11d8acb01f85ee6a8c3b95f48e68afc5baf830b432a9e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a1db764-fa66-4550-9055-0e6b719aa1e3", "node_type": "1", "metadata": {"window": "15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n", "original_text": "[24] F.T. "}, "hash": "97993b9bc04e5649cf7662af96757fa25888f63a9edfd051158ba405d204cbfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n", "mimetype": "text/plain", "start_char_idx": 63003, "end_char_idx": 63106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a1db764-fa66-4550-9055-0e6b719aa1e3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n", "original_text": "[24] F.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d18054b-7204-436d-9364-09e5434cc3ec", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Lecture Notes in Computer Science 2431, 2002, pp.  15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n", "original_text": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n"}, "hash": "dc2a81e1bc386d4eb718dd70a015c2ed9a5b2b39f414c739f96fdce1a2c76e66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f73e282a-e4fa-45e9-94bd-8490884d12e2", "node_type": "1", "metadata": {"window": "[23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n", "original_text": "Liu, K.M. "}, "hash": "08e655026181a0a6b72905ca227bb53f521820ead6f93e0c25644eca0bc1858a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[24] F.T. ", "mimetype": "text/plain", "start_char_idx": 63106, "end_char_idx": 63116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f73e282a-e4fa-45e9-94bd-8490884d12e2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n", "original_text": "Liu, K.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a1db764-fa66-4550-9055-0e6b719aa1e3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "15\u201326.\n [23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n", "original_text": "[24] F.T. "}, "hash": "e9fe864e31366e8fa5710a216303502e8a008519fb83bacafbe7ac8a6d94983e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62896b09-4cb0-479f-ab8b-1b778b2da9b4", "node_type": "1", "metadata": {"window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C. ", "original_text": "Ting, Z.-H. "}, "hash": "4055b46bd22cad18c5204fa09cccb5b114ceb3a1c75bf880fba267a4b56a6c67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu, K.M. ", "mimetype": "text/plain", "start_char_idx": 63116, "end_char_idx": 63126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62896b09-4cb0-479f-ab8b-1b778b2da9b4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C. ", "original_text": "Ting, Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f73e282a-e4fa-45e9-94bd-8490884d12e2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[23] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n", "original_text": "Liu, K.M. "}, "hash": "185f41b1597f61001b084f4ea9f5c282ee217aef5b9d5e9bb0ace5dfab036858", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "805f727f-55fb-426b-a165-d8dd618c1ead", "node_type": "1", "metadata": {"window": "Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J. ", "original_text": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n"}, "hash": "4b80a44d586752ef15150116525eed4255ac6c7523803b6067d1f5d13f493aa0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ting, Z.-H. ", "mimetype": "text/plain", "start_char_idx": 63126, "end_char_idx": 63138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "805f727f-55fb-426b-a165-d8dd618c1ead", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J. ", "original_text": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62896b09-4cb0-479f-ab8b-1b778b2da9b4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C. ", "original_text": "Ting, Z.-H. "}, "hash": "03a21189471985e5e189e6d949190b01e8490447424a60e248944975dbadee9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77e42689-03d6-43a2-ad3a-e2b272ce4617", "node_type": "1", "metadata": {"window": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n", "original_text": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n"}, "hash": "1e6ef0307b550780c6984256f8e7008a5e204dd80ffd91e2c4367f8377ece779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n", "mimetype": "text/plain", "start_char_idx": 63138, "end_char_idx": 63235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77e42689-03d6-43a2-ad3a-e2b272ce4617", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n", "original_text": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "805f727f-55fb-426b-a165-d8dd618c1ead", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J. ", "original_text": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n"}, "hash": "0c7701bc175ccdcec70248b985832e58305133f615b373ffab0bae4823c200f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46349222-d8b2-4b20-8f41-4b178fbd396b", "node_type": "1", "metadata": {"window": "[24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "original_text": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n"}, "hash": "a6c3af02e566e2056f6c0ddfdf6811a708ede14428876ea37b25e5b39df2bef5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n", "mimetype": "text/plain", "start_char_idx": 63235, "end_char_idx": 63432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46349222-d8b2-4b20-8f41-4b178fbd396b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "original_text": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77e42689-03d6-43a2-ad3a-e2b272ce4617", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Zhou, Isolation-based anomaly detection, ACM Transactions on Knowledge Discovery from Data 6 (2012) 3.\n [24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n", "original_text": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n"}, "hash": "153c9d99969a1bdb86b4afb4b16c84cd6bd953a8a6359725c26fe3992680cd14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be9069e0-e202-4ace-830e-bf90764ff159", "node_type": "1", "metadata": {"window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n", "original_text": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n"}, "hash": "1a27bca6f3c6dc98c5a5e12a34741d20091e1ef4d410c2d216935044fa205deb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n", "mimetype": "text/plain", "start_char_idx": 63432, "end_char_idx": 63616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be9069e0-e202-4ace-830e-bf90764ff159", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n", "original_text": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46349222-d8b2-4b20-8f41-4b178fbd396b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[24] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "original_text": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n"}, "hash": "f5e76d2b9fd92df4cfd71521e875a7e76ed0fb79cf99589861cca8ba7f9fca17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03f7bf7e-d6f2-4af6-bfa6-5d78a74df1fe", "node_type": "1", "metadata": {"window": "Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A. ", "original_text": "[28] S. Hariri, M.C. "}, "hash": "26b9cd4a69304a0a3d56012106ce56a60764b6659c344e81fd34ef56339018be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n", "mimetype": "text/plain", "start_char_idx": 63616, "end_char_idx": 63809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "03f7bf7e-d6f2-4af6-bfa6-5d78a74df1fe", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A. ", "original_text": "[28] S. Hariri, M.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be9069e0-e202-4ace-830e-bf90764ff159", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n", "original_text": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n"}, "hash": "ca42f2cbf54bb52e392a4eb7ecf219b8d61d363adc59efe2dd77e96816d68d70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e95b2b8-5fe4-4a68-abea-b88e4778f1c0", "node_type": "1", "metadata": {"window": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A. ", "original_text": "Kind, R.J. "}, "hash": "d0b9b2fd2bd32592b3cee80ee3996bd7d1d13acd396ec1c861a9b5c39fc41398", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[28] S. Hariri, M.C. ", "mimetype": "text/plain", "start_char_idx": 63809, "end_char_idx": 63830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e95b2b8-5fe4-4a68-abea-b88e4778f1c0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A. ", "original_text": "Kind, R.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03f7bf7e-d6f2-4af6-bfa6-5d78a74df1fe", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A. ", "original_text": "[28] S. Hariri, M.C. "}, "hash": "82234003a0fab688702da02de853cf427fa695ea5e26039f63211e2f11536fbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a928370-ad0e-454b-a812-325840965293", "node_type": "1", "metadata": {"window": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n", "original_text": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n"}, "hash": "999124da45448ecb438e3d501fe05a9807c9d7fb16409efb509991acc8b030c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Kind, R.J. ", "mimetype": "text/plain", "start_char_idx": 63830, "end_char_idx": 63841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2a928370-ad0e-454b-a812-325840965293", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n", "original_text": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e95b2b8-5fe4-4a68-abea-b88e4778f1c0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Zhou, Isolation forest, Eighth IEEE International Conference on Data Mining 2008 (2008) 413\u2013422.\n [25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A. ", "original_text": "Kind, R.J. "}, "hash": "9b81af6cc5941c7f87e4fdb8728137ed69aa05a9891238f686f8f2873c838063", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff2a0844-e881-4777-8dd0-6232781d38b9", "node_type": "1", "metadata": {"window": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). ", "original_text": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. "}, "hash": "c2317e5c50c87af917b8945a7adcebdb5e6c955f71bf4cda6a58d4d82ce96e90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n", "mimetype": "text/plain", "start_char_idx": 63841, "end_char_idx": 64013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff2a0844-e881-4777-8dd0-6232781d38b9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). ", "original_text": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a928370-ad0e-454b-a812-325840965293", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[25] F. Carcillo, Y. Le Borgne, O. Caelen, Y. Kessaci, F. Obl\u00e9, G. Bontempi, Combining unsupervised and supervised learning in credit card fraud detection, Information Sciences 557 (2021) 317\u2013331.\n [26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n", "original_text": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n"}, "hash": "c54bbabdb29fd877a728b50902ad874c4e25ea6d782036cb589f248662d2d9f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9eca2889-5ee7-4e2b-82f1-00c041d8b882", "node_type": "1", "metadata": {"window": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest. ", "original_text": "217\u2013222.\n"}, "hash": "a7bfd58aa23ab299a9b42e2c715bb8c02ef3797cae5d835a91e0a743b3b87a79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "mimetype": "text/plain", "start_char_idx": 64013, "end_char_idx": 64227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9eca2889-5ee7-4e2b-82f1-00c041d8b882", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest. ", "original_text": "217\u2013222.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff2a0844-e881-4777-8dd0-6232781d38b9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[26] P. Kulczycki, K. Franus, Methodically unified procedures for a conditional approach to outlier detection, clustering, and classification, Information Sciences 560 (2020) 504\u2013527.\n [27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). ", "original_text": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. "}, "hash": "05c1919c83304189e323ae6045f97bdaf2b495777f6a23318c97c4f57dfcd3af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c144e7e4-d06f-4d20-b161-2e09b6d9a956", "node_type": "1", "metadata": {"window": "[28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. ", "original_text": "[30] J.A. "}, "hash": "3b3e6c2e05b86de92225d0e04e2f313a0fd61919156f567159150793e74d4997", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "217\u2013222.\n", "mimetype": "text/plain", "start_char_idx": 64227, "end_char_idx": 64236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c144e7e4-d06f-4d20-b161-2e09b6d9a956", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. ", "original_text": "[30] J.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9eca2889-5ee7-4e2b-82f1-00c041d8b882", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[27] N. Shang, A. Wang, Y. Ding, K. Gai, L. Zhu, G. Zhang, A machine learning based golden-free detection method for command-activated hardware Trojan, Information Sciences 540 (2020) 292\u2013307.\n [28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest. ", "original_text": "217\u2013222.\n"}, "hash": "11bbfec2aedd85f52ab8518e6428988f03cb0c638bf6b07c531d952f3acf2f58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59257fe7-cfef-4a6b-8af3-6b10de8b3dd4", "node_type": "1", "metadata": {"window": "Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp. ", "original_text": "Hartigan, M.A. "}, "hash": "6942426b214800cb91434d216c593e85406707462b3145e8e3ea2d783f65cb87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] J.A. ", "mimetype": "text/plain", "start_char_idx": 64236, "end_char_idx": 64246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59257fe7-cfef-4a6b-8af3-6b10de8b3dd4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp. ", "original_text": "Hartigan, M.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c144e7e4-d06f-4d20-b161-2e09b6d9a956", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[28] S. Hariri, M.C.  Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. ", "original_text": "[30] J.A. "}, "hash": "3ad5d13641d57fb9ee94cb814304ed832d9fe81542efe2be5453d0dfd9b50d45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47ea6ce5-3fce-45ce-882e-a76d1a045d09", "node_type": "1", "metadata": {"window": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n", "original_text": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n"}, "hash": "ea083d1f43d535f069b27a647ef78b8bdd638ce3879086c58871bbf52b652c44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hartigan, M.A. ", "mimetype": "text/plain", "start_char_idx": 64246, "end_char_idx": 64261, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47ea6ce5-3fce-45ce-882e-a76d1a045d09", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n", "original_text": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59257fe7-cfef-4a6b-8af3-6b10de8b3dd4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Kind, R.J.  Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp. ", "original_text": "Hartigan, M.A. "}, "hash": "12b1e694fe7bd75d49ff0091f35f75557d39f805f512ed518b67d88163ec5466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "783fb3e8-74d2-4108-b989-ce818bfa67dd", "node_type": "1", "metadata": {"window": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. ", "original_text": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). "}, "hash": "4ae761bf548dd2c972425580afcbc6034c3af70cfee2863e35c9d40adeb3d3ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n", "mimetype": "text/plain", "start_char_idx": 64261, "end_char_idx": 64352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "783fb3e8-74d2-4108-b989-ce818bfa67dd", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. ", "original_text": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ea6ce5-3fce-45ce-882e-a76d1a045d09", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Brunner, Extended isolation forest, IEEE Transactions on Knowledge and Data Engineering 33 (4) (2021) 1479\u20131489, https:// doi.org/10.1109/TKDE.6910.1109/TKDE.2019.2947676.\n [29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n", "original_text": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n"}, "hash": "98d7b9fe8975211a8c0b8dac34f613dcc6a460601537aef7e5f80b2414ef7b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a12ebe50-474d-4ede-a5df-1d08984bd933", "node_type": "1", "metadata": {"window": "217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751. ", "original_text": "On detecting clustered anomalies using SCiForest. "}, "hash": "c7e655e726c947136861cd758f36a7149ab7526119acf7618a08f2d546250edc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). ", "mimetype": "text/plain", "start_char_idx": 64352, "end_char_idx": 64410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a12ebe50-474d-4ede-a5df-1d08984bd933", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751. ", "original_text": "On detecting clustered anomalies using SCiForest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "783fb3e8-74d2-4108-b989-ce818bfa67dd", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[29] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. ", "original_text": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September). "}, "hash": "827f1466c593490e612f739c7bf9fe45fef232322c91eec99fe21e5b11c5921d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eebad96f-230e-465a-88a1-de7eab42b43b", "node_type": "1", "metadata": {"window": "[30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp. ", "original_text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. "}, "hash": "8ce3946b4c5d34fbea6009a8346c84ae78943a581ef97d94f98a3edbf35004a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On detecting clustered anomalies using SCiForest. ", "mimetype": "text/plain", "start_char_idx": 64410, "end_char_idx": 64460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eebad96f-230e-465a-88a1-de7eab42b43b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp. ", "original_text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a12ebe50-474d-4ede-a5df-1d08984bd933", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "217\u2013222.\n [30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751. ", "original_text": "On detecting clustered anomalies using SCiForest. "}, "hash": "727e8b2f77f47184b1fe6fa05491526904411fd641d5ec75f6ca557cd3bcf941", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77ce11e6-d54f-484e-8aa5-4adaec08ac05", "node_type": "1", "metadata": {"window": "Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n", "original_text": "Springer, Berlin, Heidelberg, 2010, pp. "}, "hash": "2a154040dfd3946152d483168aba29894bd531f5fab2c35aae3ac15be2d4e3b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. ", "mimetype": "text/plain", "start_char_idx": 64460, "end_char_idx": 64547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77ce11e6-d54f-484e-8aa5-4adaec08ac05", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n", "original_text": "Springer, Berlin, Heidelberg, 2010, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eebad96f-230e-465a-88a1-de7eab42b43b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[30] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp. ", "original_text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. "}, "hash": "a2e5056f90a025972ee8f82e6c85974c7e7be82eb466187cbb97a6405fd57736", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a512e124-04e4-4ea3-91f2-cba2de92b6ca", "node_type": "1", "metadata": {"window": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. ", "original_text": "274\u2013290.\n"}, "hash": "a66ecabf3592b3ed18e81d414ff7057f66af9dd9da267b6cce58dbe36a4424f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer, Berlin, Heidelberg, 2010, pp. ", "mimetype": "text/plain", "start_char_idx": 64547, "end_char_idx": 64587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a512e124-04e4-4ea3-91f2-cba2de92b6ca", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. ", "original_text": "274\u2013290.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77ce11e6-d54f-484e-8aa5-4adaec08ac05", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hartigan, M.A.  Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n", "original_text": "Springer, Berlin, Heidelberg, 2010, pp. "}, "hash": "00a392b98970ab7f948384a567899d4b4029e827f21ecd2c389c69fae536826d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e3f8465-aab4-4c98-8fe7-0d0f34b2cd37", "node_type": "1", "metadata": {"window": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds.", "original_text": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. "}, "hash": "c01a825b41f1b8f20c2d44484d87bedda65114cc52868e32776dff95de377c40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "274\u2013290.\n", "mimetype": "text/plain", "start_char_idx": 64587, "end_char_idx": 64596, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e3f8465-aab4-4c98-8fe7-0d0f34b2cd37", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds.", "original_text": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a512e124-04e4-4ea3-91f2-cba2de92b6ca", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Wong, A K-Means clustering algorithm, Journal of Applied Statistics 28 (1) (1979) 100\u2013108.\n [31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. ", "original_text": "274\u2013290.\n"}, "hash": "cc647a28d33067758c42f355fe6ffc215db2ae3f8d822b7e85d6c7e74af68f31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a744c89-cdf2-42b2-bb7e-ab4fcf0dfdbf", "node_type": "1", "metadata": {"window": "On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n", "original_text": "Lecture Notes in Computer Science 11751. "}, "hash": "96f21b295242e1e181c183382af75a9d24d9e398d1043704392adcc9ebae2fe9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. ", "mimetype": "text/plain", "start_char_idx": 64596, "end_char_idx": 64715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a744c89-cdf2-42b2-bb7e-ab4fcf0dfdbf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n", "original_text": "Lecture Notes in Computer Science 11751. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e3f8465-aab4-4c98-8fe7-0d0f34b2cd37", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[31] F. T. Liu, K. M. Ting, Z. H. Zhou (2010, September).  On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds.", "original_text": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019. "}, "hash": "0a826e17b0f423c6dfa86823c42e729c77a4acc7f3b3f44b3c5c2344fb6791f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2798c25-667b-4786-9c23-a6e2a90a6973", "node_type": "1", "metadata": {"window": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L. ", "original_text": "Springer, Cham, (2019), pp. "}, "hash": "9c37157d2790636b91f26d6400179295b444dae0cf23dd322d1e2d6d3c3fb6fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lecture Notes in Computer Science 11751. ", "mimetype": "text/plain", "start_char_idx": 64715, "end_char_idx": 64756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2798c25-667b-4786-9c23-a6e2a90a6973", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L. ", "original_text": "Springer, Cham, (2019), pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a744c89-cdf2-42b2-bb7e-ab4fcf0dfdbf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "On detecting clustered anomalies using SCiForest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n", "original_text": "Lecture Notes in Computer Science 11751. "}, "hash": "d51877ef2fb0c56ee5872649b005585f19ab9bd12f9072631ddfa3a183418a42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc8e320b-2090-4bd7-8bc4-03af52d11a40", "node_type": "1", "metadata": {"window": "Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n", "original_text": "152\u2013163.\n"}, "hash": "d320d64a49798d977720867c4acb3896f2545ebdec39d91f7a2b5f31b90df721", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer, Cham, (2019), pp. ", "mimetype": "text/plain", "start_char_idx": 64756, "end_char_idx": 64784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc8e320b-2090-4bd7-8bc4-03af52d11a40", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n", "original_text": "152\u2013163.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2798c25-667b-4786-9c23-a6e2a90a6973", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases.  Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L. ", "original_text": "Springer, Cham, (2019), pp. "}, "hash": "db49ee0e0d55c5d1972da84d86ebab0aa846c552518a715aadff4ef1cccd4c96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc827857-b8c1-488e-ab83-c889e617e050", "node_type": "1", "metadata": {"window": "274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. ", "original_text": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. "}, "hash": "b6c20b8da9b10887be9e8908b942b99de07390daa04aca8ddfcf028030a89a60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "152\u2013163.\n", "mimetype": "text/plain", "start_char_idx": 64784, "end_char_idx": 64793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc827857-b8c1-488e-ab83-c889e617e050", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. ", "original_text": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc8e320b-2090-4bd7-8bc4-03af52d11a40", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Springer, Berlin, Heidelberg, 2010, pp.  274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n", "original_text": "152\u2013163.\n"}, "hash": "1e1c3910f0c4c5ea98dee711b95273dd9d06fc840619a70c9487e19288a7baa2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "135d70a3-b16b-4820-8790-96a30ea257c7", "node_type": "1", "metadata": {"window": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n", "original_text": "(Eds."}, "hash": "35c291c22e46387ebc947d637a415c3911815469c6668c2835c567ae217f3e42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. ", "mimetype": "text/plain", "start_char_idx": 64793, "end_char_idx": 64926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "135d70a3-b16b-4820-8790-96a30ea257c7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n", "original_text": "(Eds."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc827857-b8c1-488e-ab83-c889e617e050", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "274\u2013290.\n [32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. ", "original_text": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al. "}, "hash": "a2c0c7975acdf047474949fade44b5f3e30d91ea0a21ff65b7064a3e2c787b53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4dfa43f-5fa4-40ad-8fd6-bb2a7ef4cf5d", "node_type": "1", "metadata": {"window": "Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n", "original_text": "), ICAISC 2020 Proceedings, 2020.\n"}, "hash": "fa90ac5490c59bdd92c7a785b02f8d38d39ea0d4149a65dde58f47bf2423bb15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(Eds.", "mimetype": "text/plain", "start_char_idx": 64926, "end_char_idx": 64931, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4dfa43f-5fa4-40ad-8fd6-bb2a7ef4cf5d", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n", "original_text": "), ICAISC 2020 Proceedings, 2020.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "135d70a3-b16b-4820-8790-96a30ea257c7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[32] A. Mensi, M. Bicego, A novel anomaly score for isolation forests, in: Image Analysis and Processing - ICIAP 2019.  Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n", "original_text": "(Eds."}, "hash": "a7a941b8d54091fd9b2b9d5311bb9850ec352d8bfaddde9ec8e6ad394b2ced2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57213580-4ba1-47be-bfda-c88f54864224", "node_type": "1", "metadata": {"window": "Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n", "original_text": "[34] R.L. "}, "hash": "1c69280ef6526e99b44138242a3bea01bc003b8f96fdece8b0100b6c6e2955b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "), ICAISC 2020 Proceedings, 2020.\n", "mimetype": "text/plain", "start_char_idx": 64931, "end_char_idx": 64965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57213580-4ba1-47be-bfda-c88f54864224", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n", "original_text": "[34] R.L. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4dfa43f-5fa4-40ad-8fd6-bb2a7ef4cf5d", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Lecture Notes in Computer Science 11751.  Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n", "original_text": "), ICAISC 2020 Proceedings, 2020.\n"}, "hash": "9e925eabbea7e3ca25f8423059782805cfdce9c4a91ca66f6d18b980f712f7c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b64d283a-710e-461c-8f77-3e5a6252d880", "node_type": "1", "metadata": {"window": "152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n", "original_text": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n"}, "hash": "d1cc8b5ef1d52993b9cacbb300a9a3f086117771b71b0d92bb8d3035df8ae163", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[34] R.L. ", "mimetype": "text/plain", "start_char_idx": 64965, "end_char_idx": 64975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b64d283a-710e-461c-8f77-3e5a6252d880", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n", "original_text": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57213580-4ba1-47be-bfda-c88f54864224", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Springer, Cham, (2019), pp.  152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n", "original_text": "[34] R.L. "}, "hash": "90301fe91bba3da4b0bb7c30b38cba4c7bf4dfb6cd90ae5d5afb5294a59d2f8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98f63729-a051-44cf-b92a-e43a6afbcad3", "node_type": "1", "metadata": {"window": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A. ", "original_text": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. "}, "hash": "8f68515d216b70610a3d0f7f0b8cc80a1417c05c426bc48cfeb51921d56f5a4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n", "mimetype": "text/plain", "start_char_idx": 64975, "end_char_idx": 65225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98f63729-a051-44cf-b92a-e43a6afbcad3", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A. ", "original_text": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b64d283a-710e-461c-8f77-3e5a6252d880", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "152\u2013163.\n [33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n", "original_text": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n"}, "hash": "7fc6e9e3d4e4e347ecb2d071b8fd0c88182f24c5ae20c2aacf1b51175dea2990", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e28807ce-4711-47f0-bbc8-63cf0f0fbfa9", "node_type": "1", "metadata": {"window": "(Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "original_text": "1\u20136.\n"}, "hash": "956d4ad441601719853fa25010741a5891aa1def0f9ed3304d8ac36a83044464", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. ", "mimetype": "text/plain", "start_char_idx": 65225, "end_char_idx": 65384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e28807ce-4711-47f0-bbc8-63cf0f0fbfa9", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "original_text": "1\u20136.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98f63729-a051-44cf-b92a-e43a6afbcad3", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[33] P. Karczmarek, A. Kiersztyn, W. Pedrycz, N-ary isolation forest: An experimental comparative analysis,\" in: L. Rutkowski et al.  (Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A. ", "original_text": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp. "}, "hash": "fe3eb5fdc3d634d163e3d13dbed60c76ca38373333c9e4375f13c53137192e4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f8c1ad8-6196-463d-8852-bcc93761cb59", "node_type": "1", "metadata": {"window": "), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n", "original_text": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n"}, "hash": "48d00aaa1f283225d10c1aa9c05d1edd0e943c32c6ea2757e7cc0e5d283f770b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1\u20136.\n", "mimetype": "text/plain", "start_char_idx": 65384, "end_char_idx": 65389, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f8c1ad8-6196-463d-8852-bcc93761cb59", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n", "original_text": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e28807ce-4711-47f0-bbc8-63cf0f0fbfa9", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(Eds. ), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "original_text": "1\u20136.\n"}, "hash": "79f534dad17ce45064757b7baecd6f97a82fdda274f18d92d1e249a97504ba73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17510f40-4e44-4190-9a1b-c91fe1748569", "node_type": "1", "metadata": {"window": "[34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. ", "original_text": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n"}, "hash": "0b61054af9cf7850585a5f7f1367dec44b3212091dfd895c27d5fe4bbed0b05e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n", "mimetype": "text/plain", "start_char_idx": 65389, "end_char_idx": 65541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17510f40-4e44-4190-9a1b-c91fe1748569", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. ", "original_text": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f8c1ad8-6196-463d-8852-bcc93761cb59", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "), ICAISC 2020 Proceedings, 2020.\n [34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n", "original_text": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n"}, "hash": "1b4c7ca435d77cad1310cdbe5615863ed25b7479b39e9192ef44319fbd44c4d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "820c1d89-c2bf-42af-855b-51e688bbf218", "node_type": "1", "metadata": {"window": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31). ", "original_text": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n"}, "hash": "36e6bf89f5bec019c57c98468d93aeda090bc613611abf953f48ea9a2470cdfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n", "mimetype": "text/plain", "start_char_idx": 65541, "end_char_idx": 65660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "820c1d89-c2bf-42af-855b-51e688bbf218", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31). ", "original_text": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17510f40-4e44-4190-9a1b-c91fe1748569", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[34] R.L.  Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. ", "original_text": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n"}, "hash": "55138d10ab0d136a9c6eae3b4eb7d2d2bbc0e457d87caa8ac0071af57b8dc9c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9724634-ba94-4d96-94fb-3b3faa79f4b2", "node_type": "1", "metadata": {"window": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). ", "original_text": "[40] R.A.A. "}, "hash": "43b4268a7dd83561c5e3ebab3598bef5ff3534ee4404dc483513c4a1484ea4c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n", "mimetype": "text/plain", "start_char_idx": 65660, "end_char_idx": 65791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9724634-ba94-4d96-94fb-3b3faa79f4b2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). ", "original_text": "[40] R.A.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "820c1d89-c2bf-42af-855b-51e688bbf218", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Thorndike, Who belongs in the family?, Psychometrika 18 (4) (1953) 267\u2013276\n[35] P. Karczmarek, A. Kiersztyn, W. Pedrycz, E. Al, K-means-based isolation forest, Knowledge-Based Systems 195 (2020) 105659, https://doi.org/10.1016/ j.knosys.2020.105659.\n [36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31). ", "original_text": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n"}, "hash": "68d3a3bda82a4bb613a8a7bb3c87fca12a45629103944d5fa8203b09dd80d24d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad114cba-938f-40dc-ba4b-94d44a8ffb0b", "node_type": "1", "metadata": {"window": "1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo. ", "original_text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. "}, "hash": "9cee06de74ecb24a745acee4ae121e3253c995e8c22444d4ed2598f127490381", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[40] R.A.A. ", "mimetype": "text/plain", "start_char_idx": 65791, "end_char_idx": 65803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad114cba-938f-40dc-ba4b-94d44a8ffb0b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo. ", "original_text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9724634-ba94-4d96-94fb-3b3faa79f4b2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[36] P. Karczmarek, A. Kiersztyn, W. Pedrycz, Fuzzy set-based isolation forest, in: 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2020, pp.  1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). ", "original_text": "[40] R.A.A. "}, "hash": "e9945658a67a075104a9d5b88cb0aca2d39017a86786c211c7eb9df7af778b5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6732e34c-a29c-4be8-b408-7653f0f87047", "node_type": "1", "metadata": {"window": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. ", "original_text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n"}, "hash": "551ba913cd6c0ba19613109b33cab77b7f7dd4871cbad707485304d1893bf11a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "mimetype": "text/plain", "start_char_idx": 65803, "end_char_idx": 65842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6732e34c-a29c-4be8-b408-7653f0f87047", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. ", "original_text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad114cba-938f-40dc-ba4b-94d44a8ffb0b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "1\u20136.\n [37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo. ", "original_text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. "}, "hash": "9d3c427359b9f08d3188fc57dbe680059e5c8b7da902397ec20a66665f6f240d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5a2fa1c-0c3a-48c8-8bc8-8162f5ea30fa", "node_type": "1", "metadata": {"window": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n", "original_text": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. "}, "hash": "c70e7753ebbddade30ccd1d758e96ccff713088a65aba58d2ded26bae2f36b80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n", "mimetype": "text/plain", "start_char_idx": 65842, "end_char_idx": 66000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5a2fa1c-0c3a-48c8-8bc8-8162f5ea30fa", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n", "original_text": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6732e34c-a29c-4be8-b408-7653f0f87047", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[37] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Mining and Knowledge Discovery 29 (3) (2015) 626\u2013688.\n [38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. ", "original_text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n"}, "hash": "b4324d367456ef24c883f6c432b7584427cf6beea9a178233c166d8de49b8431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f49334b-4887-4163-b54b-2fbdb6406ed7", "node_type": "1", "metadata": {"window": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman. ", "original_text": "(2019, January 31). "}, "hash": "559159166631b0eaa5e28e48d90b55944a76c38d9ba2c92927b8f15b3c9e1a87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. ", "mimetype": "text/plain", "start_char_idx": 66000, "end_char_idx": 66057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f49334b-4887-4163-b54b-2fbdb6406ed7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman. ", "original_text": "(2019, January 31). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5a2fa1c-0c3a-48c8-8bc8-8162f5ea30fa", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[38] V. Chandola, A. Banerjee, V. Kumar, Anomaly Detection: A Survey, ACM Computing Surveys (CSUR) 41 (3) (2009) 1\u201372.\n [39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n", "original_text": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary. "}, "hash": "df67f7c1d88a7d445a7b21e337f58519e774c1a8501161cc18f22c1d947b6d7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c89b1d9-b50b-441d-90d3-4bb5e44a735b", "node_type": "1", "metadata": {"window": "[40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\" ", "original_text": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). "}, "hash": "91e1482c70e53ed1d188af174d68e97e3d0132952d0e099633d2ba7d74e18065", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019, January 31). ", "mimetype": "text/plain", "start_char_idx": 66057, "end_char_idx": 66077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c89b1d9-b50b-441d-90d3-4bb5e44a735b", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\" ", "original_text": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f49334b-4887-4163-b54b-2fbdb6406ed7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[39] H. Fanaee-T, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowledge-Based Systems 98 (2016) 130\u2013147.\n [40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman. ", "original_text": "(2019, January 31). "}, "hash": "31d84458a356e89c56538e449e609914bf3e455a518d8e73f5ded301a56ed1cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c9f0667-49b5-46b9-b60a-bee900f3196e", "node_type": "1", "metadata": {"window": "Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n", "original_text": "Zenodo. "}, "hash": "2d8bef01a3b21e0a39af249c18d4c3948c781f7b14087aae96726fc757a78be8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). ", "mimetype": "text/plain", "start_char_idx": 66077, "end_char_idx": 66132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c9f0667-49b5-46b9-b60a-bee900f3196e", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n", "original_text": "Zenodo. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c89b1d9-b50b-441d-90d3-4bb5e44a735b", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[40] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\" ", "original_text": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3). "}, "hash": "cb02fd9cd457f14e3afeffef70c24baefcedf168e66633ad229de2e64f574d42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7475d022-2f2d-41d3-b1ea-f9143adf57cb", "node_type": "1", "metadata": {"window": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M. ", "original_text": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. "}, "hash": "e6c6fff25456078ef3d1973edc465c3de2c4b1936d44246da16a216040cb7f7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zenodo. ", "mimetype": "text/plain", "start_char_idx": 66132, "end_char_idx": 66140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7475d022-2f2d-41d3-b1ea-f9143adf57cb", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M. ", "original_text": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c9f0667-49b5-46b9-b60a-bee900f3196e", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n", "original_text": "Zenodo. "}, "hash": "3d0fb702e1765d2bb30ea27acceefc545538b51158219dbe9d9fa5b7acce1048", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfe64190-0573-4421-a66d-fb38ee2bfbe4", "node_type": "1", "metadata": {"window": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L. ", "original_text": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n"}, "hash": "5ec6ffdbf74cfd2920315a0e61dea496a8976ce0040aa17f0ff18603e8a52efd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. ", "mimetype": "text/plain", "start_char_idx": 66140, "end_char_idx": 66202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfe64190-0573-4421-a66d-fb38ee2bfbe4", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L. ", "original_text": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7475d022-2f2d-41d3-b1ea-f9143adf57cb", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A Survey, International Journal of Information Management 45 (2019) 289\u2013307.\n [41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M. ", "original_text": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I. "}, "hash": "8fae2e8df79dffc2af534c6a019320139629910d2a3560f7db3e5976ec110274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cef97005-6729-4883-a0a2-31d28bbb80bf", "node_type": "1", "metadata": {"window": "(2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n", "original_text": "[43] A. Asuncion, D. Newman. "}, "hash": "c7202927819783b95197022e4634297f4b52ebf5b9e1082d1852485c51d803d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n", "mimetype": "text/plain", "start_char_idx": 66202, "end_char_idx": 66392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cef97005-6729-4883-a0a2-31d28bbb80bf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n", "original_text": "[43] A. Asuncion, D. Newman. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfe64190-0573-4421-a66d-fb38ee2bfbe4", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[41] Matias Carrasco Kind, & Mahdi Sadeghzadeh Ghamsary.  (2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L. ", "original_text": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n"}, "hash": "4cf50f2062cf14b8f0664fd6948ac2807daba85bee83dbeb03420e3ec946ca08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e733b390-432a-4764-9b13-2484dc1eb4e7", "node_type": "1", "metadata": {"window": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A. ", "original_text": "\u2018\u2018UCI machine learning repository.\" "}, "hash": "a50fbc6d105430db3d5209a743110e387a84a636e63ef627157646963ed45888", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[43] A. Asuncion, D. Newman. ", "mimetype": "text/plain", "start_char_idx": 66392, "end_char_idx": 66421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e733b390-432a-4764-9b13-2484dc1eb4e7", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A. ", "original_text": "\u2018\u2018UCI machine learning repository.\" "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cef97005-6729-4883-a0a2-31d28bbb80bf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(2019, January 31).  mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n", "original_text": "[43] A. Asuncion, D. Newman. "}, "hash": "6e28547b30b8eb6e980cb86a91dfe2cbe154d8c67dda9528cc88fb72a6aa0056", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac4b7ce6-3628-4871-8c32-05b6ea7801bf", "node_type": "1", "metadata": {"window": "Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n", "original_text": "(2007).\n"}, "hash": "dd4ca0eebe9ac667bb1db5a37038ea055b3609213be24f0d1e1fb2533c2e267c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2018\u2018UCI machine learning repository.\" ", "mimetype": "text/plain", "start_char_idx": 66421, "end_char_idx": 66457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac4b7ce6-3628-4871-8c32-05b6ea7801bf", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n", "original_text": "(2007).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e733b390-432a-4764-9b13-2484dc1eb4e7", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "mgckind/iso_forest: iso_forest 1.0.3 (Version v1.0.3).  Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A. ", "original_text": "\u2018\u2018UCI machine learning repository.\" "}, "hash": "30c5c2ef8b734157bd6e1c93338fefb153a184b15947448f09ad25fe94437868", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f51058dd-685d-43f3-9e54-f0a5f779148c", "node_type": "1", "metadata": {"window": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A. ", "original_text": "[44] D.M. "}, "hash": "3831c06f3fb15510ada2eaf1e9ae13a18c7f24922b68e0181c29486bf80799c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2007).\n", "mimetype": "text/plain", "start_char_idx": 66457, "end_char_idx": 66465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f51058dd-685d-43f3-9e54-f0a5f779148c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A. ", "original_text": "[44] D.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac4b7ce6-3628-4871-8c32-05b6ea7801bf", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Zenodo.  http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n", "original_text": "(2007).\n"}, "hash": "5a3db5550a19fae396f150c31703ac522bf098372a51e46c41f1a9e39169d652", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36ee47cc-78ad-421a-b633-0a6564078880", "node_type": "1", "metadata": {"window": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n", "original_text": "Rocke, D.L. "}, "hash": "f8c47271ef491e143aae049b71952df0e3d411329088367e94fd6261692e5c7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[44] D.M. ", "mimetype": "text/plain", "start_char_idx": 66465, "end_char_idx": 66475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36ee47cc-78ad-421a-b633-0a6564078880", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n", "original_text": "Rocke, D.L. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f51058dd-685d-43f3-9e54-f0a5f779148c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "http://doi.org/10.5281/zenodo.2553679\n[42] K. Yamanishi, J.I.  Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A. ", "original_text": "[44] D.M. "}, "hash": "81eeda2804258b714f68ed9b402140cede39a2f423001a8237a7983d8fedb057", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3b7076e-10d7-4ca0-8e11-8b15f1f0b8b0", "node_type": "1", "metadata": {"window": "[43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n", "original_text": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n"}, "hash": "c0c76c6e0ac338381d5992b3b93f7c6c69b4b30484fdc709301e0873d8d28cb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rocke, D.L. ", "mimetype": "text/plain", "start_char_idx": 66475, "end_char_idx": 66487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3b7076e-10d7-4ca0-8e11-8b15f1f0b8b0", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n", "original_text": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36ee47cc-78ad-421a-b633-0a6564078880", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Takeuchi, G. Williams, P. Milne, On-line unsupervised outlier detection using finite mixtures with discounting learning algorithms, Data Mining and Knowledge Discovery 8 (3) (2004) 275\u2013300.\n [43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n", "original_text": "Rocke, D.L. "}, "hash": "3a4bb66b2991e93f63c636e07cf47ae1667542630963e2cb3062133152ee661a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "428502a5-1364-4729-9fa8-eac08e994cee", "node_type": "1", "metadata": {"window": "\u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. ", "original_text": "[45] A. Dal Pozzolo, O. Caelen, R.A. "}, "hash": "ab1de15571323fa4d319feab637d460da96b4e6ff23e466dda1f3b19a6f47d77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n", "mimetype": "text/plain", "start_char_idx": 66487, "end_char_idx": 66621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "428502a5-1364-4729-9fa8-eac08e994cee", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. ", "original_text": "[45] A. Dal Pozzolo, O. Caelen, R.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3b7076e-10d7-4ca0-8e11-8b15f1f0b8b0", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[43] A. Asuncion, D. Newman.  \u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n", "original_text": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n"}, "hash": "6b6c2289c72c1723f7ef549dd7f9ec06d02f395c389c335d13a2ebeb96ba8aa4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9961b54-9d1d-4745-85ac-5011925eb0ee", "node_type": "1", "metadata": {"window": "(2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n", "original_text": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n"}, "hash": "ea934435b6c3d5199776d4c5b1a8cc93c3dd153cacd64605ad9b936e4515feef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[45] A. Dal Pozzolo, O. Caelen, R.A. ", "mimetype": "text/plain", "start_char_idx": 66621, "end_char_idx": 66658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9961b54-9d1d-4745-85ac-5011925eb0ee", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n", "original_text": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "428502a5-1364-4729-9fa8-eac08e994cee", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "\u2018\u2018UCI machine learning repository.\"  (2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. ", "original_text": "[45] A. Dal Pozzolo, O. Caelen, R.A. "}, "hash": "0b0a33bafd6610b6a4259f3eabe56576936bd4426ef89f424efaf2df14a129d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a348ba3-61a3-4ba7-a4e8-6c7899917ec5", "node_type": "1", "metadata": {"window": "[44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A. ", "original_text": "[46] A. Dal Pozzolo, O. Caelen, Y.-A. "}, "hash": "b30f0520b064ce502c90eff60fad94d720d12c5dfced9008f8a54d3b14732687", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n", "mimetype": "text/plain", "start_char_idx": 66658, "end_char_idx": 66824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a348ba3-61a3-4ba7-a4e8-6c7899917ec5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A. ", "original_text": "[46] A. Dal Pozzolo, O. Caelen, Y.-A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9961b54-9d1d-4745-85ac-5011925eb0ee", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "(2007).\n [44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n", "original_text": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n"}, "hash": "47ef70599faab634b9436259e89e6955a86fae1b7b4fea02f7500dfefdc2ccd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "045f201b-314f-4763-bc81-35392140e66c", "node_type": "1", "metadata": {"window": "Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n"}, "hash": "50bfe133930cbb2c9bf281218949e9790b1eb09dbf1dbff6dc78ec830ab21f4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[46] A. Dal Pozzolo, O. Caelen, Y.-A. ", "mimetype": "text/plain", "start_char_idx": 66824, "end_char_idx": 66862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "045f201b-314f-4763-bc81-35392140e66c", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a348ba3-61a3-4ba7-a4e8-6c7899917ec5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[44] D.M.  Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A. ", "original_text": "[46] A. Dal Pozzolo, O. Caelen, Y.-A. "}, "hash": "6ce45663d57fa13c980ad4b82b14b8d46f0b9eb58076d2481272750e00b802cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b332cb2f-656e-4e94-ac9d-1b61a4595715", "node_type": "1", "metadata": {"window": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n"}, "hash": "1935fcdc8b19745217e3f2063d3cefdee5f5f646f504262c3018498faef87437", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n", "mimetype": "text/plain", "start_char_idx": 66862, "end_char_idx": 67041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b332cb2f-656e-4e94-ac9d-1b61a4595715", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "045f201b-314f-4763-bc81-35392140e66c", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Rocke, D.L.  Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n"}, "hash": "b6cb773c77b8c2e0a67661967847783c97cd8310129801c6331e56ede1541488", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8f8938d-5b93-4511-86e7-adcb721629e5", "node_type": "1", "metadata": {"window": "[45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. "}, "hash": "307f84a6b871785175b98263927b3bb34866bd3374f9494f13939812ac913367", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n", "mimetype": "text/plain", "start_char_idx": 67041, "end_char_idx": 67274, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a8f8938d-5b93-4511-86e7-adcb721629e5", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b332cb2f-656e-4e94-ac9d-1b61a4595715", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Woodruff, Identification of outliers in multivariate data, Journal of the American Statistical Association 91 (435) (1996) 1047\u20131061.\n [45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n"}, "hash": "ae41771f25616660b618cec8a5c71b1c6f22fde6ee2a6383e508dc395947ed5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "225f2e48-9607-454f-b099-89e1148dda8f", "node_type": "1", "metadata": {"window": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n"}, "hash": "6e73565f2bbc3ee424d9b403f84fee030840af28b11bf32adfa672bb193ecf7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. ", "mimetype": "text/plain", "start_char_idx": 67274, "end_char_idx": 67440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "225f2e48-9607-454f-b099-89e1148dda8f", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8f8938d-5b93-4511-86e7-adcb721629e5", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[45] A. Dal Pozzolo, O. Caelen, R.A.  Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A. "}, "hash": "5a53a7c7b74d8a3591f94909c86b3ca7da1ee211c163a7f2d512e0d1e4160382", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6a2ad44-ca1d-401c-b89c-30f54dcff3f2", "node_type": "1", "metadata": {"window": "[46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[50] F. Carcillo, Y.-A. "}, "hash": "77b1790b0ee8e56ce02e82050a6a600965ad694544103ae4638a366f6dff4688", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n", "mimetype": "text/plain", "start_char_idx": 67440, "end_char_idx": 67607, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e6a2ad44-ca1d-401c-b89c-30f54dcff3f2", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[50] F. Carcillo, Y.-A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "225f2e48-9607-454f-b099-89e1148dda8f", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Johnson, G. Bontempi, Calibrating Probability with Undersampling for Unbalanced Classification, Symposium on Computational Intelligence and Data Mining (CIDM), 2015.\n [46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n"}, "hash": "7b97e34dba0dc81755487c664da5959904538db0bf73b0e7ae7fa2495b1d2466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39b8e3dc-44ee-4799-bfd5-f2b5419a8186", "node_type": "1", "metadata": {"window": "Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300."}, "hash": "8e1f75a850b0e71547b3d7459ebbb136ab858026de4e7e38ec56e157ba73ee72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[50] F. Carcillo, Y.-A. ", "mimetype": "text/plain", "start_char_idx": 67607, "end_char_idx": 67631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39b8e3dc-44ee-4799-bfd5-f2b5419a8186", "embedding": null, "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "9576407e-5372-4cbd-abad-286a6ead703a", "node_type": "4", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf"}, "hash": "a821bb5bf2f23c97d471966cb2e39a8656a4f69d42a43559ce1d7acf6f9812dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6a2ad44-ca1d-401c-b89c-30f54dcff3f2", "node_type": "1", "metadata": {"title": "A probabilistic generalization of isolation forest", "authors": "Tokovarov,", "year": 2022, "file_path": "ad-papers-pdf/probabilistic_generalization_of_isolation_forest.pdf", "window": "[46] A. Dal Pozzolo, O. Caelen, Y.-A.  Le Borgne, S. Waterschoot, G. Bontempi, Learned lessons in credit card fraud detection from a practitioner perspective, Expert Systems with Applications 41 (10) (2014) 4915\u20134928.\n [47] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, G. Bontempi, Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE Transactions on Neural Networks and Learning Systems 29 (8) (2018) 3784\u20133797.\n [48] A. Dal Pozzolo, Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n[49] F. Carcillo, A. Dal Pozzolo, Y.-A.  Le Borgne, O. Caelen, Y. Mazzer, G. Bontempi, Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information Fusion 41 (2018) 182\u2013194.\n [50] F. Carcillo, Y.-A.  Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "original_text": "[50] F. Carcillo, Y.-A. "}, "hash": "42b26a9237de28f0e66cd0f51b29cd9ea8b8f4082bd21a4a299780d99708051f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Le Borgne, O. Caelen, G. Bontempi, Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics 5 (4) (2018) 285\u2013300.", "mimetype": "text/plain", "start_char_idx": 67631, "end_char_idx": 67849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]