[{"id_": "68f441d6-32ed-4f74-8482-46c5691785ef", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches. ", "original_text": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c9bd793-3775-4696-8bc8-6338ad3e16d4", "node_type": "1", "metadata": {"window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. ", "original_text": "Fonts or abstract dimensions should not be changed or altered.\n\n"}, "hash": "582c1429bb4ce3fe2c28e48e1c1430ab13ff5deda77de640c663230993964ee8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c9bd793-3775-4696-8bc8-6338ad3e16d4", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. ", "original_text": "Fonts or abstract dimensions should not be changed or altered.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68f441d6-32ed-4f74-8482-46c5691785ef", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches. ", "original_text": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below. "}, "hash": "7c5b39f9fe48c7575a0df8bf8a8bd567eeece46963b05a5e972cb9e6e0e80a20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bacaf807-d316-4d31-acab-8ac2755fe17f", "node_type": "1", "metadata": {"window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n", "original_text": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n"}, "hash": "d22f853c707ab99cc01000aea1cc013982e8d58267db838a28a844a3dbf9cb74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fonts or abstract dimensions should not be changed or altered.\n\n", "mimetype": "text/plain", "start_char_idx": 121, "end_char_idx": 185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bacaf807-d316-4d31-acab-8ac2755fe17f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n", "original_text": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c9bd793-3775-4696-8bc8-6338ad3e16d4", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. ", "original_text": "Fonts or abstract dimensions should not be changed or altered.\n\n"}, "hash": "a0727be3816b4a5f49aa5f07428c99a94dbd4467f0cb72d611e4cf37a41c5a6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a176919-1e65-472c-a088-aa35dc3cc704", "node_type": "1", "metadata": {"window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. ", "original_text": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). "}, "hash": "417d459b5cf897395ba880746c6d16b0ebad3ac86cff4ca735160029cc370823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n", "mimetype": "text/plain", "start_char_idx": 185, "end_char_idx": 395, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a176919-1e65-472c-a088-aa35dc3cc704", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. ", "original_text": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bacaf807-d316-4d31-acab-8ac2755fe17f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n", "original_text": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n"}, "hash": "9c4f056b728cb78e695876329bce27158003ff660af703e7126109177a35c4b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbd0cee7-b351-4e06-a128-8b3f884550d9", "node_type": "1", "metadata": {"window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n", "original_text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. "}, "hash": "b95a3a5255a94ede6af0b1563122bb0a65b767150882f9a0b3fc6f1bb2589569", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). ", "mimetype": "text/plain", "start_char_idx": 395, "end_char_idx": 515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbd0cee7-b351-4e06-a128-8b3f884550d9", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n", "original_text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a176919-1e65-472c-a088-aa35dc3cc704", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. ", "original_text": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). "}, "hash": "e626af436841a16181b652aa2a4283b6690c7752f3b39ee57731f5c67241ebc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da1e5316-6da3-4283-8aae-9fdfb23637b8", "node_type": "1", "metadata": {"window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). ", "original_text": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches. "}, "hash": "72a1137334d5db2622021645f9f9770b251d15b3f26a43fd2e7728d86158fe65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. ", "mimetype": "text/plain", "start_char_idx": 515, "end_char_idx": 608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da1e5316-6da3-4283-8aae-9fdfb23637b8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). ", "original_text": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbd0cee7-b351-4e06-a128-8b3f884550d9", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n", "original_text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. "}, "hash": "e4ae8e07d0debc6c131b6c2fd20cebf3609fbaf74c3b544bd275a51336a5d283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e291ddcf-4c10-4631-8bb7-e74e2f54d99b", "node_type": "1", "metadata": {"window": "Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts. ", "original_text": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. "}, "hash": "c38240e6289fb9f5870f7ef7dff099ae5fd3db76f35f5a2f8b2ad6306a31e64a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches. ", "mimetype": "text/plain", "start_char_idx": 608, "end_char_idx": 735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e291ddcf-4c10-4631-8bb7-e74e2f54d99b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts. ", "original_text": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da1e5316-6da3-4283-8aae-9fdfb23637b8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Graphical Abstract (Optional)**\n\nTo create your abstract, please type over the instructions in the template box below.  Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). ", "original_text": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches. "}, "hash": "a2048aa93275c9c786ed950a1bf946fe6ef63c5930260cd5dd56dbb014d64230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9c393e1-fe3e-4f46-9416-1fc5d6652df1", "node_type": "1", "metadata": {"window": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. ", "original_text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n"}, "hash": "20cbaf2e398df4f6e6cf04635a470b173db01006867b0eef6a620800177b58a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. ", "mimetype": "text/plain", "start_char_idx": 735, "end_char_idx": 857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9c393e1-fe3e-4f46-9416-1fc5d6652df1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. ", "original_text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e291ddcf-4c10-4631-8bb7-e74e2f54d99b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Fonts or abstract dimensions should not be changed or altered.\n\n **Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts. ", "original_text": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues. "}, "hash": "4ccca46f3fb89f2658b167f8dd0957f24959b276c878544e71bac706a095c4ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055a29a0-06a5-40a6-992f-09d92fcc7ce0", "node_type": "1", "metadata": {"window": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. ", "original_text": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. "}, "hash": "050e7b3cf5b6be477702f2d6d7fcd1709edfc66773d7a24f11cc34fe240b6e61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n", "mimetype": "text/plain", "start_char_idx": 857, "end_char_idx": 1012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "055a29a0-06a5-40a6-992f-09d92fcc7ce0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. ", "original_text": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9c393e1-fe3e-4f46-9416-1fc5d6652df1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Generalized Isolation Forest for Anomaly Detection**\n\nJulien Lesouple, C\u00e9dric Baudoin, Marc Spigai and Jean-Yves Tourneret\n\n[Image: A classic Elsevier logo with a tree, a person, and the words \"NON SOLUS\".]\n\n **ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. ", "original_text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n"}, "hash": "47408ccb04d88bf027903a9da8fbaaeec982c3f5d42270c6bb435396c8907c91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0fe7448-19cb-40ec-a0d9-5adedde65cf5", "node_type": "1", "metadata": {"window": "EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n", "original_text": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n"}, "hash": "f348cb55ca39cd49357562be918515f952a22c542301d7f7cc520d94ab1c5967", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. ", "mimetype": "text/plain", "start_char_idx": 1012, "end_char_idx": 1146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0fe7448-19cb-40ec-a0d9-5adedde65cf5", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n", "original_text": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "055a29a0-06a5-40a6-992f-09d92fcc7ce0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**ELSEVIER**\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. ", "original_text": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article. "}, "hash": "f1ab0d424deb9c36b7bdb5338a72183b1f2810b773ce59386bef9e005c23907a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79b00008-dd6e-4e0d-a9ac-66ee70e4105d", "node_type": "1", "metadata": {"window": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd. ", "original_text": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). "}, "hash": "89012e411d27039aa5465afa07b48edc9ce8941d8036093b7e5102657378d434", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n", "mimetype": "text/plain", "start_char_idx": 1146, "end_char_idx": 1247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "79b00008-dd6e-4e0d-a9ac-66ee70e4105d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd. ", "original_text": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0fe7448-19cb-40ec-a0d9-5adedde65cf5", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n", "original_text": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n"}, "hash": "3e160fd60648844fc247241cd38b87e170d7e83c0935ae16d0d6a9a127465f7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1fb92fe-79fd-4d22-9a99-42baf7c9cde6", "node_type": "1", "metadata": {"window": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n", "original_text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. "}, "hash": "0e2a86514d6bd15728489ca4d9208d0f550aa92a6fcefff2c9a035505759997f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). ", "mimetype": "text/plain", "start_char_idx": 1247, "end_char_idx": 2082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1fb92fe-79fd-4d22-9a99-42baf7c9cde6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n", "original_text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79b00008-dd6e-4e0d-a9ac-66ee70e4105d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, some information can be lost when com- puting the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd. ", "original_text": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). "}, "hash": "49906c16fd1faa9d687fb9437a4f75c80fba9a7260a6b240cacce89a843da739", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b71d0a12-f73b-41e5-9362-cad6345cec98", "node_type": "1", "metadata": {"window": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1. ", "original_text": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. "}, "hash": "0626f71e137adca5d3d6b3c1db3aa15b6b1a301e576b5426cdbfc195e2691843", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. ", "mimetype": "text/plain", "start_char_idx": 2082, "end_char_idx": 2175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b71d0a12-f73b-41e5-9362-cad6345cec98", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1. ", "original_text": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1fb92fe-79fd-4d22-9a99-42baf7c9cde6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter introduces a generalized isolation for- est algorithm called Generalized IF (GIF) to overcome these is- sues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n", "original_text": "EIF has shown some interest compared to IF being for instance more robust to some artefacts. "}, "hash": "e5f122f389d4288220a9353ea10c781694ddcc54f434adb3142213698529d08a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "965210cc-f83d-4db1-bb87-83f6a878c3c5", "node_type": "1", "metadata": {"window": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al. ", "original_text": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. "}, "hash": "582e1cd41ace36ffcf75df77f23c9f9d9e46b91a7707bcef230a9f0da1363da3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. ", "mimetype": "text/plain", "start_char_idx": 2175, "end_char_idx": 2300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "965210cc-f83d-4db1-bb87-83f6a878c3c5", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al. ", "original_text": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b71d0a12-f73b-41e5-9362-cad6345cec98", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n\n ---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1. ", "original_text": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. "}, "hash": "bf06a95cb931bb8d861b425466faa7e50a78d3e0c79df975485f039c0404b773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ab3bd99-1b0c-49b2-aadc-a0c7f9852caa", "node_type": "1", "metadata": {"window": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. ", "original_text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n"}, "hash": "d0c23aad55d2202936792b418ecf88cd041fc6fe8f3ca6d873a945eb913c21d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. ", "mimetype": "text/plain", "start_char_idx": 2300, "end_char_idx": 2418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ab3bd99-1b0c-49b2-aadc-a0c7f9852caa", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. ", "original_text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "965210cc-f83d-4db1-bb87-83f6a878c3c5", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n**Research Highlights (Required)**\n\nIt should be short collection of bullet points that convey the core findings of the article.  It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al. ", "original_text": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. "}, "hash": "85d51c6218540995e70c180e004a8fb50a7db27605e8c90e7061df1ce4a81e9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "679b4e35-ef79-4bc0-9159-6b7c04633e3e", "node_type": "1", "metadata": {"window": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. ", "original_text": "\u00a9 2021 Elsevier Ltd. "}, "hash": "cb434c032e6894bb3c0f2fc2c9c8b4b5455dab834f6999191fb586682d694dc6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n", "mimetype": "text/plain", "start_char_idx": 2418, "end_char_idx": 2572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "679b4e35-ef79-4bc0-9159-6b7c04633e3e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. ", "original_text": "\u00a9 2021 Elsevier Ltd. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ab3bd99-1b0c-49b2-aadc-a0c7f9852caa", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It should include 3 to 5 bullet points (maximum 85 characters, including spaces, per bullet point.)\n\n *   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. ", "original_text": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n"}, "hash": "74e302b2b4a12150fbbc36c65c239e589c73ef1c8f2d0882c5cb78579531b95d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9664f2f2-dc00-4736-83d3-d3d46cea1526", "node_type": "1", "metadata": {"window": "EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. ", "original_text": "All rights reserved.\n\n"}, "hash": "82d37c41c076ad8824c0838b350477ccfc8d1c7b07519f1e325be58c95127d8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u00a9 2021 Elsevier Ltd. ", "mimetype": "text/plain", "start_char_idx": 2572, "end_char_idx": 2593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9664f2f2-dc00-4736-83d3-d3d46cea1526", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. ", "original_text": "All rights reserved.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "679b4e35-ef79-4bc0-9159-6b7c04633e3e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "*   We propose a new unsupervised Anomaly Detection (AD) algorithm\n*   This algorithm is based on Isolation Forest with random hyperplanes instead of random dimensions\n*   The proposed method improves the existing Extended Isolation Forest (EIF) in terms of computation time\n\n---\n\n**Pattern Recognition Letters**\n*journal homepage: www.elsevier.com*\n\n## Generalized Isolation Forest for Anomaly Detection\n\nJulien Lesouplea,\u2217\u2217, C\u00e9dric Baudoinb, Marc Spigaib, Jean-Yves Tournereta,c\n\n*aT\u00e9SA, 7 Boulevard de la Gare, 31000 Toulouse, France*\n*bThales Alenia Space, 26 Avenue Jean-Fran\u00e7ois Champollion, 31100 Toulouse France*\n*cUniversity of Toulouse/INP-ENSEEIHT/IRIT, 2 Rue Charles Camichel, 31071, Toulouse, France*\n\n### ABSTRACT\n\nThis letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF).  EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. ", "original_text": "\u00a9 2021 Elsevier Ltd. "}, "hash": "70286b5dd645a849f826833bfd2c39ea6d109201339718f41989a2d850f4908d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7dcbf6f-f6b3-4e1f-8e44-8f83d0508722", "node_type": "1", "metadata": {"window": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al. ", "original_text": "### 1. "}, "hash": "25a9da2d2b974de383ce491c92a796350bc72c1ba72657d1a4e5e33d497101a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All rights reserved.\n\n", "mimetype": "text/plain", "start_char_idx": 2593, "end_char_idx": 2615, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7dcbf6f-f6b3-4e1f-8e44-8f83d0508722", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al. ", "original_text": "### 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9664f2f2-dc00-4736-83d3-d3d46cea1526", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "EIF has shown some interest compared to IF being for instance more robust to some artefacts.  However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. ", "original_text": "All rights reserved.\n\n"}, "hash": "9a3615a1a472e1666ed89bed64da9cd0ac3bf4ae4b5a4f938de8d6a39885f41c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "796d2f14-4ae3-4273-b97c-5444ec6971d8", "node_type": "1", "metadata": {"window": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al. ", "original_text": "Introduction\n\nAnomaly Detection (AD, Chandola et al. "}, "hash": "46446c4c5a7ea9a3e5733a94c8eeae281cec148aa4e6a711a5de0bc8fbb6f285", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 1. ", "mimetype": "text/plain", "start_char_idx": 2615, "end_char_idx": 2622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "796d2f14-4ae3-4273-b97c-5444ec6971d8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al. ", "original_text": "Introduction\n\nAnomaly Detection (AD, Chandola et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7dcbf6f-f6b3-4e1f-8e44-8f83d0508722", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches.  This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al. ", "original_text": "### 1. "}, "hash": "20c050df873149651000533c9a3bddb02fac1720569d19dbb7bbf2f3c99639a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28bb72ba-b253-4ade-9fc6-4e59cdc7d55b", "node_type": "1", "metadata": {"window": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al. ", "original_text": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. "}, "hash": "14380bc8ce2514da9643a531ee20a4ba71dfe3e8e96c0d0b1336fabe7bb5b398", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction\n\nAnomaly Detection (AD, Chandola et al. ", "mimetype": "text/plain", "start_char_idx": 2622, "end_char_idx": 2675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28bb72ba-b253-4ade-9fc6-4e59cdc7d55b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al. ", "original_text": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "796d2f14-4ae3-4273-b97c-5444ec6971d8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues.  GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al. ", "original_text": "Introduction\n\nAnomaly Detection (AD, Chandola et al. "}, "hash": "300d86cc1091ac02918276f9e1541f9ccaec083d5e3301f7c6ac7e4706c7dc0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e53447c3-4f44-41f1-b50d-d006235e4194", "node_type": "1", "metadata": {"window": "\u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)). ", "original_text": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. "}, "hash": "6023186dfbf5dd74f4f894011c88bac78912c5fb2ddc77f6e159b82ee7549912", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. ", "mimetype": "text/plain", "start_char_idx": 2675, "end_char_idx": 2831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e53447c3-4f44-41f1-b50d-d006235e4194", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)). ", "original_text": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28bb72ba-b253-4ade-9fc6-4e59cdc7d55b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.\n \u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al. ", "original_text": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms. "}, "hash": "5b1e2b3aebecda3f7412d21978bc3efe0a26b20cd1c4a5664303e08eef35f7cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98f46286-0a98-4448-8788-8c8168e1e075", "node_type": "1", "metadata": {"window": "All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. ", "original_text": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. "}, "hash": "16c33aec838df1ec8e928deb415800bc3b20bf6520ca904af0729d46ca528cfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. ", "mimetype": "text/plain", "start_char_idx": 2831, "end_char_idx": 2927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98f46286-0a98-4448-8788-8c8168e1e075", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. ", "original_text": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e53447c3-4f44-41f1-b50d-d006235e4194", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\u00a9 2021 Elsevier Ltd.  All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)). ", "original_text": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies. "}, "hash": "71aca0e64a4bfea1f00e6164bfb75f9ee854b9871151db62b9b43eb926269fdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd42c68f-34a2-4dae-92b9-5fb40bf9bc73", "node_type": "1", "metadata": {"window": "### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n", "original_text": "(1999)), crowd surveillance (Leach et al. "}, "hash": "931bd40cbdec6ee2456bd91f2d3481e0d164b75ebc4f970424099b1e4d544222", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. ", "mimetype": "text/plain", "start_char_idx": 2927, "end_char_idx": 3030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd42c68f-34a2-4dae-92b9-5fb40bf9bc73", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n", "original_text": "(1999)), crowd surveillance (Leach et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98f46286-0a98-4448-8788-8c8168e1e075", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "All rights reserved.\n\n ### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. ", "original_text": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al. "}, "hash": "ccefd82ce6bee10ec7e065fe2f67596a5e625fc1c1acfff99a0b17783a6ec827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b39c60e3-f562-4997-a617-ff76365c173c", "node_type": "1", "metadata": {"window": "Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. ", "original_text": "(2014)), or in satellite telemetry monitoring (Yairi et al. "}, "hash": "4500bcd86db13d8d5eb032110c5a8e4584fb98882f6ad796cb03f1c7f16208ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(1999)), crowd surveillance (Leach et al. ", "mimetype": "text/plain", "start_char_idx": 3030, "end_char_idx": 3072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b39c60e3-f562-4997-a617-ff76365c173c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. ", "original_text": "(2014)), or in satellite telemetry monitoring (Yairi et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd42c68f-34a2-4dae-92b9-5fb40bf9bc73", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 1.  Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n", "original_text": "(1999)), crowd surveillance (Leach et al. "}, "hash": "8aa0ee2c350dc73b39b0194ffa9b1aabaced349a4c8d7a326f003ee076dbb673", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc4857c4-f084-4b23-a228-9edc89d19048", "node_type": "1", "metadata": {"window": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set. ", "original_text": "(2017); Pilastre et al. "}, "hash": "0775104b11335f6606ea987d32215a6ef25d27cf90c0fee9cdb8e7c79eff5fd4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2014)), or in satellite telemetry monitoring (Yairi et al. ", "mimetype": "text/plain", "start_char_idx": 3072, "end_char_idx": 3132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc4857c4-f084-4b23-a228-9edc89d19048", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set. ", "original_text": "(2017); Pilastre et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b39c60e3-f562-4997-a617-ff76365c173c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Introduction\n\nAnomaly Detection (AD, Chandola et al.  (2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. ", "original_text": "(2014)), or in satellite telemetry monitoring (Yairi et al. "}, "hash": "179f25fdead615ee64c218060138defef9db0eef9f2ec50bf16ff4d3a1d8ac68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccd58c5a-4ca5-47ae-be86-5d50e76043b4", "node_type": "1", "metadata": {"window": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. ", "original_text": "(2020)). "}, "hash": "d2e06ddba18299a79a6df9cc39273645afad1480e66303344347adfeaa99a6e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2017); Pilastre et al. ", "mimetype": "text/plain", "start_char_idx": 3132, "end_char_idx": 3156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ccd58c5a-4ca5-47ae-be86-5d50e76043b4", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. ", "original_text": "(2020)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc4857c4-f084-4b23-a228-9edc89d19048", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2009)) has gained attention in the past few years, due to the enhancement of modern computers and the increasing interest for machine learning algorithms.  AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set. ", "original_text": "(2017); Pilastre et al. "}, "hash": "f2cd0e08ffcddb217c7adedf8a855fa884331af5113bd3c5a672c9beba552190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac312346-2173-4341-967e-5ef1d68c456a", "node_type": "1", "metadata": {"window": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al. ", "original_text": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. "}, "hash": "05700febf14ed97aa08f4e9d5fa783499a4568b232e0fbed90af476a03eabf4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2020)). ", "mimetype": "text/plain", "start_char_idx": 3156, "end_char_idx": 3165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac312346-2173-4341-967e-5ef1d68c456a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al. ", "original_text": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccd58c5a-4ca5-47ae-be86-5d50e76043b4", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "AD consists in detecting rare patterns or unobserved samples in data, referred to as anomalies.  It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. ", "original_text": "(2020)). "}, "hash": "a8af32904b937ab5973cd27e4b0a816af0020e6ad3d9f414a4a8e9e08641a87d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc14ea2d-8b9a-4153-9eee-e85748ff3ebf", "node_type": "1", "metadata": {"window": "(1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al. ", "original_text": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n"}, "hash": "4765e3cefba0f6fb572a76fe88f2f1403bac4b7e561eeb907ebc91de0d38e7bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. ", "mimetype": "text/plain", "start_char_idx": 3165, "end_char_idx": 3356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc14ea2d-8b9a-4153-9eee-e85748ff3ebf", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al. ", "original_text": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac312346-2173-4341-967e-5ef1d68c456a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It is widely used in potentially critical environments, e.g., in credit fraud detection (Brause et al.  (1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al. ", "original_text": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time. "}, "hash": "b3faff2e40d27e918bbe88daf904a22742f982fccca3200b63a9cce9bf82f169", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bbfc269-59e1-45e5-b291-bd26a9d92c90", "node_type": "1", "metadata": {"window": "(2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. ", "original_text": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. "}, "hash": "e979992fcdcf5177a3280f5b4f9fa4ed1c53420b423bc7f89a8ff6f5c9c8d105", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n", "mimetype": "text/plain", "start_char_idx": 3356, "end_char_idx": 3508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bbfc269-59e1-45e5-b291-bd26a9d92c90", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. ", "original_text": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc14ea2d-8b9a-4153-9eee-e85748ff3ebf", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(1999)), crowd surveillance (Leach et al.  (2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al. ", "original_text": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n"}, "hash": "79c2c1109703690e4d8e3e15b6a2d50bab892922f2ea152ecd53f98a6482ac51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3b68945-0678-4c0a-8fc8-86c4975b3d8a", "node_type": "1", "metadata": {"window": "(2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. ", "original_text": "The performance of the algorithm can then be tested using a labeled dataset called test set. "}, "hash": "335199a96759c64126534ad33636a7654cc43bf719dffa8f53b8b68c36b73d8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. ", "mimetype": "text/plain", "start_char_idx": 3508, "end_char_idx": 3645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c3b68945-0678-4c0a-8fc8-86c4975b3d8a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. ", "original_text": "The performance of the algorithm can then be tested using a labeled dataset called test set. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bbfc269-59e1-45e5-b291-bd26a9d92c90", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2014)), or in satellite telemetry monitoring (Yairi et al.  (2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. ", "original_text": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset. "}, "hash": "a68b597c43f7edb2d35e082aa93fe315f46894605cf63f6441ea6642d4f0f8cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52fc3ed1-eeb8-4470-a0d1-b7ddb03712e7", "node_type": "1", "metadata": {"window": "(2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n", "original_text": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. "}, "hash": "6c97939cc5f3ad6927cc4577f1fc7df41a5769f27679f071e078595b95b2551f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The performance of the algorithm can then be tested using a labeled dataset called test set. ", "mimetype": "text/plain", "start_char_idx": 3645, "end_char_idx": 3738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52fc3ed1-eeb8-4470-a0d1-b7ddb03712e7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n", "original_text": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3b68945-0678-4c0a-8fc8-86c4975b3d8a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2017); Pilastre et al.  (2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. ", "original_text": "The performance of the algorithm can then be tested using a labeled dataset called test set. "}, "hash": "39fce4be07d04c7887ca7716fc5726241ec5179f32e4693bcd076cb756df2ebe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0e360c5-09c0-487b-b501-5188d2bfd75e", "node_type": "1", "metadata": {"window": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. ", "original_text": "(2000), Local Outlier Probability (LoOP), Kriegel et al. "}, "hash": "4f28c44aa86530cae6b0e84eb695a6f0eaef82397625a0beb88e5d3cc2f0b9d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. ", "mimetype": "text/plain", "start_char_idx": 3738, "end_char_idx": 3878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0e360c5-09c0-487b-b501-5188d2bfd75e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. ", "original_text": "(2000), Local Outlier Probability (LoOP), Kriegel et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52fc3ed1-eeb8-4470-a0d1-b7ddb03712e7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2020)).  AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n", "original_text": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al. "}, "hash": "6a554ab18aa67796c3955af87efbd4dfc786b0742f1cc7995b096406926363b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8abb34a3-a2a1-4a79-9e8e-6451d4e4dd21", "node_type": "1", "metadata": {"window": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point. ", "original_text": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al. "}, "hash": "3babd7733ba53c0d82ea75710e476e6e9dc18f3e23834ff4f16dc1a56e038a67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2000), Local Outlier Probability (LoOP), Kriegel et al. ", "mimetype": "text/plain", "start_char_idx": 3878, "end_char_idx": 3935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8abb34a3-a2a1-4a79-9e8e-6451d4e4dd21", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point. ", "original_text": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0e360c5-09c0-487b-b501-5188d2bfd75e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "AD has received an increasing interest for satellite monitoring in the past few years, with new satellite constellations, resulting in a huge amount of data to be processed at the same time.  Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. ", "original_text": "(2000), Local Outlier Probability (LoOP), Kriegel et al. "}, "hash": "716f105066e28cc2efaeb8d337fa2a1211fc67a829a9dcd93060895d69b33c0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97eff651-775f-4326-a889-83bc4d7325ba", "node_type": "1", "metadata": {"window": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree. ", "original_text": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. "}, "hash": "1ea30709b4f0d55c8ed4d8c2e3c1781532060f78fb5febd71257063fa04a5523", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al. ", "mimetype": "text/plain", "start_char_idx": 3935, "end_char_idx": 3991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97eff651-775f-4326-a889-83bc4d7325ba", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree. ", "original_text": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8abb34a3-a2a1-4a79-9e8e-6451d4e4dd21", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Time-series resulting from satellite telemetry are of course used for the constellation mission but also for system monitoring and failure prevention.\n\n This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point. ", "original_text": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al. "}, "hash": "738e617a35d9681427adefe69d29de906e9ad7b284fe101641fbe1e18591bf30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f02bc1fa-a643-4f8d-ab21-11703932cc32", "node_type": "1", "metadata": {"window": "The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. ", "original_text": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. "}, "hash": "58cc3f6456ce3624d72fa8b997b0ff2b9d5a344548e5794c2062b3ed5f4fbbd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. ", "mimetype": "text/plain", "start_char_idx": 3991, "end_char_idx": 4131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f02bc1fa-a643-4f8d-ab21-11703932cc32", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. ", "original_text": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97eff651-775f-4326-a889-83bc4d7325ba", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter focuses on unsupervised AD algorithms, which learn the normal behavior of unlabeled data using a so-called training dataset.  The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree. ", "original_text": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al. "}, "hash": "20e55bf1fb9b0a4912044193b8f994e745f4917522e1fa1bc09b52dca9f6b5a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94b233d6-ee7d-4a3d-b6f5-b2eeff025670", "node_type": "1", "metadata": {"window": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. ", "original_text": "(2008)).\n\n"}, "hash": "bbcba45f3b2719c2576f7eb857c586d1cef25d6d9c16629918e18231ad6afd41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. ", "mimetype": "text/plain", "start_char_idx": 4131, "end_char_idx": 4219, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94b233d6-ee7d-4a3d-b6f5-b2eeff025670", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. ", "original_text": "(2008)).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f02bc1fa-a643-4f8d-ab21-11703932cc32", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The performance of the algorithm can then be tested using a labeled dataset called test set.  Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. ", "original_text": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al. "}, "hash": "8220b21d65ffadf7c84885fc84d793771b1ef7447c85bf62f845be7a50fec28e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99db41de-1ceb-4e63-9e51-680975f5979b", "node_type": "1", "metadata": {"window": "(2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. ", "original_text": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. "}, "hash": "01151063622059fd1c8433c5ae4b6e9a44a8a8266f7855d02fda4f1eb9a81182", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2008)).\n\n", "mimetype": "text/plain", "start_char_idx": 4219, "end_char_idx": 4229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99db41de-1ceb-4e63-9e51-680975f5979b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. ", "original_text": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94b233d6-ee7d-4a3d-b6f5-b2eeff025670", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Various AD algorithms have been proposed in the literature including those based on nearest neighbors (Local Outlier Factor, Breunig et al.  (2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. ", "original_text": "(2008)).\n\n"}, "hash": "8a4276513b4e39ec4ae1cdd9e12aa2c3b9038b0e22e34f8ab7395546c4ba0ac4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10d7fee1-174d-483f-ab30-844c537d8f13", "node_type": "1", "metadata": {"window": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019). ", "original_text": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point. "}, "hash": "8bf956a35fc043b8a8f0b9e44ae5c496177f73d9865817b20ed76ae65995280a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. ", "mimetype": "text/plain", "start_char_idx": 4229, "end_char_idx": 4400, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10d7fee1-174d-483f-ab30-844c537d8f13", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019). ", "original_text": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99db41de-1ceb-4e63-9e51-680975f5979b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2000), Local Outlier Probability (LoOP), Kriegel et al.  (2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. ", "original_text": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data. "}, "hash": "0c19dda991ae173dfcbe7748aae3ca4f48be2fadf887f636c8ba0973ea710c5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1de963c0-7451-45f9-845c-39c3d105bd36", "node_type": "1", "metadata": {"window": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. ", "original_text": "The number of branches required to isolate each point is then computed for each tree. "}, "hash": "216d0f2bfa2f3d8420441d3a63be1bfdee7d976304edc4cda122ae5c6bc42fd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point. ", "mimetype": "text/plain", "start_char_idx": 4400, "end_char_idx": 4502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1de963c0-7451-45f9-845c-39c3d105bd36", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. ", "original_text": "The number of branches required to isolate each point is then computed for each tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10d7fee1-174d-483f-ab30-844c537d8f13", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2009) or Neighborhood Construction (NC), \u0130nkaya et al.  (2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019). ", "original_text": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point. "}, "hash": "7cd25323c3178f7e1518e2699cbde4023f87a9d5bbdefd55a24751c0aacfb39e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85fc811a-1a32-46ba-8e10-9773901f7c82", "node_type": "1", "metadata": {"window": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)). ", "original_text": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. "}, "hash": "80b93c2f44aad01201b70a82abe2fb27be05f56243fd24ae6233dd4a4f4d5389", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The number of branches required to isolate each point is then computed for each tree. ", "mimetype": "text/plain", "start_char_idx": 4502, "end_char_idx": 4588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85fc811a-1a32-46ba-8e10-9773901f7c82", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)). ", "original_text": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1de963c0-7451-45f9-845c-39c3d105bd36", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2015)), support vector machines (Support Vector Data Description, Tax and Duin (2004), One Class Support Vector Machines, Sch\u00f6lkopf et al.  (2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. ", "original_text": "The number of branches required to isolate each point is then computed for each tree. "}, "hash": "8989db9d47ec9e259618853fbea444fd54db6879df399357b253158b542edeb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f600b85a-14ca-4fa2-a23a-a4f0f7396de8", "node_type": "1", "metadata": {"window": "(2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. ", "original_text": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. "}, "hash": "a6f8ea221885bb08da6d4f21912428f584b303833aa4d64f4d870ea9d5a730ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. ", "mimetype": "text/plain", "start_char_idx": 4588, "end_char_idx": 4704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f600b85a-14ca-4fa2-a23a-a4f0f7396de8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. ", "original_text": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85fc811a-1a32-46ba-8e10-9773901f7c82", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2001)), Sparse Coding (Dutta and Banerjee (2019)), or Isolation Forest (IF, Liu et al.  (2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)). ", "original_text": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest. "}, "hash": "fb357aac0b21cdd7a94974b20aaa4463d0a46eb189f51855ef4679a8883a758e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfbd4bca-02b8-40ac-9c29-917fc6abb574", "node_type": "1", "metadata": {"window": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. ", "original_text": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. "}, "hash": "b213ea894d34f8d746b0e725431a376dd06514377901a40fd09183bfbd5f4406", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. ", "mimetype": "text/plain", "start_char_idx": 4704, "end_char_idx": 4848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfbd4bca-02b8-40ac-9c29-917fc6abb574", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. ", "original_text": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f600b85a-14ca-4fa2-a23a-a4f0f7396de8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008)).\n\n A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. ", "original_text": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data. "}, "hash": "c2bf92ed6e02200a8b22df992ff7143adad3d9d0aa235d1d46883073d480e8b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55d70de1-0407-4867-9a8e-246f4708280b", "node_type": "1", "metadata": {"window": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. ", "original_text": "(2019). "}, "hash": "9ee2d6cac95392e924d44efd846d18400566262633d285a70464faf1c7c767db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 4848, "end_char_idx": 5016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "55d70de1-0407-4867-9a8e-246f4708280b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. ", "original_text": "(2019). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfbd4bca-02b8-40ac-9c29-917fc6abb574", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A specific attention is devoted in this letter to IF, which aims at finding anomalies with the idea that in some feature space, anomalies should be \"far\" from other data.  To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. ", "original_text": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al. "}, "hash": "bdbea4941d852ce5fb57bfc67fcde8272e09c6a33a28edbf0d214bd456f3f861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c6a4039-91c1-4641-8388-1c4a7bfd983d", "node_type": "1", "metadata": {"window": "The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm. ", "original_text": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. "}, "hash": "5308b464176269f65b2edc9daa53ec71cc93599b7581a99355eb33b39fdae1c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019). ", "mimetype": "text/plain", "start_char_idx": 5016, "end_char_idx": 5024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c6a4039-91c1-4641-8388-1c4a7bfd983d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm. ", "original_text": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55d70de1-0407-4867-9a8e-246f4708280b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To look for these anomalies, IF generates random isolation trees in order to isolate each data point.  The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. ", "original_text": "(2019). "}, "hash": "7d88e4c1210b859528a2b738e4e884f60c5b34308884566efc8aa10e8ef89072", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2929aac7-2c61-4056-99a2-b3c922a9ef84", "node_type": "1", "metadata": {"window": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n", "original_text": "(2019)). "}, "hash": "f56f52447b53f084582c59d4e2f93ac3b4ba2e7802264da4ee5417458a0edeaa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 5024, "end_char_idx": 5145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2929aac7-2c61-4056-99a2-b3c922a9ef84", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n", "original_text": "(2019)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c6a4039-91c1-4641-8388-1c4a7bfd983d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The number of branches required to isolate each point is then computed for each tree.  The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm. ", "original_text": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al. "}, "hash": "a70d55771f8cb577fd824a9d330c3289a25af83afd6c942f0efcc6c2dd0a1c41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c058657-c8be-441a-88df-74c7b730e58f", "node_type": "1", "metadata": {"window": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. ", "original_text": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. "}, "hash": "a8b5f20d5d1e32301dca082f0dccba4f0683c57525cde9cf7a7429d284eb5a97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019)). ", "mimetype": "text/plain", "start_char_idx": 5145, "end_char_idx": 5154, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c058657-c8be-441a-88df-74c7b730e58f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. ", "original_text": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2929aac7-2c61-4056-99a2-b3c922a9ef84", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The mean of this number of branches defines the expected path length, which is used to isolate a point of interest.  The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n", "original_text": "(2019)). "}, "hash": "8b568c8b64b35b4a91e3a99d067428d8b6f019eae06103b9858df5ae932c4191", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f551502-ba92-4202-9201-9be8d4752f41", "node_type": "1", "metadata": {"window": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel.", "original_text": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. "}, "hash": "5ba00d11091cf49cc6223dabf7adf97caa4e95c3038c76b6b301b65b55af638d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. ", "mimetype": "text/plain", "start_char_idx": 5154, "end_char_idx": 5304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f551502-ba92-4202-9201-9be8d4752f41", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel.", "original_text": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c058657-c8be-441a-88df-74c7b730e58f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The expected path length is generally small for anomalies (contrary to nominal data) since anomalies are far from the majority of nominal data.  However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. ", "original_text": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm. "}, "hash": "64a0a3b6c043bc2227e8451e8c26ee3ba056a3445c5b1c20485d5a4ab4a928c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff5edaf3-a383-4362-9cb5-bf7b332ab23e", "node_type": "1", "metadata": {"window": "(2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. ", "original_text": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. "}, "hash": "899a25a5ba0446f8257d234a4cf12972c86abe74157a03541b2d2bdf31c0b359", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. ", "mimetype": "text/plain", "start_char_idx": 5304, "end_char_idx": 5437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff5edaf3-a383-4362-9cb5-bf7b332ab23e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. ", "original_text": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f551502-ba92-4202-9201-9be8d4752f41", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, the trees generated by IF are considering a random feature at each node, which can lead to some artefacts in the score map function, as shown in Hariri et al.  (2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel.", "original_text": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest. "}, "hash": "4f03449c46d3922a9b11442c393d5301fab79950001b5531f0590932aeedae0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "456f2357-b3b4-4f6d-8e9f-3116b252d5a8", "node_type": "1", "metadata": {"window": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. ", "original_text": "(2019) leading to the generalized isolation forest (GIF) algorithm. "}, "hash": "549a22afbe40e82f5942e136304927a72e26ac8c9cc253c55a46b6e9d1a2166b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 5437, "end_char_idx": 5542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "456f2357-b3b4-4f6d-8e9f-3116b252d5a8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. ", "original_text": "(2019) leading to the generalized isolation forest (GIF) algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff5edaf3-a383-4362-9cb5-bf7b332ab23e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019).  In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. ", "original_text": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al. "}, "hash": "e8be3b9837248eebb6b60bfde6555bbf39637ee473d3b7f8cb34ebc5b49c349a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47c24b97-c33f-4ad1-b66a-033b13890942", "node_type": "1", "metadata": {"window": "(2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster. ", "original_text": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n"}, "hash": "0a28d0a427eb6b71a9e06f1637a536c8107fd1e1f55624259a15f7c851e2ed76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) leading to the generalized isolation forest (GIF) algorithm. ", "mimetype": "text/plain", "start_char_idx": 5542, "end_char_idx": 5610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47c24b97-c33f-4ad1-b66a-033b13890942", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster. ", "original_text": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "456f2357-b3b4-4f6d-8e9f-3116b252d5a8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In order to improve the isolation of data points, tree branches with random hyperplanes can be considered (Hariri et al.  (2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. ", "original_text": "(2019) leading to the generalized isolation forest (GIF) algorithm. "}, "hash": "868f760071ac3a39e56c7c3f6d32805bc11acafad269e7d9da1e3d21b967494e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e399932-a518-42f8-906b-2a42995787fd", "node_type": "1", "metadata": {"window": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2. ", "original_text": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. "}, "hash": "16edcab448c4f77ead041028b4bd3b305c5dd1c20436fb5526ff57988c151b6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n", "mimetype": "text/plain", "start_char_idx": 5610, "end_char_idx": 5742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e399932-a518-42f8-906b-2a42995787fd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2. ", "original_text": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47c24b97-c33f-4ad1-b66a-033b13890942", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)).  Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster. ", "original_text": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n"}, "hash": "c21703e4682f49e748c94c198d09930006db9b264c6e04b299789059287c7979", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b40d9543-f861-4042-8d2a-2a10ceff5a5b", "node_type": "1", "metadata": {"window": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. ", "original_text": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel."}, "hash": "36e6d3043d4ce3364a6ffe3f9a7e49ea5cf05bbdf082925bd216c6d318b5969f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. ", "mimetype": "text/plain", "start_char_idx": 5742, "end_char_idx": 5869, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b40d9543-f861-4042-8d2a-2a10ceff5a5b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. ", "original_text": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e399932-a518-42f8-906b-2a42995787fd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Random hyperplanes are not necessarily parallel to one of the components of the feature vector and have been used in the extended IF (EIF) algorithm.  Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2. ", "original_text": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm. "}, "hash": "425bb5c1aaa463f9c2c75bcc6dd745ce29a78f1a6a3d6d90942a06e5c1061b60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "935bfe77-3d49-4a94-bc7a-c668395db64d", "node_type": "1", "metadata": {"window": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). ", "original_text": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. "}, "hash": "ad6aa51fa8876164a6abcbfddf055983c62a66f894e16db30488b9469911d117", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel.", "mimetype": "text/plain", "start_char_idx": 5869, "end_char_idx": 5965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "935bfe77-3d49-4a94-bc7a-c668395db64d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). ", "original_text": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b40d9543-f861-4042-8d2a-2a10ceff5a5b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Unfortunately, this strategy generates a lot of empty branches, which increases the complexity of the trees belonging to the forest.  This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. ", "original_text": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel."}, "hash": "5a47257f292940b09c21403bcc132a490a0e2852b76f0d01ff09d3ded2c6704f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4265bd12-68cd-42cc-ab14-754744676c2c", "node_type": "1", "metadata": {"window": "(2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n", "original_text": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. "}, "hash": "4481555e46b78828a4af91821d133dfaca678ee273c8570a8152999a0ebef33a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. ", "mimetype": "text/plain", "start_char_idx": 5965, "end_char_idx": 6079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4265bd12-68cd-42cc-ab14-754744676c2c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n", "original_text": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "935bfe77-3d49-4a94-bc7a-c668395db64d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter goes a step further by proposing a new IF construction inspired by the work of Hariri et al.  (2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). ", "original_text": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side. "}, "hash": "b24a48c2eb3ddc87e683f64b2487d956b29554c7763cf00b9c75071a82bcaf7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c0f1e3f-6ff2-4647-a535-cb70d12cd575", "node_type": "1", "metadata": {"window": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig. ", "original_text": "A red line encloses the main cluster. "}, "hash": "1afacd809cb974c5266b5477836621f4fe4802e49927331fa5d501c93d868326", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. ", "mimetype": "text/plain", "start_char_idx": 6079, "end_char_idx": 6211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c0f1e3f-6ff2-4647-a535-cb70d12cd575", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig. ", "original_text": "A red line encloses the main cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4265bd12-68cd-42cc-ab14-754744676c2c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) leading to the generalized isolation forest (GIF) algorithm.  The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n", "original_text": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers. "}, "hash": "49a24abf4e3776857cef53357ab57a2a65850d27132af159e6250a0dd0992303", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a014cad4-e7bb-4194-883d-0e203e1601ff", "node_type": "1", "metadata": {"window": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data. ", "original_text": "The x-axis is X1 and the y-axis is X2. "}, "hash": "81bc3302746547b881ef5c02c8a6ab17f78905ae7661c889329680b1fe434353", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A red line encloses the main cluster. ", "mimetype": "text/plain", "start_char_idx": 6211, "end_char_idx": 6249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a014cad4-e7bb-4194-883d-0e203e1601ff", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data. ", "original_text": "The x-axis is X1 and the y-axis is X2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c0f1e3f-6ff2-4647-a535-cb70d12cd575", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The GIF algorithm generates trees without any empty branch, which significantly improves the execution times when compared to EIF.\n\n This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig. ", "original_text": "A red line encloses the main cluster. "}, "hash": "a8277c9899d86939c2ce09632fbdbd69ee175e97a34949746a604cc8ad5b28cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d7fdff6-76fa-438f-b13e-6e36c99a6cee", "node_type": "1", "metadata": {"window": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). ", "original_text": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. "}, "hash": "2a07bf71798d5a6f5d99be12da4dee784f3c1c597da3163c7641a8e9cc9abe1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is X1 and the y-axis is X2. ", "mimetype": "text/plain", "start_char_idx": 6249, "end_char_idx": 6288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d7fdff6-76fa-438f-b13e-6e36c99a6cee", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). ", "original_text": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a014cad4-e7bb-4194-883d-0e203e1601ff", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter is organized as follow: Section II recalls the principles of IF and EIF and introduces the proposed GIF algorithm.  Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data. ", "original_text": "The x-axis is X1 and the y-axis is X2. "}, "hash": "7de0d4c142cd5f59328eb527f0aa1bd802f12fe19fe8e1fe5a9c68fd56d09b81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99cb1786-23b0-4c91-a430-cca544068d59", "node_type": "1", "metadata": {"window": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n", "original_text": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). "}, "hash": "cccd63325d4816f14153945a858637b368a81a93ea8403565298fd0c900d6dd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. ", "mimetype": "text/plain", "start_char_idx": 6288, "end_char_idx": 6397, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99cb1786-23b0-4c91-a430-cca544068d59", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n", "original_text": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d7fdff6-76fa-438f-b13e-6e36c99a6cee", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Section III evaluates the performance of GIF using experiments\n\n---\n\u2217\u2217Corresponding author: Tel. : +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). ", "original_text": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space. "}, "hash": "2a176ea0fb16880bd39557d2fa840e6d95fa007e4628a8d7ea5640247dc001a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "917a6d56-b77f-4749-a1e0-04ada0589ad1", "node_type": "1", "metadata": {"window": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets. ", "original_text": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n"}, "hash": "f1dba9512c4fa56bb6e43ceac2b2e6c49c48323aa25d9556fcec8b148341a1f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). ", "mimetype": "text/plain", "start_char_idx": 6397, "end_char_idx": 6510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "917a6d56-b77f-4749-a1e0-04ada0589ad1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets. ", "original_text": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99cb1786-23b0-4c91-a430-cca544068d59", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ": +33-5-61-24-73-64;\ne-mail: julien.lesouple@tesa.prd.fr (Julien Lesouple)\n\n---\n\n[Image: Two charts side-by-side.  The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n", "original_text": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score). "}, "hash": "296fe4d6c7f30fedaf6ee5bab119b729451c98050e9b242114e56abecaf9dbe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7717391f-8c9d-47ea-a30b-d5bdf1201ba0", "node_type": "1", "metadata": {"window": "A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n", "original_text": "**Fig. "}, "hash": "d8ad0fe8e5cd642fe26bf907517772800c81356aaabf1299288cde695b3dbbbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n", "mimetype": "text/plain", "start_char_idx": 6510, "end_char_idx": 6653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7717391f-8c9d-47ea-a30b-d5bdf1201ba0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "917a6d56-b77f-4749-a1e0-04ada0589ad1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The left chart, titled \"Data features in 2D\", shows a scatter plot of data points, with a cluster in the center and a few outliers.  A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets. ", "original_text": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n"}, "hash": "9bc600f6b5a5e5529414fc8592257eb333c3fa8a4e7fd9571a8fb8a37e9f9bd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19354102-0b9c-42f6-b592-3afdee1fecda", "node_type": "1", "metadata": {"window": "The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2. ", "original_text": "1:** Illustration of IF problems using artificial 2D data. "}, "hash": "aa11b1ab79312f7de4f82a5767933fdd8990d1a7841edbbed43143e6e6d46dd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 6653, "end_char_idx": 6660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19354102-0b9c-42f6-b592-3afdee1fecda", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2. ", "original_text": "1:** Illustration of IF problems using artificial 2D data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7717391f-8c9d-47ea-a30b-d5bdf1201ba0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A red line encloses the main cluster.  The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n", "original_text": "**Fig. "}, "hash": "f47a754ccec3a6caeb7e43116e6129195641614815647c358f0768dd49e92e88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bb0e7aa-369b-472b-93b7-92698dafd33e", "node_type": "1", "metadata": {"window": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1. ", "original_text": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). "}, "hash": "f0d3e65faf94786f70de8f377ee9ff4477de1435f86f645b71faf968082486b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1:** Illustration of IF problems using artificial 2D data. ", "mimetype": "text/plain", "start_char_idx": 6660, "end_char_idx": 6719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0bb0e7aa-369b-472b-93b7-92698dafd33e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1. ", "original_text": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19354102-0b9c-42f6-b592-3afdee1fecda", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The x-axis is X1 and the y-axis is X2.  The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2. ", "original_text": "1:** Illustration of IF problems using artificial 2D data. "}, "hash": "67cbdd1ae35baf347b6a419e99400929d8212e63190fceefe176d5289398a90a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "977c7ca1-2d6d-4255-9adf-d56d9c87b1ee", "node_type": "1", "metadata": {"window": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. ", "original_text": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n"}, "hash": "f5012808fccd6bce72caf06e0c152d10f216afe270ad2744a8181fbd46d64aa8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). ", "mimetype": "text/plain", "start_char_idx": 6719, "end_char_idx": 6819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "977c7ca1-2d6d-4255-9adf-d56d9c87b1ee", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. ", "original_text": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bb0e7aa-369b-472b-93b7-92698dafd33e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The right chart, titled \"Anomaly color map in 2D\", shows a heat map of anomaly scores for the same 2D space.  The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1. ", "original_text": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red). "}, "hash": "1b8b979cc48df3b3f5b34b614b8d12b94c7dd48bac27946b01d0d2516833a8a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c360af4-c065-4692-a143-300e045c5ae3", "node_type": "1", "metadata": {"window": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n", "original_text": "---\n\non both synthetic and real benchmark datasets. "}, "hash": "61999eefbf7e385e1d1f423b7c1ac5542e04425cbba155cdaca764c75a8dd8b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n", "mimetype": "text/plain", "start_char_idx": 6819, "end_char_idx": 6959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c360af4-c065-4692-a143-300e045c5ae3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n", "original_text": "---\n\non both synthetic and real benchmark datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "977c7ca1-2d6d-4255-9adf-d56d9c87b1ee", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The central cluster area is blue (low anomaly score), while the surrounding area is yellow (high anomaly score).  There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. ", "original_text": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n"}, "hash": "173ae52ceaab827e654f3dbe21740b31583f521ca3a72704a57c3b00e4594413", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f71a5af-d930-4c17-9cf7-2cccb371de41", "node_type": "1", "metadata": {"window": "**Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, . ", "original_text": "Conclusion are reported in Section IV.\n\n"}, "hash": "f8e71476dd1f79d11c482362de28e59a783b63a320d59d3fa39f46b50ec66299", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\non both synthetic and real benchmark datasets. ", "mimetype": "text/plain", "start_char_idx": 6959, "end_char_idx": 7011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f71a5af-d930-4c17-9cf7-2cccb371de41", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, . ", "original_text": "Conclusion are reported in Section IV.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c360af4-c065-4692-a143-300e045c5ae3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "There are distinct vertical and horizontal bands of lower anomaly scores extending from the central cluster, creating a cross-like artifact.]\n\n **Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n", "original_text": "---\n\non both synthetic and real benchmark datasets. "}, "hash": "ab2c52a18e3ba8e1d630ee81935ef8a9830ab3fdeb6b537fef2b8c28d99547d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a60b9d0-bd31-4474-8a44-9857f039fa14", "node_type": "1", "metadata": {"window": "1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  . ", "original_text": "### 2. "}, "hash": "4133f54995cf38034dbb4ffacb0f437bda488347e4082013c64bbe327a941eb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conclusion are reported in Section IV.\n\n", "mimetype": "text/plain", "start_char_idx": 7011, "end_char_idx": 7051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a60b9d0-bd31-4474-8a44-9857f039fa14", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  . ", "original_text": "### 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f71a5af-d930-4c17-9cf7-2cccb371de41", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, . ", "original_text": "Conclusion are reported in Section IV.\n\n"}, "hash": "32d6a24ca5e88a6b183a41d29a0c903a9fb14d3ecfafee4ebe741706ed1d7840", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59c4e727-2f78-4c18-ae22-240faa7c9634", "node_type": "1", "metadata": {"window": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  . ", "original_text": "Isolation Forest\n\n#### 2.1. "}, "hash": "95de828811ef2a4f43ff0d753d39e851923d4396671b040384837bdf7711eb67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2. ", "mimetype": "text/plain", "start_char_idx": 7051, "end_char_idx": 7058, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59c4e727-2f78-4c18-ae22-240faa7c9634", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  . ", "original_text": "Isolation Forest\n\n#### 2.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a60b9d0-bd31-4474-8a44-9857f039fa14", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1:** Illustration of IF problems using artificial 2D data.  Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  . ", "original_text": "### 2. "}, "hash": "9dbb21f7cd56924dd5471045674a6577b32bdb10bbbd516f88d3f9d4f912f073", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d37b5ec-02f4-4f12-8d44-0fc0afa42404", "node_type": "1", "metadata": {"window": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 . ", "original_text": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. "}, "hash": "9177e5d66e94d6b7959e23a5a4ed2d25cdabc2a53e8157fa3f106ee864ed777f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation Forest\n\n#### 2.1. ", "mimetype": "text/plain", "start_char_idx": 7058, "end_char_idx": 7086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6d37b5ec-02f4-4f12-8d44-0fc0afa42404", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 . ", "original_text": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59c4e727-2f78-4c18-ae22-240faa7c9634", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Training data are depicted in the left figure as well as the curve s(x, n) = s0 (displayed in red).  The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  . ", "original_text": "Isolation Forest\n\n#### 2.1. "}, "hash": "9ae29179912749f05fa56d13f2c1859a763b5d06b634cf133ee1193c20ed9d55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c98fd32-c525-4c6d-a6dc-9bc8e3d4b400", "node_type": "1", "metadata": {"window": "---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  . ", "original_text": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n"}, "hash": "0454f6356a1605214fc3654188ae4fbc5dfd295f56cac78b89062f976a96e0fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. ", "mimetype": "text/plain", "start_char_idx": 7086, "end_char_idx": 7252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c98fd32-c525-4c6d-a6dc-9bc8e3d4b400", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  . ", "original_text": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d37b5ec-02f4-4f12-8d44-0fc0afa42404", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The right figure shows the heat map of the anomaly score (dark blue corresponds to values next to 0 and light yellow to value close to 1).\n\n ---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 . ", "original_text": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector. "}, "hash": "d95ea48a470eedc7d2a71d1aae4486efd85a3655a959cf69750880a1dbccc890", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95eaf3b0-fa11-4178-addd-6cff4f4595d2", "node_type": "1", "metadata": {"window": "Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  . ", "original_text": "To create a random isolation tree, assume that we have n training data {x\u2081, . "}, "hash": "88285adac3542f6c68c11def205dc2e36f64f85e284cac45d032b0ef9f5c9440", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n", "mimetype": "text/plain", "start_char_idx": 7252, "end_char_idx": 7462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95eaf3b0-fa11-4178-addd-6cff4f4595d2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  . ", "original_text": "To create a random isolation tree, assume that we have n training data {x\u2081, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c98fd32-c525-4c6d-a6dc-9bc8e3d4b400", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\non both synthetic and real benchmark datasets.  Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  . ", "original_text": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n"}, "hash": "f673dd71b178ab26909e30146458cb97477b80c575a2392bf9e88b202926dae1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c560eb7a-fb16-4748-bbe1-e565c1eae0ba", "node_type": "1", "metadata": {"window": "### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48. ", "original_text": ". "}, "hash": "81c8a6af6e2a1cefb8af134a359c1fccb9950e5784c41633e7475888421ade39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To create a random isolation tree, assume that we have n training data {x\u2081, . ", "mimetype": "text/plain", "start_char_idx": 7462, "end_char_idx": 7540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c560eb7a-fb16-4748-bbe1-e565c1eae0ba", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95eaf3b0-fa11-4178-addd-6cff4f4595d2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conclusion are reported in Section IV.\n\n ### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  . ", "original_text": "To create a random isolation tree, assume that we have n training data {x\u2081, . "}, "hash": "ca96e97c578bf4d60f256677075f9ef4f66c4f0e2f65313ad494ddd08a1520a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e527e489-6f2e-4b4d-83f4-19882d19c15e", "node_type": "1", "metadata": {"window": "Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, . ", "original_text": ". "}, "hash": "dea227abbc77106e2c9554186432165f8bf258c5b6df6dd7d23bc0786b9257c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 7538, "end_char_idx": 7540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e527e489-6f2e-4b4d-83f4-19882d19c15e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, . ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c560eb7a-fb16-4748-bbe1-e565c1eae0ba", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 2.  Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48. ", "original_text": ". "}, "hash": "461eefd74c5d894f61808cceefc43782e18d2af1d997138ce0416a1f2cbb48b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e816b771-d343-4799-8457-b0c5498b6968", "node_type": "1", "metadata": {"window": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  . ", "original_text": ", xn}, where xi = [xi,1 . "}, "hash": "77bf219895c6a16c1f448fa3b10c2eb1f9a7990725491766a366930df59827c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 7540, "end_char_idx": 7542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e816b771-d343-4799-8457-b0c5498b6968", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  . ", "original_text": ", xn}, where xi = [xi,1 . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e527e489-6f2e-4b4d-83f4-19882d19c15e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Isolation Forest\n\n#### 2.1.  Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, . ", "original_text": ". "}, "hash": "4d95737d70571127f0d626abd3fa100fac06e5ec16d51270d4d37663de54b48b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f143ade-8d5b-4a33-9507-d3a0bab8c6d3", "node_type": "1", "metadata": {"window": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  . ", "original_text": ". "}, "hash": "d6d2f525cb6744ea5f2c21e2df59e003d599b87be1e89aaf91d1dac74dbd388d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", xn}, where xi = [xi,1 . ", "mimetype": "text/plain", "start_char_idx": 7544, "end_char_idx": 7570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f143ade-8d5b-4a33-9507-d3a0bab8c6d3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  . ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e816b771-d343-4799-8457-b0c5498b6968", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Original Formulation\n\nIF generates t > 0 random trees to partition the data, and computes for each tree the number of nodes required to isolate each training vector.  Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  . ", "original_text": ", xn}, where xi = [xi,1 . "}, "hash": "e7e32fbdae514cb44222af93600122f91c96aa6a19b5b2376e4318bac3fa1700", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9d3949e-edca-425a-9e20-4c6aa1fffbee", "node_type": "1", "metadata": {"window": "To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. ", "original_text": ". "}, "hash": "80ce6206a417b21a85642b36501c5f6f75a7772cfbb879f088c4e1b22421e943", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 7568, "end_char_idx": 7570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9d3949e-edca-425a-9e20-4c6aa1fffbee", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f143ade-8d5b-4a33-9507-d3a0bab8c6d3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomalies are then detected as the vectors whose average path lengths are the smallest, motivated by the fact that nominal data are more concentrated than anomalies and thus require more nodes to be isolated.\n\n To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  . ", "original_text": ". "}, "hash": "238c3d87200e87052903ea84037bf35838e674f12128e2071a6540f9a9e2767a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "813af245-e00b-4926-b6a2-dc064ce117d1", "node_type": "1", "metadata": {"window": ".  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. ", "original_text": "xi,d]\u1d40 \u2208 \u211d\u1d48. "}, "hash": "4601f4c1f0ce0dec8c01b00634fd390139ed18aa5ad06da0555025e139dd8162", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 7570, "end_char_idx": 7572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "813af245-e00b-4926-b6a2-dc064ce117d1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. ", "original_text": "xi,d]\u1d40 \u2208 \u211d\u1d48. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9d3949e-edca-425a-9e20-4c6aa1fffbee", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To create a random isolation tree, assume that we have n training data {x\u2081, .  .  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. ", "original_text": ". "}, "hash": "df9ae90e65e4b1ca09ff61698be0030ee597131c1d9fc76770789cb0782b61e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7e331f8-54af-4154-aecd-73e0e11bcb40", "node_type": "1", "metadata": {"window": ".  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. ", "original_text": "We will also use the notation X = [x\u1d40\u2081, . "}, "hash": "3067e3f2423cd3bacb179792f64944f0f640d43d1e4a2fdb855cca00352614f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "xi,d]\u1d40 \u2208 \u211d\u1d48. ", "mimetype": "text/plain", "start_char_idx": 7574, "end_char_idx": 7587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7e331f8-54af-4154-aecd-73e0e11bcb40", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. ", "original_text": "We will also use the notation X = [x\u1d40\u2081, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "813af245-e00b-4926-b6a2-dc064ce117d1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  .  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. ", "original_text": "xi,d]\u1d40 \u2208 \u211d\u1d48. "}, "hash": "96959ed220cc7ea0c2879094427e8f70497f9ce87a19012f9a665a5ca5d45d3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5483ae85-212b-4362-a446-92adb77baa86", "node_type": "1", "metadata": {"window": ", xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. ", "original_text": ". "}, "hash": "9a7943866de6e751eadb427aabe2b0d5e354c90e753734d3305f6ba49079b66e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We will also use the notation X = [x\u1d40\u2081, . ", "mimetype": "text/plain", "start_char_idx": 7587, "end_char_idx": 7629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5483ae85-212b-4362-a446-92adb77baa86", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ", xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7e331f8-54af-4154-aecd-73e0e11bcb40", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  , xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. ", "original_text": "We will also use the notation X = [x\u1d40\u2081, . "}, "hash": "9e314adc6f130966ead5363e9e85b9e8cb432631862ed852c693c13a3183c6d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4885cad0-6bd7-4db9-8cf6-00c967a0822c", "node_type": "1", "metadata": {"window": ".  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset. ", "original_text": ". "}, "hash": "32e3b449fc2bf98bb39b0eb86d0c9abf2e4c54e13fcd88298808b7795a3b107c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 7627, "end_char_idx": 7629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4885cad0-6bd7-4db9-8cf6-00c967a0822c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5483ae85-212b-4362-a446-92adb77baa86", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ", xn}, where xi = [xi,1 .  .  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. ", "original_text": ". "}, "hash": "e0e270436bcc473a7a137c7c8b27fa2e3e47dd825d0fedca35ca7ca8e809e696", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f7d6f48-36df-4552-a0d2-ca5bc9ffc714", "node_type": "1", "metadata": {"window": ".  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al. ", "original_text": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. "}, "hash": "0e3421cf7fc85b4075e9f6d70bbcf9b135a7741fb03cdbaf6fc545bbb3389708", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 7629, "end_char_idx": 7631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f7d6f48-36df-4552-a0d2-ca5bc9ffc714", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al. ", "original_text": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4885cad0-6bd7-4db9-8cf6-00c967a0822c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  .  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset. ", "original_text": ". "}, "hash": "6286adca85c2c6700237dc720c584783581601437a184d48e6a8cede1456f410", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21b3d650-337b-486e-8f86-ae32de99a607", "node_type": "1", "metadata": {"window": "xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n", "original_text": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. "}, "hash": "633a570437f06944d01f17430a752dfa510efc46d8df25376abfa5641b5e1c5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. ", "mimetype": "text/plain", "start_char_idx": 7633, "end_char_idx": 7696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21b3d650-337b-486e-8f86-ae32de99a607", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n", "original_text": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f7d6f48-36df-4552-a0d2-ca5bc9ffc714", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al. ", "original_text": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data. "}, "hash": "264791b5cbb4bb8a39a818264e897eff52cc2134f03f4a6f6317a12fc1d75fed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f399142b-f46e-4ece-a872-f1df51124aef", "node_type": "1", "metadata": {"window": "We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. ", "original_text": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. "}, "hash": "d95f39104f622d784a55c23b65fa8266ea25aab52bf580de89f1735e787082e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. ", "mimetype": "text/plain", "start_char_idx": 7696, "end_char_idx": 7912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f399142b-f46e-4ece-a872-f1df51124aef", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. ", "original_text": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21b3d650-337b-486e-8f86-ae32de99a607", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "xi,d]\u1d40 \u2208 \u211d\u1d48.  We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n", "original_text": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q]. "}, "hash": "9c33ec7e62b453ca7e4eeec8ef06411a894f828e4290e6ba3f7a99129ccb27ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e91ea52b-0861-4153-b254-0c93b47cf4f2", "node_type": "1", "metadata": {"window": ".  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). ", "original_text": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. "}, "hash": "abcb8db566abe82a1aec296f03cdc03dd9c604da0670fcafff604de54d85cc52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. ", "mimetype": "text/plain", "start_char_idx": 7912, "end_char_idx": 8093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e91ea52b-0861-4153-b254-0c93b47cf4f2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). ", "original_text": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f399142b-f46e-4ece-a872-f1df51124aef", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "We will also use the notation X = [x\u1d40\u2081, .  .  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. ", "original_text": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}. "}, "hash": "12e51127fedc116c9695ae7463bbf760d39330d0d65dfd4082e57d075cefc7a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d143bc3-f0f9-4a9c-91f5-c95c54ed2eb9", "node_type": "1", "metadata": {"window": ".  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. ", "original_text": "To create an IF, this procedure could be applied several times to the whole learning dataset. "}, "hash": "dc95a500eac6c55f7675a806fceae7a9f2d300b5518349ce65ffe38377f9e298", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. ", "mimetype": "text/plain", "start_char_idx": 8093, "end_char_idx": 8252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d143bc3-f0f9-4a9c-91f5-c95c54ed2eb9", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. ", "original_text": "To create an IF, this procedure could be applied several times to the whole learning dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e91ea52b-0861-4153-b254-0c93b47cf4f2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  .  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). ", "original_text": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached. "}, "hash": "e56ad32f8c6a82f30e9569f182f332208db0a586e940b5d0381aa22db9f1cad2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcab6028-67cc-4f4f-9c42-0e93ce5c6ab2", "node_type": "1", "metadata": {"window": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. ", "original_text": "However, authors in Liu et al. "}, "hash": "b1409f8a272799bf44d26fd206d7ca4639ad2995bcf4ef4de4a4bd4680cd8800", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To create an IF, this procedure could be applied several times to the whole learning dataset. ", "mimetype": "text/plain", "start_char_idx": 8252, "end_char_idx": 8346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bcab6028-67cc-4f4f-9c42-0e93ce5c6ab2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. ", "original_text": "However, authors in Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d143bc3-f0f9-4a9c-91f5-c95c54ed2eb9", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ".  , x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. ", "original_text": "To create an IF, this procedure could be applied several times to the whole learning dataset. "}, "hash": "1906d2ee644b1f6b1153a6e074914ed3a072cf869b848eb8beec214454b4cd37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f562fca2-3055-47e8-85b3-1bd59da73e29", "node_type": "1", "metadata": {"window": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. ", "original_text": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n"}, "hash": "0a82cab7798c22618550034721d21c219f957bb1fe7072e6b570af899f26db25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, authors in Liu et al. ", "mimetype": "text/plain", "start_char_idx": 8346, "end_char_idx": 8377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f562fca2-3055-47e8-85b3-1bd59da73e29", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. ", "original_text": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcab6028-67cc-4f4f-9c42-0e93ce5c6ab2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": ", x\u1d40n]\u1d40 \u2208 \u211d\u207f\u02e3\u1d48 for the matrix gathering all the training data.  To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. ", "original_text": "However, authors in Liu et al. "}, "hash": "85c9fa3f88d241e12074f11f6e0f65951ba786bbf418a8827c0555ad025a4569", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1a10fee-05d2-417d-a7a8-3d6376434ee0", "node_type": "1", "metadata": {"window": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. ", "original_text": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. "}, "hash": "b3809d8b9fc359dacdb2ed5c1ecb209ea8a26200a3f4061d3b4f389c7cce2486", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n", "mimetype": "text/plain", "start_char_idx": 8377, "end_char_idx": 8573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d1a10fee-05d2-417d-a7a8-3d6376434ee0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. ", "original_text": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f562fca2-3055-47e8-85b3-1bd59da73e29", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To create a random node and split the dataset into two subsets, one component of \u211d\u1d48 (denoted as q) is chosen randomly, and a split value p is sampled uniformly in the interval [min\u1d62=\u2081,...,\u2099 x\u1d62,q; max\u1d62=\u2081,...,\u2099 x\u1d62,q].  The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. ", "original_text": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n"}, "hash": "5253de08b6238f13fdb9f4cabf6628de582f21e3e70983abec8a6bb398d769c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b48fe45-aa1e-4f04-88aa-e5eae50d2613", "node_type": "1", "metadata": {"window": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. ", "original_text": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). "}, "hash": "ee26e3f4649ff36c41f9771e9798bc24d015a980cd61c2850c955c64b4ccbeba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. ", "mimetype": "text/plain", "start_char_idx": 8573, "end_char_idx": 8801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b48fe45-aa1e-4f04-88aa-e5eae50d2613", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. ", "original_text": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1a10fee-05d2-417d-a7a8-3d6376434ee0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The dataset is then split into two parts: the so-called left branch corresponding to the set {x\u1d62, x\u1d62,q \u2264 p} and the so-called right branch, corresponding to the set {x\u1d62, x\u1d62,q > p}.  The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. ", "original_text": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree. "}, "hash": "98352dc467631420205e2c386a7dccc1b7b1f23d0bb3dbf3b45df4316d0fda6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca0aa221-9068-43f0-bad9-b0d655c2279f", "node_type": "1", "metadata": {"window": "To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080. ", "original_text": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. "}, "hash": "22487d9cde9c202182c6475ee7c4603fe84f288faef6d74cc41cd8ca2503dd50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). ", "mimetype": "text/plain", "start_char_idx": 8801, "end_char_idx": 9129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca0aa221-9068-43f0-bad9-b0d655c2279f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080. ", "original_text": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b48fe45-aa1e-4f04-88aa-e5eae50d2613", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The tree is created by applying this procedure iteratively to each branch until a branch contains a unique data point, or until some depth l has been reached.  To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. ", "original_text": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant). "}, "hash": "667463893017be5613a17938bdaad87b6a63d884c1ce6c0cfcc25f0a9bb55384", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8718fb1e-d475-48b9-aa37-5517fecc5185", "node_type": "1", "metadata": {"window": "However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al. ", "original_text": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. "}, "hash": "46d6342b3ef3218c50b4765bfb20a8f150f0d3051393ffcffed023a21aa8de3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. ", "mimetype": "text/plain", "start_char_idx": 9129, "end_char_idx": 9197, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8718fb1e-d475-48b9-aa37-5517fecc5185", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al. ", "original_text": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca0aa221-9068-43f0-bad9-b0d655c2279f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To create an IF, this procedure could be applied several times to the whole learning dataset.  However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080. ", "original_text": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5. "}, "hash": "b37909a10891f06df73ed9f2303b08e1c39816d8fd3ef451984ed974683ebe45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7583e9dd-afc8-4b2e-b0e7-944f6cabb452", "node_type": "1", "metadata": {"window": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1. ", "original_text": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. "}, "hash": "d3b8246694a2b506adab74c6298bdf8de13699ed8fe54f08a19f8b07601a5bab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. ", "mimetype": "text/plain", "start_char_idx": 9197, "end_char_idx": 9289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7583e9dd-afc8-4b2e-b0e7-944f6cabb452", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1. ", "original_text": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8718fb1e-d475-48b9-aa37-5517fecc5185", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, authors in Liu et al.  (2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al. ", "original_text": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0. "}, "hash": "90cc52bf6049e9b1aee04f8b088351b49d80981bc4e61eee399b563d724ad219", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "011f006e-17ae-42c3-8e73-e029f1f2cafe", "node_type": "1", "metadata": {"window": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n", "original_text": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. "}, "hash": "2d697e037520f28e9760bdc6940569b2c9a49d47437e4a225d67e6217a012b22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. ", "mimetype": "text/plain", "start_char_idx": 9289, "end_char_idx": 9414, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "011f006e-17ae-42c3-8e73-e029f1f2cafe", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n", "original_text": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7583e9dd-afc8-4b2e-b0e7-944f6cabb452", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008) have shown that for each tree, a sub-sample of the whole dataset of size \u03c8 > 0 (chosen to \u03c8 = 256 in this letter) can be considered with similar performance and improved computation time.\n\n Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1. ", "original_text": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1. "}, "hash": "cc45c7b4029abed9a396bfc7301dc85c0537e84211005720f1cda065aaae2ecd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a6bf551-e7a7-4c7d-9a4c-696dab748aa1", "node_type": "1", "metadata": {"window": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. ", "original_text": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. "}, "hash": "56b09a27989a7b050810beb7f6b26123d45db0dc237fee0088e271696b517abc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. ", "mimetype": "text/plain", "start_char_idx": 9414, "end_char_idx": 9558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a6bf551-e7a7-4c7d-9a4c-696dab748aa1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. ", "original_text": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "011f006e-17ae-42c3-8e73-e029f1f2cafe", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Once the forest has been created by generating t random isolation trees, the expected path length h(x) to isolate a point x is computed using the mean of the path lengths required to isolate the point using each generated tree.  Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n", "original_text": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080. "}, "hash": "c1fe85445f815c49d7ca48f06ab0ac092474f30573924fb30bf6e2a659be8179", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1d015cf-145e-44a3-a0c9-10b3f15021cd", "node_type": "1", "metadata": {"window": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created. ", "original_text": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080. "}, "hash": "007bd9efae42688c0a6775c79ba993624a774d1c67839cdc5d5a3c4614435e7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. ", "mimetype": "text/plain", "start_char_idx": 9558, "end_char_idx": 9715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1d015cf-145e-44a3-a0c9-10b3f15021cd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created. ", "original_text": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a6bf551-e7a7-4c7d-9a4c-696dab748aa1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally, an anomaly score is defined as\n\ns(x) = 2^(\u2212E[h(x)]/c(\u03c8)) (1)\n\nwhere c(n) is the average value of h(x) for a dataset of size n, which can be computed as\n\nc(n) = 2H(n \u2212 1) \u2212 2(n \u2212 1)/n (2)\n\nwhere H(n) is the nth harmonic number (that can be approximated by ln(n) + \u03b3, where \u03b3 \u2248 0.577 is the Euler-Mascheroni\u2019s constant).  Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. ", "original_text": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector. "}, "hash": "45f1cd5506bb6930b7519b8f5eb3812f2f7a7d8bbc66f20201cf67cca5ae8382", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5f865ed-3129-41af-8467-8a65c5719070", "node_type": "1", "metadata": {"window": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n", "original_text": "Authors in Liu et al. "}, "hash": "2ba9501806d6cefebc9e5ad7a1fda82ca8dd5cb0cb4f45aa50a1dc1e76a2f26a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080. ", "mimetype": "text/plain", "start_char_idx": 9715, "end_char_idx": 9789, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f5f865ed-3129-41af-8467-8a65c5719070", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n", "original_text": "Authors in Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1d015cf-145e-44a3-a0c9-10b3f15021cd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus, when E[h(x)] = c(n), the anomaly score of x is s(x, n) = 0.5.  When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created. ", "original_text": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080. "}, "hash": "ccc46b6c7edce9faae8a9fff06c421907555881133bd3bd6cc9b02e939152e37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33f58f70-b773-46dc-b744-36a91ff4b34b", "node_type": "1", "metadata": {"window": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\". ", "original_text": "(2008) have proposed values for the different parameters that are summarized in Table 1. "}, "hash": "3f1ecff433996942d47e237b3b5eab7e1e5ec9a303feb92380db87fc3f768cf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Authors in Liu et al. ", "mimetype": "text/plain", "start_char_idx": 9789, "end_char_idx": 9811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33f58f70-b773-46dc-b744-36a91ff4b34b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\". ", "original_text": "(2008) have proposed values for the different parameters that are summarized in Table 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5f865ed-3129-41af-8467-8a65c5719070", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "When h(x) tends to +\u221e, i.e., when x is not an isolated point, the anomaly score tends to 0.  Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n", "original_text": "Authors in Liu et al. "}, "hash": "e1917c870dea487c9dff2a4ae820039cdde61d23e13dfea46f2c61a2e97420c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d777a26-884c-4d48-b982-8ecc8ef10748", "node_type": "1", "metadata": {"window": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points. ", "original_text": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n"}, "hash": "bbacab9fe7be6f97a7ff3aef431bffed792546383fb85e2a90104c90079fdc19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2008) have proposed values for the different parameters that are summarized in Table 1. ", "mimetype": "text/plain", "start_char_idx": 9811, "end_char_idx": 9900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d777a26-884c-4d48-b982-8ecc8ef10748", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points. ", "original_text": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33f58f70-b773-46dc-b744-36a91ff4b34b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally, when h(x) is small compared to c(n), i.e., when x is an isolated point, the corresponding anomaly score tends to 1.  Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\". ", "original_text": "(2008) have proposed values for the different parameters that are summarized in Table 1. "}, "hash": "d6361fba78a1cd2ceec0aa7e5cdb23a8d14f7aa8830bd29edc356c02f83e81f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8fdeb68-46d0-42a2-bbe4-82100684aff3", "node_type": "1", "metadata": {"window": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\". ", "original_text": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. "}, "hash": "b85df90009b75e90944b86dcbf72326deff981fea341641a53f9539ba463ef58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n", "mimetype": "text/plain", "start_char_idx": 9900, "end_char_idx": 9981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8fdeb68-46d0-42a2-bbe4-82100684aff3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\". ", "original_text": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d777a26-884c-4d48-b982-8ecc8ef10748", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus we can define an anomaly threshold s\u2080 \u2208 [0, 1] such that x is detected as an anomaly when s(x) > s\u2080, and as a nominal data when s(x) \u2264 s\u2080.  Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points. ", "original_text": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n"}, "hash": "f643d696f1f21e1c33edd93f25979eb7cb42326cbc829e0598d8c1015fc55630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c03de7f1-1c09-4f67-be69-1a1f7c3c12a7", "node_type": "1", "metadata": {"window": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn. ", "original_text": "However, this algorithm suffers from a bias due to the way trees are created. "}, "hash": "2fd8a7eaab8ff26bb6e788b026d7bfdbcebac26e780fe45afea4d314b0766bd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. ", "mimetype": "text/plain", "start_char_idx": 9981, "end_char_idx": 10333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c03de7f1-1c09-4f67-be69-1a1f7c3c12a7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn. ", "original_text": "However, this algorithm suffers from a bias due to the way trees are created. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8fdeb68-46d0-42a2-bbe4-82100684aff3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Of course, the closer the anomaly score to 1, the more likely x is an anomaly, and the closer the anomaly score to 0, the more likely x is a nominal vector.  Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\". ", "original_text": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient. "}, "hash": "4cd8eec00d9a64ca3f13b68db5ef1c59dbc052c21d94cbc4a9aa99f973edb993", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e344ed37-59a8-4a7f-ad32-65ed3779485c", "node_type": "1", "metadata": {"window": "Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n", "original_text": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n"}, "hash": "afbc57ff4fd959cfd9ed1a5c18f948535f5867f9ba5ecb948e9e14f30b937e3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this algorithm suffers from a bias due to the way trees are created. ", "mimetype": "text/plain", "start_char_idx": 10333, "end_char_idx": 10411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e344ed37-59a8-4a7f-ad32-65ed3779485c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n", "original_text": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c03de7f1-1c09-4f67-be69-1a1f7c3c12a7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus, a trade-off has to be made to determine an appropriate value of s\u2080.  Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn. ", "original_text": "However, this algorithm suffers from a bias due to the way trees are created. "}, "hash": "1382353f85dfaf728b39a7b3a905a1e958dfa7a1fee02f7d68736f9b289f2b19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80fc2173-78b1-4667-be35-b35d8ac8e817", "node_type": "1", "metadata": {"window": "(2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig. ", "original_text": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\". "}, "hash": "14539d17056cabef1944e7837f53c00eaa4aaa8e7b2bd537ec90f513d04af81e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n", "mimetype": "text/plain", "start_char_idx": 10411, "end_char_idx": 10701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80fc2173-78b1-4667-be35-b35d8ac8e817", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig. ", "original_text": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e344ed37-59a8-4a7f-ad32-65ed3779485c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Authors in Liu et al.  (2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n", "original_text": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n"}, "hash": "c272e3fcd9e34495e7d181df945b30d77ce86aef0b42d63d8ef548978aaccf38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da2adcdb-18e4-4c2e-94ac-09a53a04c5e3", "node_type": "1", "metadata": {"window": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data. ", "original_text": "It's a 2D scatter plot with a few data points. "}, "hash": "2134651f23fc37e98ca48a6a16a7b3492b0ecce8c4d8d504b243f2a6ec3fe395", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\". ", "mimetype": "text/plain", "start_char_idx": 10701, "end_char_idx": 10764, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da2adcdb-18e4-4c2e-94ac-09a53a04c5e3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data. ", "original_text": "It's a 2D scatter plot with a few data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80fc2173-78b1-4667-be35-b35d8ac8e817", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008) have proposed values for the different parameters that are summarized in Table 1.  The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig. ", "original_text": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\". "}, "hash": "74bec66c7475d4da0178fa454d3cfe7ad3219422854df69429738780d7b6091f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e005e1dd-f877-44bd-84df-b7e192b01d8d", "node_type": "1", "metadata": {"window": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. ", "original_text": "A large rectangle represents the \"Sampling area\". "}, "hash": "715078a2da8e0578dea730cc10d717e39d3383751c6e7e74962c79053486fc5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's a 2D scatter plot with a few data points. ", "mimetype": "text/plain", "start_char_idx": 10764, "end_char_idx": 10811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e005e1dd-f877-44bd-84df-b7e192b01d8d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. ", "original_text": "A large rectangle represents the \"Sampling area\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da2adcdb-18e4-4c2e-94ac-09a53a04c5e3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The resulting IF\n\n**Table 1:** Proposed values for the various parameters of IF.\n | Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data. ", "original_text": "It's a 2D scatter plot with a few data points. "}, "hash": "666d0e4a0a8653a353bd28e09f01984833bcc24277d297cbe52dbd103c31fc9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bff60012-f728-4c71-88ed-67925a427a1d", "node_type": "1", "metadata": {"window": "However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). ", "original_text": "A line labeled \"Splitting hyperplane\" is drawn. "}, "hash": "97988c027f8eed1f2eb8bc8ac96548de3ff61d08298802fa7524cd2608b280d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A large rectangle represents the \"Sampling area\". ", "mimetype": "text/plain", "start_char_idx": 10811, "end_char_idx": 10861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bff60012-f728-4c71-88ed-67925a427a1d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). ", "original_text": "A line labeled \"Splitting hyperplane\" is drawn. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e005e1dd-f877-44bd-84df-b7e192b01d8d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "| Parameters | Meaning | Proposed value |\n| :--- | :--- | :--- |\n| t | Number of trees | 100 |\n| \u03c8 | Sub-sample size | 256 |\n| l | Tree maximum depth | ceil(log\u2082 \u03c8) = 8 |\n| s\u2080 | Anomaly detection threshold | 0.6 |\n\nalgorithm is a convenient solution to detect anomalies without assumptions on the data distribution and it is computationally efficient.  However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. ", "original_text": "A large rectangle represents the \"Sampling area\". "}, "hash": "34efe7dc6e48fc76a5715c07211b3532a72a43e7f31c7636c26c63a518d9fe42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edf5b30c-af72-40b0-ad04-8d823b43afa9", "node_type": "1", "metadata": {"window": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n", "original_text": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n"}, "hash": "ded90579f6deb13ca4560f723ad81fbdf690944e71a78a88e6c619464e8f3870", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A line labeled \"Splitting hyperplane\" is drawn. ", "mimetype": "text/plain", "start_char_idx": 10861, "end_char_idx": 10909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "edf5b30c-af72-40b0-ad04-8d823b43afa9", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n", "original_text": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bff60012-f728-4c71-88ed-67925a427a1d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, this algorithm suffers from a bias due to the way trees are created.  Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). ", "original_text": "A line labeled \"Splitting hyperplane\" is drawn. "}, "hash": "a6f8081864cc013e6fdb7776ae955eb342f5c7238c765b3a9bd9f2c20a3d3117", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c172e64d-214f-460e-ba31-4778f4b3d0cc", "node_type": "1", "metadata": {"window": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\". ", "original_text": "**Fig. "}, "hash": "4c549786671d8738c05723b26dff9b6739fea814fca149f1d54ea93708f14244", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n", "mimetype": "text/plain", "start_char_idx": 10909, "end_char_idx": 11006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c172e64d-214f-460e-ba31-4778f4b3d0cc", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\". ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edf5b30c-af72-40b0-ad04-8d823b43afa9", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, by randomly choosing one dimension to split the data, parallel hyperplanes are used (with a normal vector collinear to the selected dimension), and data spread around stripes parallel to the axis and passing through the cluster have a lower anomaly score, as depicted in Figure 1.\n\n ---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n", "original_text": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n"}, "hash": "00fd64dcd7990d0dfb677dd1f08b382ebc500b7f5a6276eccbc98c44d892f163", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad26bda4-a47c-4644-bb8f-8b1ec109a96d", "node_type": "1", "metadata": {"window": "It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points. ", "original_text": "2:** Illustration of an EIF drawback using artificial 2D data. "}, "hash": "098c4fbd5357bffc12cd2dca5243d90b363b713c42602948b685cd4dd0d6f14e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 11006, "end_char_idx": 11013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad26bda4-a47c-4644-bb8f-8b1ec109a96d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points. ", "original_text": "2:** Illustration of an EIF drawback using artificial 2D data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c172e64d-214f-460e-ba31-4778f4b3d0cc", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n[Image: A chart titled \"Illustration of one step in EIF\".  It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\". ", "original_text": "**Fig. "}, "hash": "ad415f8071dcf6b9c11da5572cfaa16658b982a252b292e5b387c6ac1951e021", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adf495e2-a546-492e-af6d-1890a0ca78c2", "node_type": "1", "metadata": {"window": "A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. ", "original_text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. "}, "hash": "4db50ba85d173a61a1be863aeadef69e70a622bb0390206eaf1893901094c7ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2:** Illustration of an EIF drawback using artificial 2D data. ", "mimetype": "text/plain", "start_char_idx": 11013, "end_char_idx": 11076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "adf495e2-a546-492e-af6d-1890a0ca78c2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. ", "original_text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad26bda4-a47c-4644-bb8f-8b1ec109a96d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It's a 2D scatter plot with a few data points.  A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points. ", "original_text": "2:** Illustration of an EIF drawback using artificial 2D data. "}, "hash": "fb3f31d5c51e7dd8e23a8af4552606001b18eff547db020ebdcf31c17d2d254f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61f495ac-6ab3-4707-a95d-abca2a68fc4b", "node_type": "1", "metadata": {"window": "A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n", "original_text": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). "}, "hash": "4a4f44710995e12a12ee78987b5654f3d66ec126efe1bae5b809e540de49954f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. ", "mimetype": "text/plain", "start_char_idx": 11076, "end_char_idx": 11188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61f495ac-6ab3-4707-a95d-abca2a68fc4b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n", "original_text": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adf495e2-a546-492e-af6d-1890a0ca78c2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A large rectangle represents the \"Sampling area\".  A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. ", "original_text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area. "}, "hash": "446360cf21be48c8bb6d41d8be837cbb7ee40e4947277502c99a6ceb567c9b4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0896a22-7707-4e55-8f26-931e1b91b511", "node_type": "1", "metadata": {"window": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig. ", "original_text": "Thus the corresponding right branch of the tree will be empty.\n\n"}, "hash": "d3b1495a18f81880a3a2759e8267a33279b8f3e44ce6fe2bdcdb53b0291fba90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). ", "mimetype": "text/plain", "start_char_idx": 11188, "end_char_idx": 11290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0896a22-7707-4e55-8f26-931e1b91b511", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig. ", "original_text": "Thus the corresponding right branch of the tree will be empty.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61f495ac-6ab3-4707-a95d-abca2a68fc4b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A line labeled \"Splitting hyperplane\" is drawn.  All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n", "original_text": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome). "}, "hash": "2080a23c8a7f56341382efee7e17045e9b393ca011583bca757db222f471af06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58e7f14a-ecc3-4d81-b8b1-44b7378304db", "node_type": "1", "metadata": {"window": "**Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach. ", "original_text": "[Image: A chart titled \"Illustration of one step in proposed EIF\". "}, "hash": "8eabc428a58e28e2fee98dff59e0fe2a77aee04f55cfa203e25cca87cb32b976", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus the corresponding right branch of the tree will be empty.\n\n", "mimetype": "text/plain", "start_char_idx": 11290, "end_char_idx": 11354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58e7f14a-ecc3-4d81-b8b1-44b7378304db", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach. ", "original_text": "[Image: A chart titled \"Illustration of one step in proposed EIF\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0896a22-7707-4e55-8f26-931e1b91b511", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "All data points lie on one side of this line, illustrating how EIF can create an empty branch.]\n\n **Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig. ", "original_text": "Thus the corresponding right branch of the tree will be empty.\n\n"}, "hash": "6e64d6b16a749dea40dc9a0251f665cbd5fe05fd452b7e564820284812d77576", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a964d65-dd43-437a-92cf-73529c17df4c", "node_type": "1", "metadata": {"window": "2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). ", "original_text": "It's a 2D scatter plot with the same data points. "}, "hash": "1a9ae5daa44a49fbe907f1483107761786de611c81420ab3c68d0765ef348273", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Image: A chart titled \"Illustration of one step in proposed EIF\". ", "mimetype": "text/plain", "start_char_idx": 11354, "end_char_idx": 11421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a964d65-dd43-437a-92cf-73529c17df4c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). ", "original_text": "It's a 2D scatter plot with the same data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58e7f14a-ecc3-4d81-b8b1-44b7378304db", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach. ", "original_text": "[Image: A chart titled \"Illustration of one step in proposed EIF\". "}, "hash": "0794abb2fa9c10e27b64382456fd2d3d80bb35cee35124cd14a61d0f92969da3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fe20087-a862-49b6-a5a8-8bd1a8baf1f5", "node_type": "1", "metadata": {"window": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n", "original_text": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. "}, "hash": "db05699ebf0a9a18f1126990e0084a7114ca7861341466cd62d3e8d9bd661a3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's a 2D scatter plot with the same data points. ", "mimetype": "text/plain", "start_char_idx": 11421, "end_char_idx": 11471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fe20087-a862-49b6-a5a8-8bd1a8baf1f5", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n", "original_text": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a964d65-dd43-437a-92cf-73529c17df4c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "2:** Illustration of an EIF drawback using artificial 2D data.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). ", "original_text": "It's a 2D scatter plot with the same data points. "}, "hash": "9b9e327fa0c2497f9dc6ae0139c0ac25178e8dfd62cecf82345106a6aa3c92e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "140e01de-3469-41be-af21-972961c4151f", "node_type": "1", "metadata": {"window": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2. ", "original_text": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n"}, "hash": "596ef6aae1bb6c484d783b5206726fbb58010a123a5256e63c7f95f213d16ae9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. ", "mimetype": "text/plain", "start_char_idx": 11471, "end_char_idx": 11621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "140e01de-3469-41be-af21-972961c4151f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2. ", "original_text": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fe20087-a862-49b6-a5a8-8bd1a8baf1f5", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area.  As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n", "original_text": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line. "}, "hash": "b54e34a8cf53f88352c2f28f2b0fab46973b6a2a5fae5a7021fa6a6c18e33122", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "436c85d6-b397-40a4-95cb-89e4a1c3b7ed", "node_type": "1", "metadata": {"window": "Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. ", "original_text": "**Fig. "}, "hash": "0ad7f03f21fb6841e415a4dec52e4da5d2133ebc7b5bf86d62de8b6e28bc6a5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n", "mimetype": "text/plain", "start_char_idx": 11621, "end_char_idx": 11742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "436c85d6-b397-40a4-95cb-89e4a1c3b7ed", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "140e01de-3469-41be-af21-972961c4151f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, using this strategy, all the data points are below the hyperplane (for this outcome).  Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2. ", "original_text": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n"}, "hash": "c46c18c295ae8970a09bef087de31bbe72628ea7cc87317ed3b67b3f76955475", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42851474-aa88-4e27-83ca-30b42d4a5034", "node_type": "1", "metadata": {"window": "[Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al. ", "original_text": "3:** Illustration of the proposed GIF approach. "}, "hash": "18571882e9ac09341532b995b5c3b1e4051e3fe9f3a1d4019c863b2ccf75e206", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 11742, "end_char_idx": 11749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42851474-aa88-4e27-83ca-30b42d4a5034", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al. ", "original_text": "3:** Illustration of the proposed GIF approach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "436c85d6-b397-40a4-95cb-89e4a1c3b7ed", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Thus the corresponding right branch of the tree will be empty.\n\n [Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. ", "original_text": "**Fig. "}, "hash": "fae85ceb8a04edb2c4299c41e28e02428388ca3681fbe7ccddcea73c7aa94674", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec33676f-ce92-4d0d-9851-aae6f504c857", "node_type": "1", "metadata": {"window": "It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF. ", "original_text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). "}, "hash": "6779fec247e29c76a4caa1e3dab19d61ce16292570829b8cdb44eda3e9369631", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3:** Illustration of the proposed GIF approach. ", "mimetype": "text/plain", "start_char_idx": 11749, "end_char_idx": 11797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ec33676f-ce92-4d0d-9851-aae6f504c857", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF. ", "original_text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42851474-aa88-4e27-83ca-30b42d4a5034", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A chart titled \"Illustration of one step in proposed EIF\".  It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al. ", "original_text": "3:** Illustration of the proposed GIF approach. "}, "hash": "5d293a0bbd0737ea9fbb8172f49fc99f6cf8d6dbad890dcb4e88fd1a99231f34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "583e7cfd-5204-431c-bab7-e99f7523e16c", "node_type": "1", "metadata": {"window": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al. ", "original_text": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n"}, "hash": "3d7c7abbfbf9f90fe91d6a58ef15283f7516665d206c32d8a332b3bcab379aa5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). ", "mimetype": "text/plain", "start_char_idx": 11797, "end_char_idx": 11953, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "583e7cfd-5204-431c-bab7-e99f7523e16c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al. ", "original_text": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec33676f-ce92-4d0d-9851-aae6f504c857", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "It's a 2D scatter plot with the same data points.  A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF. ", "original_text": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square). "}, "hash": "65be992134388b7d057e0042bb142b74957063c343fd30066c9655e9fa11c6ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f00635f-68d0-4be0-a92c-68fc5fd205f3", "node_type": "1", "metadata": {"window": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. ", "original_text": "---\n\n#### 2.2. "}, "hash": "78a3d09d2018a29f4adbd874fc6ece7197e253e512e554512d5bf4fb53fea14b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n", "mimetype": "text/plain", "start_char_idx": 11953, "end_char_idx": 12050, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f00635f-68d0-4be0-a92c-68fc5fd205f3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. ", "original_text": "---\n\n#### 2.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "583e7cfd-5204-431c-bab7-e99f7523e16c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A line labeled \"Splitting hyperplane\" is drawn, but this time it passes through the data points, ensuring that points fall on both sides of the line.  The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al. ", "original_text": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n"}, "hash": "c3cdfe1ad8177bc7625084042b1a71054a466603d08cd66086da3db864a10723", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e19f3fb2-e44a-4c6a-ba8e-0faa6ebc2da1", "node_type": "1", "metadata": {"window": "**Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). ", "original_text": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. "}, "hash": "a06133054ebf16cd980ba9f11909b24925ecc1357e89b5ea3c001d1fff14fe23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n#### 2.2. ", "mimetype": "text/plain", "start_char_idx": 12050, "end_char_idx": 12065, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e19f3fb2-e44a-4c6a-ba8e-0faa6ebc2da1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). ", "original_text": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f00635f-68d0-4be0-a92c-68fc5fd205f3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The \"Sampling area\" is now a line segment between the projections of the outermost data points onto the line's normal.]\n\n **Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. ", "original_text": "---\n\n#### 2.2. "}, "hash": "c665b3acf2d56f533bb6cc88314de678c894cd7b1caa7a6176ac58f64468694a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3191dbf0-35d1-4088-aee2-e90523211e5e", "node_type": "1", "metadata": {"window": "3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. ", "original_text": "1, an improved solution was presented in Hariri et al. "}, "hash": "5c6f978066373273afdc4ef6d7f90ae0d5d42adbdc001a7f613dd49f0d4b851c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. ", "mimetype": "text/plain", "start_char_idx": 12065, "end_char_idx": 12131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3191dbf0-35d1-4088-aee2-e90523211e5e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. ", "original_text": "1, an improved solution was presented in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e19f3fb2-e44a-4c6a-ba8e-0faa6ebc2da1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). ", "original_text": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig. "}, "hash": "05d1abe9ce764a70eb0211bdf76eabf04298d0021d00301cb5153170130f5e6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eafebfb4-4073-4af1-b28c-55ae15be4981", "node_type": "1", "metadata": {"window": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). ", "original_text": "(2019) referred to as EIF. "}, "hash": "6e5d858175ebe7dce833251562ced493090fd8fd5a218cdee478088dc4ba9469", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, an improved solution was presented in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 12131, "end_char_idx": 12186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eafebfb4-4073-4af1-b28c-55ae15be4981", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). ", "original_text": "(2019) referred to as EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3191dbf0-35d1-4088-aee2-e90523211e5e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "3:** Illustration of the proposed GIF approach.  A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. ", "original_text": "1, an improved solution was presented in Hariri et al. "}, "hash": "7a260ff790ee150d571f23317af88affc0222ef4d0d092c3ca09d9d5b119950d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dadd6b91-6658-45c0-bdfb-4b31c3cb29a6", "node_type": "1", "metadata": {"window": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. ", "original_text": "As explained in Hariri et al. "}, "hash": "92439c8b7150c7d7157821217f831f6e55e0fcae248a6c9019606b016821444e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) referred to as EIF. ", "mimetype": "text/plain", "start_char_idx": 12186, "end_char_idx": 12213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dadd6b91-6658-45c0-bdfb-4b31c3cb29a6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. ", "original_text": "As explained in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eafebfb4-4073-4af1-b28c-55ae15be4981", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A splitting hyperplane is created by sampling a random unit vector and a random intercept in the sampling area, which reduces to a line (and not a square).  This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). ", "original_text": "(2019) referred to as EIF. "}, "hash": "404076292362317f26119bae677e607691655359c35225b037b4495f80f0ca93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38464448-f8cd-4470-b9b4-62a20f2f82a6", "node_type": "1", "metadata": {"window": "---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2). ", "original_text": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. "}, "hash": "7c85d548f89d91249ed2ef645458f77c3e4bb05e54997ea3cb5da5d7e953c6c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As explained in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 12213, "end_char_idx": 12243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38464448-f8cd-4470-b9b4-62a20f2f82a6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2). ", "original_text": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dadd6b91-6658-45c0-bdfb-4b31c3cb29a6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This strategy has the advantage of having data points on each side of the splitting hyperplane.\n\n ---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. ", "original_text": "As explained in Hariri et al. "}, "hash": "85f2453b562076297d5ca70a3d9295e2dca09237198d923bafa01926ecb995e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aebbdc00-4f92-4319-b5dc-21106916e457", "node_type": "1", "metadata": {"window": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). ", "original_text": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). "}, "hash": "60a163a9150bfee9ea17178b78def94cb05beb39da32ec77fb06ebe6e6ae8632", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. ", "mimetype": "text/plain", "start_char_idx": 12243, "end_char_idx": 12340, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aebbdc00-4f92-4319-b5dc-21106916e457", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). ", "original_text": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38464448-f8cd-4470-b9b4-62a20f2f82a6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n#### 2.2.  Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2). ", "original_text": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data. "}, "hash": "b638704ef6cc982c916f98fa6ca05f42d4d69fdfe28f20b367d84a877599370e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c7c4a02-4187-40fa-85e7-60344ab8f635", "node_type": "1", "metadata": {"window": "1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). ", "original_text": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. "}, "hash": "45eaae2f672a694a5c20a510e8e4c9e27ebec847a9780a1d3f99824c6a245c50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). ", "mimetype": "text/plain", "start_char_idx": 12340, "end_char_idx": 12563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c7c4a02-4187-40fa-85e7-60344ab8f635", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). ", "original_text": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aebbdc00-4f92-4319-b5dc-21106916e457", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Extended IF\n\nTo avoid artefacts such as those illustrated in Fig.  1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). ", "original_text": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x). "}, "hash": "1ad4641faef8147643abb0a3a674edd904fb3ca9bc550c64c268c65816117b37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b7d699c-a32b-4f58-923d-77767c426d87", "node_type": "1", "metadata": {"window": "(2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig. ", "original_text": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). "}, "hash": "c66cda2968bf00e27497943606c60655d21f6da27a8e67f3efb83462f4d7f3fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 12563, "end_char_idx": 12707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b7d699c-a32b-4f58-923d-77767c426d87", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig. ", "original_text": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c7c4a02-4187-40fa-85e7-60344ab8f635", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1, an improved solution was presented in Hariri et al.  (2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). ", "original_text": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al. "}, "hash": "7e37481e843219abd29c2b863e3d2b82d6f97fdf5cf59e12ff37c87d9550ba72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11dad163-3f4e-4aa8-b4ad-d55683997aaf", "node_type": "1", "metadata": {"window": "As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n", "original_text": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. "}, "hash": "72cc2096802a0ed414327e214c107204e5c9faba01b7279491752a8e108a4433", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). ", "mimetype": "text/plain", "start_char_idx": 12707, "end_char_idx": 12838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "11dad163-3f4e-4aa8-b4ad-d55683997aaf", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n", "original_text": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b7d699c-a32b-4f58-923d-77767c426d87", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) referred to as EIF.  As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig. ", "original_text": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)). "}, "hash": "98411ccc518e564f2a5cb2278096436d439d222f6608428026eeb5ea98208d09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe52ab2e-e838-4791-8bf4-319a0a346ed6", "node_type": "1", "metadata": {"window": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n", "original_text": "2). "}, "hash": "621ade9e8cc427c8659cbd1031374777839ca9e4a1dbb6daf39501f97bb93d50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. ", "mimetype": "text/plain", "start_char_idx": 12838, "end_char_idx": 13023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe52ab2e-e838-4791-8bf4-319a0a346ed6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n", "original_text": "2). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11dad163-3f4e-4aa8-b4ad-d55683997aaf", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As explained in Hariri et al.  (2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n", "original_text": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig. "}, "hash": "1b8b724c1c92a348cbde02cafb7945a52c0b99f65a1622708194e0135f7e0377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e033461a-0ab2-436e-bdd2-f004984d93a0", "node_type": "1", "metadata": {"window": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3. ", "original_text": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). "}, "hash": "a9cffee8dc2f8b003fa19399e41151bdfc68407267ca56076d4d712979c71a03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2). ", "mimetype": "text/plain", "start_char_idx": 13023, "end_char_idx": 13027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e033461a-0ab2-436e-bdd2-f004984d93a0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3. ", "original_text": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe52ab2e-e838-4791-8bf4-319a0a346ed6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019), the main drawback of IF is due to the way hyperplanes are constructed to split the data.  Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n", "original_text": "2). "}, "hash": "bdadeac03bb8e119ca77a4c871325bb49dee7a53ae42f218ee5249f02e972150", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fed9c3c1-5ac7-4842-b4d2-7974ef78b150", "node_type": "1", "metadata": {"window": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. ", "original_text": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). "}, "hash": "a6373d2b6be5bf4dc16922c4bc296c5d3063a840dc0547cb1fb506b771ebf7c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). ", "mimetype": "text/plain", "start_char_idx": 13027, "end_char_idx": 13174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fed9c3c1-5ac7-4842-b4d2-7974ef78b150", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. ", "original_text": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e033461a-0ab2-436e-bdd2-f004984d93a0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, since the drawn normal vectors are chosen according to each dimension of \u211d\u1d48, a discrete set of orthogonal directions is generated, which is at the origin of these vertical lines appearing in the level sets of s(x).  To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3. ", "original_text": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree). "}, "hash": "3dc1887dab29f334a3ec08202cf6b1e75f3d72d82dac325204d379430d807962", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b90cee45-b63e-448a-813b-85207496533d", "node_type": "1", "metadata": {"window": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. ", "original_text": "This situation is depicted in Fig. "}, "hash": "d14667d10b1f934134e18cb3ff0517665c3625ee8f3daa44d62c866c33b720a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). ", "mimetype": "text/plain", "start_char_idx": 13174, "end_char_idx": 13549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b90cee45-b63e-448a-813b-85207496533d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. ", "original_text": "This situation is depicted in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fed9c3c1-5ac7-4842-b4d2-7974ef78b150", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To mitigate this problem, a normal vector w can be sampled for each decision hyperplane randomly chosen in the unit sphere of \u211d\u1d48 (Hariri et al.  (2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. ", "original_text": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data). "}, "hash": "17fe7f068bbffdac33871a8ce8e09c84b0eb5c545ffaae290a311724a9fc30f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "049d3c19-18fe-4e60-bf3c-51bdcb5ed390", "node_type": "1", "metadata": {"window": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values. ", "original_text": "2 for the previous 2D example.\n\n"}, "hash": "eeaad2eb660a5722579302e9353b383ddf2f035e41b1044565010b2cd55065a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This situation is depicted in Fig. ", "mimetype": "text/plain", "start_char_idx": 13549, "end_char_idx": 13584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "049d3c19-18fe-4e60-bf3c-51bdcb5ed390", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values. ", "original_text": "2 for the previous 2D example.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b90cee45-b63e-448a-813b-85207496533d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)), i.e., a Gaussian vector is sampled according to u ~ N(0, I\u1d48) \u2208 \u211d\u1d48 and normalized leading to w = u/||u||\u2082 (Muller (1959)).  To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. ", "original_text": "This situation is depicted in Fig. "}, "hash": "40d73b6c8c4bb5aacd8b1805bb017d3c47c634c73801e9d9406a036468a525ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff0a2e33-8a36-4d9b-a3e9-62ed4937ee5d", "node_type": "1", "metadata": {"window": "2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. ", "original_text": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n"}, "hash": "4f2d41e6e25778850d314c83bf48facd8655099129c41fba324a1a64c824ddc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 for the previous 2D example.\n\n", "mimetype": "text/plain", "start_char_idx": 13584, "end_char_idx": 13616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff0a2e33-8a36-4d9b-a3e9-62ed4937ee5d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. ", "original_text": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "049d3c19-18fe-4e60-bf3c-51bdcb5ed390", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To select the split value, an intercept vector p \u2208 \u211d\u1d48 is sampled uniformly in the smallest axis-bouding hypercube enclosing all the samples at a branching point (as illustrated in Fig.  2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values. ", "original_text": "2 for the previous 2D example.\n\n"}, "hash": "31f0685d084e43a03b121b38fb2bd88ba2736934b8e0e200378e79170b781794", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b15301a-2d9a-4194-a63e-efa070e5fef1", "node_type": "1", "metadata": {"window": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. ", "original_text": "#### 2.3. "}, "hash": "13ef4c12e2d39e9f3c857af020757f63922f4aa9d99a425423047ccdb7344ef3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n", "mimetype": "text/plain", "start_char_idx": 13616, "end_char_idx": 13784, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b15301a-2d9a-4194-a63e-efa070e5fef1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. ", "original_text": "#### 2.3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff0a2e33-8a36-4d9b-a3e9-62ed4937ee5d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "2).  The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. ", "original_text": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n"}, "hash": "ac5c4fdc3b36db057b8f85b64325dbd0c529d06a14fbbf241831a9d9701b5ee1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "489cfaba-0d04-4639-b5e6-91c25c53f0fd", "node_type": "1", "metadata": {"window": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3. ", "original_text": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. "}, "hash": "1012eb3a181f00c6659cc8e8bb79ada173cea38f0aa135ea21e9b3eca7e05f47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 2.3. ", "mimetype": "text/plain", "start_char_idx": 13784, "end_char_idx": 13794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "489cfaba-0d04-4639-b5e6-91c25c53f0fd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3. ", "original_text": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b15301a-2d9a-4194-a63e-efa070e5fef1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The two branches of the tree are defined depending on whether (x \u2212 p)\u1d40w > 0 (right branch of the tree) or (x \u2212 p)\u1d40w \u2264 0 (left branch of the tree).  The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. ", "original_text": "#### 2.3. "}, "hash": "5933ab395b6d2279b233fb4f233012fd864bef62718a026738853a5561a3ed82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84624d2e-491f-4c61-a32a-740aee966100", "node_type": "1", "metadata": {"window": "This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF. ", "original_text": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. "}, "hash": "42f1e4487a999cc8cc8a2801f55bad2dd14ccade624f9096cd9a264ff284377f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. ", "mimetype": "text/plain", "start_char_idx": 13794, "end_char_idx": 13929, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84624d2e-491f-4c61-a32a-740aee966100", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF. ", "original_text": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "489cfaba-0d04-4639-b5e6-91c25c53f0fd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The hyperplane is thus the one defined by the normal vector w and containing the intercept point p. One drawback of this method is that it can lead to empty branches in the tree, which goes against the idea of IF (whose idea is to split the tree until the number of points equals one or until a given maximal depth has been reached in order to efficiently isolate the data).  This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3. ", "original_text": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF. "}, "hash": "baca8f6ccd704f1e6233cf9282dc233fe14c253d8f57d30ad6e9866ed82a2dca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a2fd3b1-e416-4d6f-9914-af52f15a8b09", "node_type": "1", "metadata": {"window": "2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. ", "original_text": "3) and sample a split value uniformly between these two values. "}, "hash": "895495f113d02e077b3d1574c51798a5907d442fcd0c791d593451a3bc7d543b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. ", "mimetype": "text/plain", "start_char_idx": 13929, "end_char_idx": 14114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1a2fd3b1-e416-4d6f-9914-af52f15a8b09", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. ", "original_text": "3) and sample a split value uniformly between these two values. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84624d2e-491f-4c61-a32a-740aee966100", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This situation is depicted in Fig.  2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF. ", "original_text": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig. "}, "hash": "a0c29fd07d6cd776eaa65d71dd9f96264ad08f46b5698a2ebdb1f7962a62f1b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2f41c94-0ac2-46eb-b47c-d2260a3f7456", "node_type": "1", "metadata": {"window": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. ", "original_text": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. "}, "hash": "fc0e0752b06e83e85371fe27b1b8918598b20846acdd5c5ace44ed32990758c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3) and sample a split value uniformly between these two values. ", "mimetype": "text/plain", "start_char_idx": 14114, "end_char_idx": 14178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2f41c94-0ac2-46eb-b47c-d2260a3f7456", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. ", "original_text": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a2fd3b1-e416-4d6f-9914-af52f15a8b09", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "2 for the previous 2D example.\n\n This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. ", "original_text": "3) and sample a split value uniformly between these two values. "}, "hash": "75862e53323102ac84b2986646eefa03adbbf98b8ddca63f4bc3841f338ad772", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77e49d1a-461a-4286-8775-79951854e268", "node_type": "1", "metadata": {"window": "#### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. ", "original_text": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. "}, "hash": "b6764bc43233eb3004775d13c24a672d0cf800436e7ca79710e6706e96f199fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. ", "mimetype": "text/plain", "start_char_idx": 14178, "end_char_idx": 14371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77e49d1a-461a-4286-8775-79951854e268", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "#### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. ", "original_text": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2f41c94-0ac2-46eb-b47c-d2260a3f7456", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This letter studies a variation of EIF avoiding empty branches in each tree, referred to as generalized isolation forest (GIF), which is detailed in the next section.\n\n #### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. ", "original_text": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value. "}, "hash": "caf639920f74edc22ac9ff317fa5af5096aa62975aedb3165205de0efc1cb6bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eab1de2e-c0cd-48fc-b272-23c7955b6a12", "node_type": "1", "metadata": {"window": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF. ", "original_text": "3. "}, "hash": "af1451fe2723e1724400bc23b423b54b0135e4f17dc47af43cb87b4a40d2ec3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 14371, "end_char_idx": 14579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eab1de2e-c0cd-48fc-b272-23c7955b6a12", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF. ", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77e49d1a-461a-4286-8775-79951854e268", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "#### 2.3.  Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. ", "original_text": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig. "}, "hash": "812ea0e0ec304b8585ed4546d843b6c82f043ee5ec9b36bfbf800d53aee173c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c6c936c-d1ad-4cbf-ac15-9087134ac18c", "node_type": "1", "metadata": {"window": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. ", "original_text": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF. "}, "hash": "90c7ba8fca0ca7f650883ee3c90c681acb0f08c48b1f819c449583c0801e7864", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 14579, "end_char_idx": 14582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c6c936c-d1ad-4cbf-ac15-9087134ac18c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. ", "original_text": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eab1de2e-c0cd-48fc-b272-23c7955b6a12", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Generalized Isolation Forest\n\nIn order to avoid empty branches in EIF, we transpose the EIF problem into the original one defining IF.  More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF. ", "original_text": "3. "}, "hash": "f5f804bc47abc8e407536f7ca625fbf812a22e1040223aa864d849f18af85382", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85fc992b-3178-434a-9278-758cc4845f8c", "node_type": "1", "metadata": {"window": "3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg. ", "original_text": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. "}, "hash": "9e5ff6913432257374fc3fb3289b230a1ecd1f5d3d6b57ba39475a662b7673c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF. ", "mimetype": "text/plain", "start_char_idx": 14582, "end_char_idx": 14668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85fc992b-3178-434a-9278-758cc4845f8c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg. ", "original_text": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c6c936c-d1ad-4cbf-ac15-9087134ac18c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "More precisely, we propose to project all the data on the sampled normal unit vector, look for the minimum and maximum values of the projections (identified by the dotted lines in Fig.  3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. ", "original_text": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF. "}, "hash": "ed6635f97a2a7ff9d496898fd068a8fdd7e6c5f9bd7e060e152f3ce3de064eb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24c47c50-77a4-454f-8291-d113f2a76614", "node_type": "1", "metadata": {"window": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al. ", "original_text": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. "}, "hash": "e099292ac4bb54e6d96a0b864824b3e2736234e58664b59bd9685394721a4093", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. ", "mimetype": "text/plain", "start_char_idx": 14668, "end_char_idx": 14777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24c47c50-77a4-454f-8291-d113f2a76614", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al. ", "original_text": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85fc992b-3178-434a-9278-758cc4845f8c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "3) and sample a split value uniformly between these two values.  Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg. ", "original_text": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data. "}, "hash": "0774beb08588d5ed308a51170621cd062e482b42817ade231bb1b786246ed7e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc690c39-7923-42ae-b21a-655f26c7a433", "node_type": "1", "metadata": {"window": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al. ", "original_text": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. "}, "hash": "f2ab4e7e8551615b4a14b4fde6111451e01df15c6ac9957e9e67f0b01e755148", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. ", "mimetype": "text/plain", "start_char_idx": 14777, "end_char_idx": 14919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc690c39-7923-42ae-b21a-655f26c7a433", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al. ", "original_text": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24c47c50-77a4-454f-8291-d113f2a76614", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that this sampling ensures that there is at least one data in each branch of a tree: the first branch being defined from the min value and the second branch associated with the max value.  This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al. ", "original_text": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube. "}, "hash": "7638aa04d9539d5e75e68a60adbc22792ade45f80d7d954cedda772ccb710bf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08c5f340-caa8-4513-b26a-5a3e4f389740", "node_type": "1", "metadata": {"window": "3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n", "original_text": "Conversely, this volume equals 0 for GIF. "}, "hash": "dae933286e05bf04c4348f967f3d0104c8c1fd123faba4a25def55cbb3c5c062", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. ", "mimetype": "text/plain", "start_char_idx": 14919, "end_char_idx": 15103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "08c5f340-caa8-4513-b26a-5a3e4f389740", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n", "original_text": "Conversely, this volume equals 0 for GIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc690c39-7923-42ae-b21a-655f26c7a433", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This is equivalent to sample an intercept point on the restriction of the line spanned by the normal vector to the segment between the minimum and maximum values of the projected data points as shown in Fig.  3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al. ", "original_text": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube. "}, "hash": "8993476a40144ff1e6073d32805b9b75b853ace549327226d95cd4e3a456d484", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a13266c2-63bc-4519-95a3-4cbd2b0aa72c", "node_type": "1", "metadata": {"window": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3. ", "original_text": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. "}, "hash": "da2fc4c5560b77741c53c263e2bb8e077d404eb69b75328fdcb08d5e92891493", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conversely, this volume equals 0 for GIF. ", "mimetype": "text/plain", "start_char_idx": 15103, "end_char_idx": 15145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a13266c2-63bc-4519-95a3-4cbd2b0aa72c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3. ", "original_text": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08c5f340-caa8-4513-b26a-5a3e4f389740", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "3.  This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n", "original_text": "Conversely, this volume equals 0 for GIF. "}, "hash": "4d4b892cdf551c92b18c4e5775054ac006cbfa23aa9a97b53a210f4c99554d1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d95b56c-1760-4819-afa4-f2edef5ed8fe", "node_type": "1", "metadata": {"window": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. ", "original_text": "Finally, the proposed method can be defined by three algorithms summarized in Alg. "}, "hash": "d42187836dd11cd2d7263c26cc0aedc7ddb65c8fa8f3c7b8592e3e6452f105f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. ", "mimetype": "text/plain", "start_char_idx": 15145, "end_char_idx": 15336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d95b56c-1760-4819-afa4-f2edef5ed8fe", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. ", "original_text": "Finally, the proposed method can be defined by three algorithms summarized in Alg. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a13266c2-63bc-4519-95a3-4cbd2b0aa72c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This strategy ensures that the two branches of a tree are not empty, contrary to EIF.  Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3. ", "original_text": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations. "}, "hash": "1c00fe5d7c1181f95f67711966caf47591b2bed2bb871a84cb42393260fe8e46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21a54d28-3142-4763-b05d-f25b970fe114", "node_type": "1", "metadata": {"window": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n", "original_text": "1, 2 and 3, inspired by Hariri et al. "}, "hash": "8184b9f7d8ffb9486aa1bcfcc8271d722bd25b8338c309b1da83fd1c21b0158b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, the proposed method can be defined by three algorithms summarized in Alg. ", "mimetype": "text/plain", "start_char_idx": 15336, "end_char_idx": 15419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21a54d28-3142-4763-b05d-f25b970fe114", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n", "original_text": "1, 2 and 3, inspired by Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d95b56c-1760-4819-afa4-f2edef5ed8fe", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that it is equivalent to EIF where the sampling volume has been reduced to the convex hull of the data.  Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. ", "original_text": "Finally, the proposed method can be defined by three algorithms summarized in Alg. "}, "hash": "980464c9de48ff10223baa4f17cd6be3f0028bb0aab0ee9f04b95568c3ea6d75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01705e22-daf9-436f-8e2f-3e4edf2569c8", "node_type": "1", "metadata": {"window": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1. ", "original_text": "(2019) and Liu et al. "}, "hash": "ef6fd88dd00cfb0c5ca09631016b7b5b82768bc4ed3952f55a8a2f655a4c0083", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, 2 and 3, inspired by Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 15419, "end_char_idx": 15457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01705e22-daf9-436f-8e2f-3e4edf2569c8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1. ", "original_text": "(2019) and Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21a54d28-3142-4763-b05d-f25b970fe114", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Empty branches in EIF are due to intercepts sampled outside the convex hull of the considered samples and inside the axis-bounding hypercube.  For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n", "original_text": "1, 2 and 3, inspired by Hariri et al. "}, "hash": "168e3b7125ed32890d769d75f0f41e29b8a62139d95b0e06fc83cc49242ead4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ec4eb93-39f2-4679-9e07-a24a681a0ebb", "node_type": "1", "metadata": {"window": "Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. ", "original_text": "(2008).\n\n"}, "hash": "9fab5c82425bfeff613408bcbcdd6f95fcaf4457af177e6d2830f117252fce53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) and Liu et al. ", "mimetype": "text/plain", "start_char_idx": 15457, "end_char_idx": 15479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ec4eb93-39f2-4679-9e07-a24a681a0ebb", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. ", "original_text": "(2008).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01705e22-daf9-436f-8e2f-3e4edf2569c8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For EIF, the probability of sampling an intercept leading to an empty branch is therefore the volume between the hypercube and the convex hull, divided by the volume of the hypercube.  Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1. ", "original_text": "(2019) and Liu et al. "}, "hash": "b9571bef57649dab0462f6d37552c7505081bed7400f298e024cb04b1472b17d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb68f527-6d5e-496c-ae79-a8245a298934", "node_type": "1", "metadata": {"window": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4. ", "original_text": "### 3. "}, "hash": "e161d553647681b0859fb86201a63d0cb6a4eda40ee5e1d371f6fd21549e73f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2008).\n\n", "mimetype": "text/plain", "start_char_idx": 15479, "end_char_idx": 15488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb68f527-6d5e-496c-ae79-a8245a298934", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4. ", "original_text": "### 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ec4eb93-39f2-4679-9e07-a24a681a0ebb", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conversely, this volume equals 0 for GIF.  Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. ", "original_text": "(2008).\n\n"}, "hash": "dcb8440b366ac34ff683e940b869b0c64bb462d58bf5d86712463430053c97fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61a103d3-a98c-4fec-9db8-0a0aae7c3efb", "node_type": "1", "metadata": {"window": "Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. ", "original_text": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. "}, "hash": "7d232f33307286274b0bc91e8a8768d996e3a0a1883b9853c590e56eee6dab3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3. ", "mimetype": "text/plain", "start_char_idx": 15488, "end_char_idx": 15495, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61a103d3-a98c-4fec-9db8-0a0aae7c3efb", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. ", "original_text": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb68f527-6d5e-496c-ae79-a8245a298934", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that probability of having an empty branch in EIF increases as the number of dimensions increases, due to the curse of dimensionality, which motivates the need to avoid such situations.  Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4. ", "original_text": "### 3. "}, "hash": "3e9442a3481db165e11232e9c405cb309041ae2b9828f831bdb6cd1d1dd1309e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfb664ac-d8dc-4630-80b7-647ea5bc9ff2", "node_type": "1", "metadata": {"window": "1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. ", "original_text": "(2019) and Goldstein (2015).\n\n"}, "hash": "aae2391bcf7e2223f190c6d720ac04aa2511dc637a2ef5619993a7de7b8d71bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 15495, "end_char_idx": 15657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfb664ac-d8dc-4630-80b7-647ea5bc9ff2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. ", "original_text": "(2019) and Goldstein (2015).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61a103d3-a98c-4fec-9db8-0a0aae7c3efb", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally, the proposed method can be defined by three algorithms summarized in Alg.  1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. ", "original_text": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al. "}, "hash": "2a0beec842170bdd4971d5f66e9b5da7ab6768067d7e83cdac810de6bde48264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e7ffb8e-d622-4696-af06-4a2c619388e3", "node_type": "1", "metadata": {"window": "(2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. ", "original_text": "#### 3.1. "}, "hash": "118b8ccc169939d000f24aab357dfa760d09981e00b0268bd978d8eb1d3a8d7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) and Goldstein (2015).\n\n", "mimetype": "text/plain", "start_char_idx": 15657, "end_char_idx": 15687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e7ffb8e-d622-4696-af06-4a2c619388e3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. ", "original_text": "#### 3.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfb664ac-d8dc-4630-80b7-647ea5bc9ff2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1, 2 and 3, inspired by Hariri et al.  (2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. ", "original_text": "(2019) and Goldstein (2015).\n\n"}, "hash": "e0338f88224809d82b60035b006b9573440b807febefa468e48644d3b386683e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd950ca3-c8a8-4cc3-83f9-2cbcc99a7fe2", "node_type": "1", "metadata": {"window": "(2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5. ", "original_text": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. "}, "hash": "b3e0c2f6840e5263a385220916c456e6587d071076507deb6c9ffe4b8a7e55e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 3.1. ", "mimetype": "text/plain", "start_char_idx": 15687, "end_char_idx": 15697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd950ca3-c8a8-4cc3-83f9-2cbcc99a7fe2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5. ", "original_text": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e7ffb8e-d622-4696-af06-4a2c619388e3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) and Liu et al.  (2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. ", "original_text": "#### 3.1. "}, "hash": "1c132b16964099ca19558f4a861f2fdf3f3b3683f71c39a00b9e1a6fa011ecaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f784c88-64dd-4294-a27d-31d84c71fe83", "node_type": "1", "metadata": {"window": "### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. ", "original_text": "4. "}, "hash": "634f3edb81e00e6689b5406191fd3f83c62eab3bf59d9c1c25bb184579a41728", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. ", "mimetype": "text/plain", "start_char_idx": 15697, "end_char_idx": 17912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f784c88-64dd-4294-a27d-31d84c71fe83", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. ", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd950ca3-c8a8-4cc3-83f9-2cbcc99a7fe2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2008).\n\n ### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5. ", "original_text": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig. "}, "hash": "3bd01e413e71fb7d7ea895a07bd1556f7192e98608569eef0ef96af727bfcd1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5922ae12-0495-4b3f-9db7-3ebc594146cb", "node_type": "1", "metadata": {"window": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. ", "original_text": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. "}, "hash": "1448f7d667449e93c7c181c43f3a1ee79e6e38aa7b901adc40c6c3618f0db8c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 17912, "end_char_idx": 17915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5922ae12-0495-4b3f-9db7-3ebc594146cb", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. ", "original_text": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f784c88-64dd-4294-a27d-31d84c71fe83", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 3.  Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. ", "original_text": "4. "}, "hash": "bacb8a148bfde70846a727732a95e72a71476dc34cec361c2fba88c31adac2ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bceb0463-7b7e-4e42-ab15-2139d4fdd8e5", "node_type": "1", "metadata": {"window": "(2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n", "original_text": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. "}, "hash": "6f7790852bd1464a2d4ef4702149b2f48a3a30232ff73a0711fa381ace9fa197", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. ", "mimetype": "text/plain", "start_char_idx": 17915, "end_char_idx": 18019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bceb0463-7b7e-4e42-ab15-2139d4fdd8e5", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n", "original_text": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5922ae12-0495-4b3f-9db7-3ebc594146cb", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Experiments\n\nThis section evaluates the performance of the proposed GIF algorithm using synthetic 2D data and some benchmark datasets considered in Hariri et al.  (2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. ", "original_text": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest. "}, "hash": "7e40f0c765af3237728ddf00b7217a51fdebd3a423ec70681e24fa9009813b31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b5042ab-b90e-4d51-a0ae-260eb7b9535c", "node_type": "1", "metadata": {"window": "#### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots. ", "original_text": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. "}, "hash": "122f975afd75dd76a5d9af5be71a54b0ab0aab0e4f405af50b346ae62d17598b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. ", "mimetype": "text/plain", "start_char_idx": 18019, "end_char_idx": 18136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b5042ab-b90e-4d51-a0ae-260eb7b9535c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "#### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots. ", "original_text": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bceb0463-7b7e-4e42-ab15-2139d4fdd8e5", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) and Goldstein (2015).\n\n #### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n", "original_text": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid. "}, "hash": "4aed87deb8f8259f2c79007869e1957b2c952a2b71e6bf70606579d286ad891c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dcec6c1-7582-4990-a850-01794f52a195", "node_type": "1", "metadata": {"window": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points. ", "original_text": "5. "}, "hash": "9839cc01250728d15ca39182e063ef1cfa6db5458c706acb0b240b4c3b092742", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. ", "mimetype": "text/plain", "start_char_idx": 18136, "end_char_idx": 18249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4dcec6c1-7582-4990-a850-01794f52a195", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points. ", "original_text": "5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b5042ab-b90e-4d51-a0ae-260eb7b9535c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "#### 3.1.  Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots. ", "original_text": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig. "}, "hash": "16d5e99363313a962fd6a40c20f07ac963062099ff9da138109d2220d57c8853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce13ace8-7158-4dd6-9cf5-170aff619c3d", "node_type": "1", "metadata": {"window": "4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points. ", "original_text": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. "}, "hash": "3d79c9921171c44d47e5f180f13c8f49273a2dfe2ec7f4fc13e00a3b9731f9f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. ", "mimetype": "text/plain", "start_char_idx": 18249, "end_char_idx": 18252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ce13ace8-7158-4dd6-9cf5-170aff619c3d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points. ", "original_text": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dcec6c1-7582-4990-a850-01794f52a195", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Synthetic datasets\n\nIn order to appreciate the benefits of GIF with respect to EIF and IF, we first consider three datasets of synthetic 2D samples\n\n---\n\n**Algorithm 1 Create the forest**\n```\nInput: X - input data, t - number of trees, \u03c8 - subsampling size\nOutput: Forest - a set of iTrees\n1: function IFOREST(X, t, \u03c8)\n2:    initialize Forest \u2190 struct          \u25b7 Empty structure\n3:    set l = ceil(log\u2082 \u03c8)                  \u25b7 Height limit\n4:    for all i = 1 to t do\n5:        X' \u2190 Sample(X, \u03c8)              \u25b7 Subsample of size \u03c8\n6:        Forest.Tree(i) \u2190 ITREE(X', 0, l)\n7:    end for\n8: end function\n```\n\n**Algorithm 2 Create a tree**\n```\nInput: X - input data, e - current tree height, l - height limit\nOutput: Tree - an iTree\n1: function ITREE(X, e, l)\n2:    initialize Tree \u2190 struct          \u25b7 Empty structure\n3:    if e \u2265 l or |X| \u2264 1 then\n4:        Tree.Size \u2190 |X|                \u25b7 Number of remaining data\n5:        Tree.Type \u2190 'ext'              \u25b7 No nodes after this one\n6:    else\n7:        draw w ~ N(0, I\u1d48)\n8:        w \u2190 w/||w||\u2082                   \u25b7 Random unit vector of \u211d\u1d48\n9:        p_min \u2190 min(Xw)\n10:       p_max \u2190 max(Xw)\n11:       draw p ~ U([p_min; p_max])\n12:       X_l \u2190 X(Xw \u2264 p, :)\n13:       X_r \u2190 X(Xw > p, :)\n14:       Tree.Level \u2190 e                 \u25b7 Level of the node\n15:       Tree.Left \u2190 ITREE(X_l, e + 1, l)\n16:       Tree.Right \u2190 ITREE(X_r, e + 1, l)\n17:       Tree.Normal \u2190 w\n18:       Tree.Threshold \u2190 p\n19:       Tree.Type \u2190 'int'              \u25b7 Nodes after this one\n20:   end if\n21: end function\n```\n\n**Algorithm 3 Compute isolation score (Path Length)**\n```\nInput: x - input vector, Tree - an iTree, e - current path length\n1: # e must be initialized to 0 when first called\nOutput: Length - isolation score\n2: function PL(x, Tree, e)\n3:    if Tree.Type = 'ext' then\n4:        if Tree.size > 1 then\n5:            Length \u2190 e + c(Tree.size)  \u25b7 see (2)\n6:        else\n7:            Length \u2190 e\n8:        end if\n9:    else\n10:       w \u2190 Tree.Normal\n11:       p \u2190 Tree.Threshold\n12:       if x\u1d40w \u2264 p then\n13:           Length \u2190 PL(x, Tree.Left, e + 1)\n14:       else\n15:           Length \u2190 PL(x, Tree.Right, e + 1)\n16:       end if\n17:   end if\n18: end function\n```\n\ndisplayed in Fig.  4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points. ", "original_text": "5. "}, "hash": "849f3ea65a5220346dc3d9247c8f60883dab0358e08f20591d681a8c5445afb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4828029c-c65e-4c55-9608-befa346e5a2c", "node_type": "1", "metadata": {"window": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n", "original_text": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. "}, "hash": "4f341d17a988467664b5be98782200c1956e31ef59e928cb50c08f677333b76a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 18252, "end_char_idx": 18342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4828029c-c65e-4c55-9608-befa346e5a2c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n", "original_text": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce13ace8-7158-4dd6-9cf5-170aff619c3d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "4.  For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points. ", "original_text": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al. "}, "hash": "7f7296197ca36e5801d77ae799b966a252e68ae0692e8264da12f2db720280e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f38646e0-18a3-40cb-abf8-244d8a705598", "node_type": "1", "metadata": {"window": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig. ", "original_text": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n"}, "hash": "3df0c3f73ebe61a76cee4e9bf6372c8a2e7503d432131ce0233ba7720db89cc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. ", "mimetype": "text/plain", "start_char_idx": 18342, "end_char_idx": 18477, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f38646e0-18a3-40cb-abf8-244d8a705598", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig. ", "original_text": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4828029c-c65e-4c55-9608-befa346e5a2c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, IF, EIF and GIF are run on the same data to learn the corresponding isolation forest.  After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n", "original_text": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF. "}, "hash": "1e3288006cd6411fa90364b71f3d1eabc29b30e7b3c060fe0b83785319dc25c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b58cda5d-abee-4556-826f-c4e6a96fe71d", "node_type": "1", "metadata": {"window": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n", "original_text": "[Image: Three scatter plots. "}, "hash": "057aa0211c2954586f3053521d81df82c0b9f3e0e316e793c2ecffe460ce8dfd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n", "mimetype": "text/plain", "start_char_idx": 18477, "end_char_idx": 18665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b58cda5d-abee-4556-826f-c4e6a96fe71d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n", "original_text": "[Image: Three scatter plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f38646e0-18a3-40cb-abf8-244d8a705598", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "After building the isolation forests, a square area containing all the samples is transformed into a 100 \u00d7 100 grid.  The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig. ", "original_text": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n"}, "hash": "57fce40960101591a9197f85e3997ed440f7cfc1799b010ac11c3b7bb1484d39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2386b1d7-e78f-470f-af90-c257e0b9d052", "node_type": "1", "metadata": {"window": "5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps. ", "original_text": "The first, \"Single Blob\", shows a single dense circular cluster of points. "}, "hash": "d3028618f67c5efebb7ec50682b546a69e779ab520d204f5786c0a37c3b7f70f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Image: Three scatter plots. ", "mimetype": "text/plain", "start_char_idx": 18665, "end_char_idx": 18694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2386b1d7-e78f-470f-af90-c257e0b9d052", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps. ", "original_text": "The first, \"Single Blob\", shows a single dense circular cluster of points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b58cda5d-abee-4556-826f-c4e6a96fe71d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The anomaly score is computed for each point of this grid in order to build heat maps that are displayed in Fig.  5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n", "original_text": "[Image: Three scatter plots. "}, "hash": "8cb5e20d1a445035f0c1894972636553b6514f1757a585587c6fb1d9850f2ca9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee3a467a-752c-4ac3-82e5-f32079ee1bb5", "node_type": "1", "metadata": {"window": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig. ", "original_text": "The second, \"Double Blob\", shows two distinct circular clusters of points. "}, "hash": "c73fcff1676ff12825d2a7b4beb0c427cc4a631909a6ee54e0eb4dbe7bcc348f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first, \"Single Blob\", shows a single dense circular cluster of points. ", "mimetype": "text/plain", "start_char_idx": 18694, "end_char_idx": 18769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ee3a467a-752c-4ac3-82e5-f32079ee1bb5", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig. ", "original_text": "The second, \"Double Blob\", shows two distinct circular clusters of points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2386b1d7-e78f-470f-af90-c257e0b9d052", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "5.  The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps. ", "original_text": "The first, \"Single Blob\", shows a single dense circular cluster of points. "}, "hash": "a40cc3aa67c7c93126ac676d682610375863aac7da276f72c06ef6424061a827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b643eeef-d06f-4b60-a3ba-4fdc228110a3", "node_type": "1", "metadata": {"window": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal). ", "original_text": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n"}, "hash": "cde2fd166c367616d3de775ec188cd5d9a37a710685d387fceaad28442634372", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second, \"Double Blob\", shows two distinct circular clusters of points. ", "mimetype": "text/plain", "start_char_idx": 18769, "end_char_idx": 18844, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b643eeef-d06f-4b60-a3ba-4fdc228110a3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal). ", "original_text": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee3a467a-752c-4ac3-82e5-f32079ee1bb5", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The advantages of EIF and GIF with respect to IF, as already highlighted in Hariri et al.  (2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig. ", "original_text": "The second, \"Double Blob\", shows two distinct circular clusters of points. "}, "hash": "f9eb4156dca82917e46226a8623096817eecb511ecec2f3e685e6aafe0614f26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24cd2eb3-ed74-409a-a81c-c66404a29267", "node_type": "1", "metadata": {"window": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF). ", "original_text": "**Fig. "}, "hash": "51c06534b91deae9cb96fcd95fa8a347fcf45d64ecb3dbe7c2ea3a0754930117", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n", "mimetype": "text/plain", "start_char_idx": 18844, "end_char_idx": 18917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24cd2eb3-ed74-409a-a81c-c66404a29267", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF). ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b643eeef-d06f-4b60-a3ba-4fdc228110a3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019), are clear: the \u201ccross\u201d on the single blob, the sinusoid, and the ghost blobs for the second example disappear for GIF and EIF.  In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal). ", "original_text": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n"}, "hash": "f2d971deb611011d912baee9500d875fc2d28d7ae630203ff0a7dc7216a52ea2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ecb18af-8c41-4107-bd41-5e343f198fd2", "node_type": "1", "metadata": {"window": "[Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. ", "original_text": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n"}, "hash": "44950556de58333ea1dce6cfac5b4491642ed16b02bc5ee07589e5509be80f4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 18917, "end_char_idx": 18924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ecb18af-8c41-4107-bd41-5e343f198fd2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. ", "original_text": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24cd2eb3-ed74-409a-a81c-c66404a29267", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In order to have a quantitative appreciation of the various methods, the next experiments consider several benchmark datasets whose anomalies are detected using the different algorithms.\n\n [Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF). ", "original_text": "**Fig. "}, "hash": "9fdf5e8f4f2e306b17d6959ac634c540099347958b6312514d3fe160b3ccba02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a27368-deac-4091-b2f1-e5f46582f5a0", "node_type": "1", "metadata": {"window": "The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. ", "original_text": "[Image: A 3x3 grid of heat maps. "}, "hash": "b442ed60a59492ad262ca806fe7bdb8a40d613b50709d32685fe4aa7692a5471", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n", "mimetype": "text/plain", "start_char_idx": 18924, "end_char_idx": 18995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2a27368-deac-4091-b2f1-e5f46582f5a0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. ", "original_text": "[Image: A 3x3 grid of heat maps. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ecb18af-8c41-4107-bd41-5e343f198fd2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: Three scatter plots.  The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. ", "original_text": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n"}, "hash": "1f3f099ffcf852e6422386df1ff9cfb152177c0504d9e59ab0d6200748f3ad18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cb63f9f-373a-4c0d-8179-e93efa5290ab", "node_type": "1", "metadata": {"window": "The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n", "original_text": "The rows correspond to the datasets from Fig. "}, "hash": "36b2e3a3179c3ef5cb1d91d1d17d4904af053a063033b5037ed30524f055b192", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Image: A 3x3 grid of heat maps. ", "mimetype": "text/plain", "start_char_idx": 18995, "end_char_idx": 19028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0cb63f9f-373a-4c0d-8179-e93efa5290ab", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n", "original_text": "The rows correspond to the datasets from Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a27368-deac-4091-b2f1-e5f46582f5a0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The first, \"Single Blob\", shows a single dense circular cluster of points.  The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. ", "original_text": "[Image: A 3x3 grid of heat maps. "}, "hash": "2a792b4bc7a4cbc090ddfcb69c4b274765131da5b84ff7ba8e9262a33157065b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86cec02c-6c56-4fa3-9f92-ffbfbd362989", "node_type": "1", "metadata": {"window": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig. ", "original_text": "4 (single blob, double blob, sinusoidal). "}, "hash": "e66299c510d88f1616da33520b00fde2725633e582e86d87cf3406a1d015e6ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rows correspond to the datasets from Fig. ", "mimetype": "text/plain", "start_char_idx": 19028, "end_char_idx": 19074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86cec02c-6c56-4fa3-9f92-ffbfbd362989", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig. ", "original_text": "4 (single blob, double blob, sinusoidal). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cb63f9f-373a-4c0d-8179-e93efa5290ab", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The second, \"Double Blob\", shows two distinct circular clusters of points.  The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n", "original_text": "The rows correspond to the datasets from Fig. "}, "hash": "6256efb19003b7acff14094faa4afa132cdcbf37dc2d83b8f3ecb1951bf5143d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c604bf7-9d8b-4573-b3d1-ed7b3d94fb1b", "node_type": "1", "metadata": {"window": "**Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. ", "original_text": "The columns correspond to the algorithms (IF, EIF, GIF). "}, "hash": "7bb6cfc7a97a743795d8271c95139aaa6e63a9c4ac82efeb6b874493055d2b6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4 (single blob, double blob, sinusoidal). ", "mimetype": "text/plain", "start_char_idx": 19074, "end_char_idx": 19116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c604bf7-9d8b-4573-b3d1-ed7b3d94fb1b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. ", "original_text": "The columns correspond to the algorithms (IF, EIF, GIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86cec02c-6c56-4fa3-9f92-ffbfbd362989", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The third, \"Sinusoidal\", shows points arranged in a sine wave pattern.]\n\n **Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig. ", "original_text": "4 (single blob, double blob, sinusoidal). "}, "hash": "47869fea0ce2803d9674981229b11825154718895bd2c169442513108033453f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8748ebb6-d6b0-433f-9067-43573b0e5c3d", "node_type": "1", "metadata": {"window": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n", "original_text": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. "}, "hash": "318958c2f3cf55db2904e3fede856b83987415931d585bab6dabc38adb049552", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The columns correspond to the algorithms (IF, EIF, GIF). ", "mimetype": "text/plain", "start_char_idx": 19116, "end_char_idx": 19173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8748ebb6-d6b0-433f-9067-43573b0e5c3d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n", "original_text": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c604bf7-9d8b-4573-b3d1-ed7b3d94fb1b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. ", "original_text": "The columns correspond to the algorithms (IF, EIF, GIF). "}, "hash": "efbfcbe1799f6a5e44fadbf95e0ca540d291b762fc7d6bd70dce5ba8356ecf6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16c3cbf5-4494-478a-9f18-c76cbb39e253", "node_type": "1", "metadata": {"window": "[Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2. ", "original_text": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. "}, "hash": "70be0005e2dcf1359c135ead7a459630c07bb120e0fee41831436db42aebf2cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. ", "mimetype": "text/plain", "start_char_idx": 19173, "end_char_idx": 19272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16c3cbf5-4494-478a-9f18-c76cbb39e253", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2. ", "original_text": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8748ebb6-d6b0-433f-9067-43573b0e5c3d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "4:** Synthetic 2D datasets used to visualize the gain of EIF and GIF.\n\n [Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n", "original_text": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob. "}, "hash": "af8de962a84320f7da37dee4d7ee3ed64ea387e7571b8a345ac9016a3a4e4c25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74cd2162-b5b4-400b-b6fb-a781f84769db", "node_type": "1", "metadata": {"window": "The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. ", "original_text": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n"}, "hash": "d7cac2855ab21bc9e7e3460046a94546b2e26efe1a779d5f0fde9e3b6ff803e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. ", "mimetype": "text/plain", "start_char_idx": 19272, "end_char_idx": 19359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74cd2162-b5b4-400b-b6fb-a781f84769db", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. ", "original_text": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16c3cbf5-4494-478a-9f18-c76cbb39e253", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A 3x3 grid of heat maps.  The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2. ", "original_text": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts. "}, "hash": "eef7fe3fabd1f6ef007e895de83ae047989864e17911da0c483c6a3bb49f90dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62cf65ed-716a-434e-9786-4b74d94e8da0", "node_type": "1", "metadata": {"window": "4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015). ", "original_text": "**Fig. "}, "hash": "ddc85de87190d0e39cbc376bec0b1e1527a7a6f9169839a7464786c8b8524d41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n", "mimetype": "text/plain", "start_char_idx": 19359, "end_char_idx": 19433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62cf65ed-716a-434e-9786-4b74d94e8da0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015). ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74cd2162-b5b4-400b-b6fb-a781f84769db", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The rows correspond to the datasets from Fig.  4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. ", "original_text": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n"}, "hash": "ed7b7d965b861194498fbe457b1a26a625b06710dee4c40ec727fad13b4a5b99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "330415fe-160a-4062-af40-34b6e0c6dbe2", "node_type": "1", "metadata": {"window": "The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. ", "original_text": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. "}, "hash": "4403f222fc1ea278bf3b025bc05f4e27a2098b98994fe74181ea1b05311a41dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 19433, "end_char_idx": 19440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "330415fe-160a-4062-af40-34b6e0c6dbe2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. ", "original_text": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62cf65ed-716a-434e-9786-4b74d94e8da0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "4 (single blob, double blob, sinusoidal).  The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015). ", "original_text": "**Fig. "}, "hash": "8e7bd90dd2632e6eff30ffece82dbcad5d904fe6f331cf6127edb62028902c02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4db7f5fb-b33b-4ef0-8b62-50c01722d409", "node_type": "1", "metadata": {"window": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)). ", "original_text": "Pink values correspond to low anomaly scores and yellow to high.\n\n"}, "hash": "b2864c6b9e3c776aeb716f82e5d1aec0c5d186bb52fdecb56180964a7f7c01a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. ", "mimetype": "text/plain", "start_char_idx": 19440, "end_char_idx": 19599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4db7f5fb-b33b-4ef0-8b62-50c01722d409", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)). ", "original_text": "Pink values correspond to low anomaly scores and yellow to high.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "330415fe-160a-4062-af40-34b6e0c6dbe2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The columns correspond to the algorithms (IF, EIF, GIF).  The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. ", "original_text": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom. "}, "hash": "0e886727c1e5545f8d7eaa0cf35f0f3ea1916504f977e95011518561cadb4425", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97c62e9b-3461-4f8f-b35b-c63ce38dbef2", "node_type": "1", "metadata": {"window": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n", "original_text": "---\n\n#### 3.2. "}, "hash": "86761f276913b7f0e84b4b5f7762b2431f0e568fe98540b930eef4611269562b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pink values correspond to low anomaly scores and yellow to high.\n\n", "mimetype": "text/plain", "start_char_idx": 19599, "end_char_idx": 19665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97c62e9b-3461-4f8f-b35b-c63ce38dbef2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n", "original_text": "---\n\n#### 3.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4db7f5fb-b33b-4ef0-8b62-50c01722d409", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The IF column shows artifacts: a cross shape for the single blob, ghost blobs for the double blob.  The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)). ", "original_text": "Pink values correspond to low anomaly scores and yellow to high.\n\n"}, "hash": "62cea65fc042b55616624a7d044d52ee0e04a6318986c7c49aebd0a6eadfd04c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dff72503-5335-4552-848e-8964d9de8bd8", "node_type": "1", "metadata": {"window": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. ", "original_text": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. "}, "hash": "c537d3c18bbe2fbfcccb3c769848e05e20762b4ec57821c993c55628704d045a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n#### 3.2. ", "mimetype": "text/plain", "start_char_idx": 19665, "end_char_idx": 19680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dff72503-5335-4552-848e-8964d9de8bd8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. ", "original_text": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97c62e9b-3461-4f8f-b35b-c63ce38dbef2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The EIF and GIF columns show smooth, circular anomaly regions without these artifacts.  Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n", "original_text": "---\n\n#### 3.2. "}, "hash": "601d103f481b41b1818f0b976344dd34f0ddb53f871e9e946e8a6e27acc713ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "506b4c20-e643-49bf-8ab2-21d657ef8c44", "node_type": "1", "metadata": {"window": "**Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig. ", "original_text": "(2019)\u00b9 and Goldstein (2015). "}, "hash": "28f35ce290659621f3b8f70a6de1fb9298da0ad5e404e8a31c9670ee7ad172f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 19680, "end_char_idx": 19792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "506b4c20-e643-49bf-8ab2-21d657ef8c44", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig. ", "original_text": "(2019)\u00b9 and Goldstein (2015). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dff72503-5335-4552-848e-8964d9de8bd8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pink/purple indicates low anomaly scores, yellow indicates high scores.]\n\n **Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. ", "original_text": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al. "}, "hash": "24c8a19fec3cc51ffc0330e6a32050588c9ccb3125fc6a510ddbe7b5c5e6ea3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a569ae1d-5067-4cdf-9197-01dde5f833bd", "node_type": "1", "metadata": {"window": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig. ", "original_text": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. "}, "hash": "18f33be93b22a74b83c3ad2fb58267d79ea088ff3a587fd2dc65171ed9f0bab7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019)\u00b9 and Goldstein (2015). ", "mimetype": "text/plain", "start_char_idx": 19792, "end_char_idx": 19822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a569ae1d-5067-4cdf-9197-01dde5f833bd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig. ", "original_text": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "506b4c20-e643-49bf-8ab2-21d657ef8c44", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig. ", "original_text": "(2019)\u00b9 and Goldstein (2015). "}, "hash": "914ebebf6e0812281e1e64daa0be698696a8de88833234bbcb96e8f9fbea2570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb61d735-2448-4ca4-a0a8-db7c30b53c55", "node_type": "1", "metadata": {"window": "Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves. ", "original_text": "(2019)). "}, "hash": "e8d485ac3ca08bdc2682fcca2750da72ef14ad00982d05e81c8f91f98bebf486", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 19822, "end_char_idx": 20000, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb61d735-2448-4ca4-a0a8-db7c30b53c55", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves. ", "original_text": "(2019)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a569ae1d-5067-4cdf-9197-01dde5f833bd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "5:** Heat maps for the three algorithms and the three datasets: IF, EIF, GIF from left to right, and single blob, dual blob and sinusoidal from top to bottom.  Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig. ", "original_text": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al. "}, "hash": "c0d4b6c84ef55c485292fd6f0bbd5dd96930f978b4d5d02e89ade7a4e3069986", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49ec290c-97b2-4ecc-827f-b0e2018b8958", "node_type": "1", "metadata": {"window": "---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. ", "original_text": "Since\n\n**Table 2:** Datasets used in the experiments.\n"}, "hash": "19d0a74f0c21ede1651fab97a522c7d94ab799e21d18488c3d2bdcc2312037d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019)). ", "mimetype": "text/plain", "start_char_idx": 20000, "end_char_idx": 20009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49ec290c-97b2-4ecc-827f-b0e2018b8958", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. ", "original_text": "Since\n\n**Table 2:** Datasets used in the experiments.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb61d735-2448-4ca4-a0a8-db7c30b53c55", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pink values correspond to low anomaly scores and yellow to high.\n\n ---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves. ", "original_text": "(2019)). "}, "hash": "67ce2c53b70854ca99416adfc0616c6bcb0e6d8dc6ff29d3ade959d52e7e7f0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "374b475e-7e6d-4f18-99df-856f313a87ee", "node_type": "1", "metadata": {"window": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. ", "original_text": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. "}, "hash": "474e00759119ca4b8b246bd6054d3a2823f98700d46df488e306ce2f22a6a96a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since\n\n**Table 2:** Datasets used in the experiments.\n", "mimetype": "text/plain", "start_char_idx": 20009, "end_char_idx": 20063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "374b475e-7e6d-4f18-99df-856f313a87ee", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. ", "original_text": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49ec290c-97b2-4ecc-827f-b0e2018b8958", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n\n#### 3.2.  Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. ", "original_text": "Since\n\n**Table 2:** Datasets used in the experiments.\n"}, "hash": "cf2a425203d2c4fb1dd10ab32466ac36327c5bd4f1e42fe12075f10ff35d8307", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b6b7f92-190c-4571-9892-e5aada5096e2", "node_type": "1", "metadata": {"window": "(2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n", "original_text": "The results are gathered in Fig. "}, "hash": "de11a529c40fc18e8e9c34d9d4f68bd061feeb68d07c25f95cda159163b9e806", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. ", "mimetype": "text/plain", "start_char_idx": 20063, "end_char_idx": 21040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b6b7f92-190c-4571-9892-e5aada5096e2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n", "original_text": "The results are gathered in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "374b475e-7e6d-4f18-99df-856f313a87ee", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Benchmark datasets\n\nThis section evaluates the performance of GIF on the datasets investigated in Hariri et al.  (2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. ", "original_text": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals. "}, "hash": "2c59afd0d6dd7295de102364e3e125d756b0bc1c9bed82bec8b94bd9a77e00ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9fdc72c-dc35-46c1-b311-9f5921033946", "node_type": "1", "metadata": {"window": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig. ", "original_text": "6 for the ROC and in Fig. "}, "hash": "722e0fd8e2d88037fdad441d1412ef2833c7cd8fe525926d1379362d42310d45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results are gathered in Fig. ", "mimetype": "text/plain", "start_char_idx": 21040, "end_char_idx": 21073, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9fdc72c-dc35-46c1-b311-9f5921033946", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig. ", "original_text": "6 for the ROC and in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b6b7f92-190c-4571-9892-e5aada5096e2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)\u00b9 and Goldstein (2015).  Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n", "original_text": "The results are gathered in Fig. "}, "hash": "4228b7206b7220d3e9d2c07022fa424c46f722be5b04148953e826201553a349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c437fd1d-5437-4156-ac11-3e543c737429", "node_type": "1", "metadata": {"window": "(2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "7 for PR curves. "}, "hash": "40c3a1818f511e9fd4be116c0605bf1a2fa8ca11ab7ce575a8f66c115143982b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6 for the ROC and in Fig. ", "mimetype": "text/plain", "start_char_idx": 21073, "end_char_idx": 21099, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c437fd1d-5437-4156-ac11-3e543c737429", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "7 for PR curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9fdc72c-dc35-46c1-b311-9f5921033946", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that the different datasets are described in Table 2 and are ranked in increasing order regarding the anomaly proportion (datasets in italic are those used in Hariri et al.  (2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig. ", "original_text": "6 for the ROC and in Fig. "}, "hash": "039b3e4f3a1d3326a89988dbd812da8e06090dcd9a99ebd99eca09966fed3344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dab299c-1f88-4cd6-9805-d4c1a2108f84", "node_type": "1", "metadata": {"window": "Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig. ", "original_text": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. "}, "hash": "5c62b3d461557bc5eb623d489f35f2e6e77f76de54df847dd3fe776c8c23e40a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 for PR curves. ", "mimetype": "text/plain", "start_char_idx": 21099, "end_char_idx": 21116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1dab299c-1f88-4cd6-9805-d4c1a2108f84", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig. ", "original_text": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c437fd1d-5437-4156-ac11-3e543c737429", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019)).  Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "7 for PR curves. "}, "hash": "233e010d0829ff69475b240bd5625dddaf64f7086a3f2295f423cee9986cbb23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2622af4e-873e-49f8-95ce-190a5ce0b80a", "node_type": "1", "metadata": {"window": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC. ", "original_text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. "}, "hash": "a16ab0248f3b6dddff175f3209f2b3a46acb995ecf084b4b3ab3df191dfb62db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. ", "mimetype": "text/plain", "start_char_idx": 21116, "end_char_idx": 21222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2622af4e-873e-49f8-95ce-190a5ce0b80a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC. ", "original_text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dab299c-1f88-4cd6-9805-d4c1a2108f84", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Since\n\n**Table 2:** Datasets used in the experiments.\n | Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig. ", "original_text": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets. "}, "hash": "9326a11b8fbe666542875074ae069e5a3115ec44eebb71765d187f28ba71fb8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7266fb56-95c5-40c2-ae0f-27cd88456d42", "node_type": "1", "metadata": {"window": "The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n"}, "hash": "c9e22c29d396303c0334f90220f7953637a6fa1b789f085b66d1ab0ead4d9dee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. ", "mimetype": "text/plain", "start_char_idx": 21222, "end_char_idx": 21362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7266fb56-95c5-40c2-ae0f-27cd88456d42", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2622af4e-873e-49f8-95ce-190a5ce0b80a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "| Name | Samples n | Features d | Anomalies |\n| :--- | :--- | :--- | :--- |\n| Pen Local | 6724 | 16 | 0.15% |\n| *Forest Cover* | 286048 | 10 | 0.96% |\n| Speech | 3686 | 400 | 1.65% |\n| *Shuttle* | 46464 | 9 | 1.89% |\n| *Mammography* | 11183 | 6 | 2.32% |\n| Breast Cancer | 367 | 30 | 2.72% |\n| Aloi | 50000 | 27 | 3.02% |\n| *ANN Thyroid* | 6916 | 21 | 3.61% |\n| *Letter* | 1600 | 32 | 6.25% |\n| Cardio | 1831 | 21 | 9.60% |\n| *Pen Global* | 809 | 16 | 11.12% |\n| *Satellite* | 6435 | 36 | 31.64% |\n| *Ionosphere* | 351 | 33 | 35.90% |\n\nIF-based anomaly detectors include some randomness due to the way the trees are built, Monte-Carlo simulations (using 100 iterations) were performed for all the datasets and the three methods (IF, EIF and GIF) to compute the average area under the curve (AUC) for both receiver operational characteristics (ROC) and precision recall (PR) curves, as well as quantiles \u03b1/2 and 1 \u2212 \u03b1/2 where \u03b1 = 5% in order to obtain 95% confidence intervals.  The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC. ", "original_text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval. "}, "hash": "2dc0f1205296c38c882e25dfd57a6c32907cd75a486ada8e88144e1174c7222c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed37c662-84b9-4b38-aba4-1feb2820393a", "node_type": "1", "metadata": {"window": "6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n", "original_text": "**Fig. "}, "hash": "9eba7b5bdf91c7dafc9998c26ecef4d090ef40827db8c2acd17e2c268c4f7aaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n", "mimetype": "text/plain", "start_char_idx": 21362, "end_char_idx": 21425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed37c662-84b9-4b38-aba4-1feb2820393a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7266fb56-95c5-40c2-ae0f-27cd88456d42", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The results are gathered in Fig.  6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n"}, "hash": "e0ccc6eb4877298e3935d0d27e869ccf312050901e389845b7f486f929e891f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c0be651-afde-4262-9955-bc9b11ff58d1", "node_type": "1", "metadata": {"window": "7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig. ", "original_text": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "433b5c3e903cca44fe1d3dd3d077991c16824b20dba6b80f7822c6f000f8a3e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 21425, "end_char_idx": 21432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c0be651-afde-4262-9955-bc9b11ff58d1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig. ", "original_text": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed37c662-84b9-4b38-aba4-1feb2820393a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "6 for the ROC and in Fig.  7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n", "original_text": "**Fig. "}, "hash": "5de5aee135a8211328533166c7d1f54ba8c3b4457eaa2ef239c1967f1b38f61b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a5d185d-8c1a-4dbd-9019-723c60e1afcc", "node_type": "1", "metadata": {"window": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "[Image: A horizontal bar chart similar to Fig. "}, "hash": "ed186265ea2d7e8951e54a60d407f888df5321a0eb920fd55035d01208587c21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "mimetype": "text/plain", "start_char_idx": 21432, "end_char_idx": 21561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a5d185d-8c1a-4dbd-9019-723c60e1afcc", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "[Image: A horizontal bar chart similar to Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c0be651-afde-4262-9955-bc9b11ff58d1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "7 for PR curves.  Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig. ", "original_text": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "0d68a7c8cd516e659d00103968b455dc53df0389a1a8bbf5dcfd6254874caad1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "246cba6b-0936-46f6-884e-05f7d67c66dc", "node_type": "1", "metadata": {"window": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. ", "original_text": "6, but comparing PR AUC. "}, "hash": "db90fec1ddd456077bd4e53ed17fadaa5528c65a71059e9be7c85c3161665a12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Image: A horizontal bar chart similar to Fig. ", "mimetype": "text/plain", "start_char_idx": 21561, "end_char_idx": 21608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "246cba6b-0936-46f6-884e-05f7d67c66dc", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. ", "original_text": "6, but comparing PR AUC. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a5d185d-8c1a-4dbd-9019-723c60e1afcc", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that the computations were\n\n[Image: A horizontal bar chart comparing ROC AUC for different datasets.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "[Image: A horizontal bar chart similar to Fig. "}, "hash": "fb8ddbf48b754f046cc3f7c9f44965d7b91a0e6cbcd8e37019c9b9a7832c4c2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0c2f497-7765-45cd-a3b8-d2734be9731b", "node_type": "1", "metadata": {"window": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. ", "original_text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. "}, "hash": "4fd4a97caf2ed85acb6e993efa5e9aa5416123d1c91a7276f65207a29c808cd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6, but comparing PR AUC. ", "mimetype": "text/plain", "start_char_idx": 21608, "end_char_idx": 21633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0c2f497-7765-45cd-a3b8-d2734be9731b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. ", "original_text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "246cba6b-0936-46f6-884e-05f7d67c66dc", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean ROC AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. ", "original_text": "6, but comparing PR AUC. "}, "hash": "c25d5eec2ff038d8148bf9e4cab09462d2eef40ad718d11bb137d3e5562d0f51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa27338e-b680-4434-94ca-047532f9259e", "node_type": "1", "metadata": {"window": "**Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n"}, "hash": "ddfa1876c86b4bc8a776f50a4c256d977b6dc7011dd8f521672ecb5d8fa402f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. ", "mimetype": "text/plain", "start_char_idx": 21633, "end_char_idx": 21772, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fa27338e-b680-4434-94ca-047532f9259e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0c2f497-7765-45cd-a3b8-d2734be9731b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"ROC AUC\".]\n\n **Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. ", "original_text": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval. "}, "hash": "c3cea1da7142ed93905b8c67c7a8f0f003fa62de133ae9a50e71aa5fb22f350c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e4c7d0f-b7c8-4f0b-9118-6058526b446a", "node_type": "1", "metadata": {"window": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature. ", "original_text": "**Fig. "}, "hash": "cf2e74e3bdf853c21ec46746b43e02ebe59fa63158aade82e3b8bd3a01f96085", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n", "mimetype": "text/plain", "start_char_idx": 21772, "end_char_idx": 21834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6e4c7d0f-b7c8-4f0b-9118-6058526b446a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa27338e-b680-4434-94ca-047532f9259e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n"}, "hash": "bcdbe83a87d84a4821b1696611dc3cd45d442e4426083ac99bac1ee2ec354785", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2108fc79-ae11-44ec-a3f9-95fcb3b125fb", "node_type": "1", "metadata": {"window": "[Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n", "original_text": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "e1b2b40aacf42d27e3f3fd7924cf7f6fa9ae58e7fe779ad9d73993a22139418d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 21834, "end_char_idx": 21841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2108fc79-ae11-44ec-a3f9-95fcb3b125fb", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n", "original_text": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e4c7d0f-b7c8-4f0b-9118-6058526b446a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "6:** Comparison of ROC AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature. ", "original_text": "**Fig. "}, "hash": "ba8d7728333f05c5348c18d50cb2d7caf12c57e23e70930be3998fa1d52de977", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5054a9c2-41b9-444f-b4e0-c76c83c706be", "node_type": "1", "metadata": {"window": "6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. ", "original_text": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. "}, "hash": "1c05dd1428bde351e13104fdab43df7808e9077f8f3d14e8ceb1359a274d6d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "mimetype": "text/plain", "start_char_idx": 21841, "end_char_idx": 21969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5054a9c2-41b9-444f-b4e0-c76c83c706be", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. ", "original_text": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2108fc79-ae11-44ec-a3f9-95fcb3b125fb", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A horizontal bar chart similar to Fig.  6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n", "original_text": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "fac93b45eec0b3a1361e3fd18fa947a14f356123c322a0c266f6637a1048c1f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d127103c-5cef-492b-bbbf-69bf2e33489f", "node_type": "1", "metadata": {"window": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. ", "original_text": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. "}, "hash": "e66779b723b39d63521c264836c7adb06e23ab29d20d44b195487833f044d7fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. ", "mimetype": "text/plain", "start_char_idx": 21969, "end_char_idx": 22096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d127103c-5cef-492b-bbbf-69bf2e33489f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. ", "original_text": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5054a9c2-41b9-444f-b4e0-c76c83c706be", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "6, but comparing PR AUC.  For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. ", "original_text": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF. "}, "hash": "0178ba12262a1dbced86f0d4e7f414ac58f7c9711a8b585192b4e23238ec729b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67c9c8e9-51f2-433b-8a33-62c3a8aa2a72", "node_type": "1", "metadata": {"window": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. ", "original_text": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. "}, "hash": "060c28f1daaeb6e45c3584efefdfa15c031456bf7e4553443477d23ed100da6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. ", "mimetype": "text/plain", "start_char_idx": 22096, "end_char_idx": 22181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67c9c8e9-51f2-433b-8a33-62c3a8aa2a72", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. ", "original_text": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d127103c-5cef-492b-bbbf-69bf2e33489f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are three horizontal bars (Standard, Extended, Generalized) showing the mean PR AUC and a 95% confidence interval.  The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. ", "original_text": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074. "}, "hash": "d98a9da1d852b79dc447b2cbaf33c94b416cb93aab03f65a8a8bed8755644299", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "405e4802-e27d-4e57-87df-74706433d4c4", "node_type": "1", "metadata": {"window": "**Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. ", "original_text": "This preprocessing is not necessary for IF because splittings are made along a single feature. "}, "hash": "7caeef814d2d85021267a51b437a0e76ba29653aa56b721dc9b57d5b75871023", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. ", "mimetype": "text/plain", "start_char_idx": 22181, "end_char_idx": 22292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "405e4802-e27d-4e57-87df-74706433d4c4", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. ", "original_text": "This preprocessing is not necessary for IF because splittings are made along a single feature. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67c9c8e9-51f2-433b-8a33-62c3a8aa2a72", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"PR AUC\".]\n\n **Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. ", "original_text": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature. "}, "hash": "9e4dd3b48713755c78184bf3dee7be329e4dbab927ac57a04f429569f5507514", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39b32f83-2eb3-455d-98e8-13711e0b25f0", "node_type": "1", "metadata": {"window": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar. ", "original_text": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n"}, "hash": "46bfe0d993c792916d0659cab4609b1547b8739e7e327272149581c4c1fecdbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This preprocessing is not necessary for IF because splittings are made along a single feature. ", "mimetype": "text/plain", "start_char_idx": 22292, "end_char_idx": 22387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39b32f83-2eb3-455d-98e8-13711e0b25f0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar. ", "original_text": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "405e4802-e27d-4e57-87df-74706433d4c4", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. ", "original_text": "This preprocessing is not necessary for IF because splittings are made along a single feature. "}, "hash": "bfbe101bc898e1e52c76b6adb41c9acaddc36d98cd8d40dd6a302de9b98afa9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f8c45c2-3920-4947-9213-df926304e5db", "node_type": "1", "metadata": {"window": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. ", "original_text": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. "}, "hash": "1f17221c788aa66ca6bf9b8ab0e6431951da1d4403270a0f5d2655f95186b539", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n", "mimetype": "text/plain", "start_char_idx": 22387, "end_char_idx": 22507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f8c45c2-3920-4947-9213-df926304e5db", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. ", "original_text": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39b32f83-2eb3-455d-98e8-13711e0b25f0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "7:** Comparison of PR AUC for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar. ", "original_text": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n"}, "hash": "3211721fa2684456e21b7f439f7a50bf75d8b15e8d98eee05dd32971cd9376b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd3dab00-9060-42fd-b184-d12cbfdfe0b8", "node_type": "1", "metadata": {"window": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n", "original_text": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. "}, "hash": "94d1cc1c3809640f2401aa8112686513c370f8821ff8277293cf8cec805a3358", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. ", "mimetype": "text/plain", "start_char_idx": 22507, "end_char_idx": 22768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd3dab00-9060-42fd-b184-d12cbfdfe0b8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n", "original_text": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f8c45c2-3920-4947-9213-df926304e5db", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "made using Python, with the IF algorithm from scikit learn\u00b2, EIF from the author\u2019s github\u00b3, and our own implementation of GIF.  The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. ", "original_text": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF. "}, "hash": "2695ed1e7b1ff64dfa8eef414988fc9592ad3faef97bdfa87869cf75cfb60b39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02cccd92-72b4-418b-a978-c5cfbd08360e", "node_type": "1", "metadata": {"window": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. ", "original_text": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. "}, "hash": "6b9d467f4b0f4f808427899e0bd42983e5a0aaeec89156eb9c960bd76eaa5f1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. ", "mimetype": "text/plain", "start_char_idx": 22768, "end_char_idx": 22863, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "02cccd92-72b4-418b-a978-c5cfbd08360e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. ", "original_text": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd3dab00-9060-42fd-b184-d12cbfdfe0b8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The whole code as long as the datasets are available on the first author\u2019s webpage\u2074.  Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n", "original_text": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid. "}, "hash": "69145c4f548d2260ae4f6df4dd2bca8f3b5260313a49574b4e21bdbbd616bca2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7536f385-64bd-4a29-8268-332d43895ca2", "node_type": "1", "metadata": {"window": "This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. ", "original_text": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. "}, "hash": "fcccc1c7999fe15ae5b91e4ffd95bd737e14ac9b87e12430f6d4f2ffa5d7bc72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. ", "mimetype": "text/plain", "start_char_idx": 22863, "end_char_idx": 22998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7536f385-64bd-4a29-8268-332d43895ca2", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. ", "original_text": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02cccd92-72b4-418b-a978-c5cfbd08360e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Note that all datasets have been preprocessed in order to obtain zero mean and unit variance for each feature.  This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. ", "original_text": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF. "}, "hash": "fb7f5a9ad4716de443753dbe56d35d3e9853e3897841eac43b6f27bc5e30cf7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e00ea31-ee89-49a1-8e79-cb927f869e0b", "node_type": "1", "metadata": {"window": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. ", "original_text": "From these experiments, we conclude that the performances of EIF and GIF are globally similar. "}, "hash": "03bd7d102a5be375519eb1f9941af9da548995b9f7bfe07fd08f1b5e08348837", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. ", "mimetype": "text/plain", "start_char_idx": 22998, "end_char_idx": 23080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e00ea31-ee89-49a1-8e79-cb927f869e0b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. ", "original_text": "From these experiments, we conclude that the performances of EIF and GIF are globally similar. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7536f385-64bd-4a29-8268-332d43895ca2", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This preprocessing is not necessary for IF because splittings are made along a single feature.  However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. ", "original_text": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere. "}, "hash": "7fedcae4cb9cca883d697696d919c0d935728c9942493fb65891d8d3b6850b6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4093585-48bd-45cd-adbc-db76f0ca9a73", "node_type": "1", "metadata": {"window": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. ", "original_text": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. "}, "hash": "fbb9d420eee83dda314d64367fac338591889f61be57035deb52380bb3b7abf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From these experiments, we conclude that the performances of EIF and GIF are globally similar. ", "mimetype": "text/plain", "start_char_idx": 23080, "end_char_idx": 23175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4093585-48bd-45cd-adbc-db76f0ca9a73", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. ", "original_text": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e00ea31-ee89-49a1-8e79-cb927f869e0b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, they are useful for EIF ad GIF especially in high dimensions since these algorithms are sensitive to scaling.\n\n As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. ", "original_text": "From these experiments, we conclude that the performances of EIF and GIF are globally similar. "}, "hash": "867cd723bafba221ffe204a7e30815122ad7296ae3a28656b28aa50d5b297d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1601062b-1d86-4837-8fbd-aecdd8d10304", "node_type": "1", "metadata": {"window": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. ", "original_text": "The results are shown in Figs 8 and 9.\n\n"}, "hash": "8e6c048dd9bf852b62998899d8db1bf3bc40ee1a5e065e58d76435455f80d1e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. ", "mimetype": "text/plain", "start_char_idx": 23175, "end_char_idx": 23489, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1601062b-1d86-4837-8fbd-aecdd8d10304", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. ", "original_text": "The results are shown in Figs 8 and 9.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4093585-48bd-45cd-adbc-db76f0ca9a73", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, there is not a significant difference between EIF and GIF in terms of ROC AUC, except for the datasets Ionosphere, Satellite, Pen Global, and Letter, where EIF seems to give a better result, and datasets Forest Cover and Cardio in favor of GIF.  EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. ", "original_text": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest. "}, "hash": "58c93b14b5d8b8f4b0d962538de7eff40542f49bc5a59dc8f5f613c2bba3498d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7d1bc2a-54c9-4685-80ed-42c7f1dbb19f", "node_type": "1", "metadata": {"window": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. ", "original_text": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. "}, "hash": "3a0b387951368c2229f112fd20886f095314180ceb092d355ab7fe8c5e419f84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results are shown in Figs 8 and 9.\n\n", "mimetype": "text/plain", "start_char_idx": 23489, "end_char_idx": 23529, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7d1bc2a-54c9-4685-80ed-42c7f1dbb19f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. ", "original_text": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1601062b-1d86-4837-8fbd-aecdd8d10304", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "EIF and GIF also provide good results when compared to IF, except for the dataset ANN Thyroid.  Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. ", "original_text": "The results are shown in Figs 8 and 9.\n\n"}, "hash": "71a554580d76f78656e0f229f49e2038f656ffefb6ebd6ab20ca1c0e82deafcf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a33066b9-3e38-4b17-9f90-e16fef514539", "node_type": "1", "metadata": {"window": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n", "original_text": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. "}, "hash": "8aa0989dca87ebda7340e3b101f4d4a71123e475b72a7b88efd683154cf80181", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. ", "mimetype": "text/plain", "start_char_idx": 23529, "end_char_idx": 23674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a33066b9-3e38-4b17-9f90-e16fef514539", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n", "original_text": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7d1bc2a-54c9-4685-80ed-42c7f1dbb19f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Regarding PR AUC, EIF and GIF seem to have the same behavior, except for datasets Forest Cover and Shuttle, where GIF outperforms EIF.  Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. ", "original_text": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals. "}, "hash": "55eff2fc51ca4c41148807d73847ee46b189e5360ff41f52909112420753e6f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b0005e4-1d22-4381-861f-7cca5d201986", "node_type": "1", "metadata": {"window": "From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig. ", "original_text": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. "}, "hash": "f495ec861ad2c4d087eb4bb1f5b3ac885cebc00a1671e0eb505baefda8e151c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. ", "mimetype": "text/plain", "start_char_idx": 23674, "end_char_idx": 23790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b0005e4-1d22-4381-861f-7cca5d201986", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig. ", "original_text": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a33066b9-3e38-4b17-9f90-e16fef514539", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Finally note that EIF performs better than IF and GIF for the dataset Ionosphere.  From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n", "original_text": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data. "}, "hash": "1f6e320f8d6d89aed03bde34e3b2cbecba3cbfdded7831d9cb9e07f4f0eb7ec6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b3a4f43-d49a-47c9-8dd6-57a5692f2e4c", "node_type": "1", "metadata": {"window": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. "}, "hash": "d406358d5ee3d2cbda0cbb30fa568dd8fc438cf4f6b2afa18afb67be473d4feb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. ", "mimetype": "text/plain", "start_char_idx": 23790, "end_char_idx": 23892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b3a4f43-d49a-47c9-8dd6-57a5692f2e4c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b0005e4-1d22-4381-861f-7cca5d201986", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "From these experiments, we conclude that the performances of EIF and GIF are globally similar.  In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig. ", "original_text": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data. "}, "hash": "cb42bd19f02d24c712aa18b826628e3f1591856029e473dfc4613a4ed32e6589", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21d0abe4-c1be-4b26-bea2-93b9b2a2fc3c", "node_type": "1", "metadata": {"window": "The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth. ", "original_text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. "}, "hash": "c5192296003973c024f4d75e147c43a915a0479654b9a4dd79aac13907831ebb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. ", "mimetype": "text/plain", "start_char_idx": 23892, "end_char_idx": 24213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21d0abe4-c1be-4b26-bea2-93b9b2a2fc3c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth. ", "original_text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b3a4f43-d49a-47c9-8dd6-57a5692f2e4c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In order to appreciate the interest of GIF, we have compared the execution times of the different algorithms, i.e., the time required to produce the forest (for both EIF and GIF) and the average proportion of external nodes at the maximum depth among all the external nodes computed for all the trees of a forest.  The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time. "}, "hash": "b8a321eb1794d275145dd9a7f94179fa0192a65119fd42b0dc5b7f703ca91f5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49c5076e-c9c3-465e-88f3-4f048b47f774", "node_type": "1", "metadata": {"window": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. ", "original_text": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. "}, "hash": "da1ae083a15448dd1b179d628a3ae4438ea31091ad4a31526d9c432b59bc67a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. ", "mimetype": "text/plain", "start_char_idx": 24213, "end_char_idx": 24349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49c5076e-c9c3-465e-88f3-4f048b47f774", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. ", "original_text": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21d0abe4-c1be-4b26-bea2-93b9b2a2fc3c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The results are shown in Figs 8 and 9.\n\n As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth. ", "original_text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval. "}, "hash": "eeee5504c7f1a6d6b3090afaa5bbfb30be8865bed136a217c9fa28330b92689f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44ae4580-c05c-4d00-bf2c-c170ad4ae436", "node_type": "1", "metadata": {"window": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n"}, "hash": "b533ae51f23e85fdeddcc3203485cf6336e5e5fc97447cd7ac87173b3c32b519", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. ", "mimetype": "text/plain", "start_char_idx": 24349, "end_char_idx": 24423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44ae4580-c05c-4d00-bf2c-c170ad4ae436", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49c5076e-c9c3-465e-88f3-4f048b47f774", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, the times to compute the forests are significantly smaller for GIF compared to EIF, with generally smaller confidence intervals.  The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. ", "original_text": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars. "}, "hash": "cf3ec841fb906189de66b9c47ea54bae3897a42258e418108a0cf93af92e81fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e093ea8a-c3f2-4771-9177-20d335ea15be", "node_type": "1", "metadata": {"window": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n", "original_text": "**Fig. "}, "hash": "b959d07600d1b22a42c03e02ccbc17420a5813a7920846e7b5e25efa72998d87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n", "mimetype": "text/plain", "start_char_idx": 24423, "end_char_idx": 24503, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e093ea8a-c3f2-4771-9177-20d335ea15be", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44ae4580-c05c-4d00-bf2c-c170ad4ae436", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The mean proportion of limit nodes among all the external nodes shows the capability of the method to isolate data.  Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n"}, "hash": "632b3d974dd516f1e631bcc7a1bc9162ffdcccd9cd4359cfdca9c513f32e06be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "995c5e9f-5ef4-4d3c-aa75-daf4806b3af3", "node_type": "1", "metadata": {"window": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig. ", "original_text": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "9e2a52070ed5a6cd95f39bfd05f6afa3fed117ba97ef132df5bf38a614af931c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 24503, "end_char_idx": 24510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "995c5e9f-5ef4-4d3c-aa75-daf4806b3af3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig. ", "original_text": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e093ea8a-c3f2-4771-9177-20d335ea15be", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, an external node is either due to a reach of the given maximal depth, or to an isolated data.  Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n", "original_text": "**Fig. "}, "hash": "0fc2a966dca359a577cb792022c2eb28ff8100456f096d2df6cceb2212ac1fa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d5b32da-3f51-457c-8238-c8d6825b3cf0", "node_type": "1", "metadata": {"window": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth. "}, "hash": "7b0de01dd287864ae773e18be07210da93cfe9044b56968cedf536cfb29f2616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "mimetype": "text/plain", "start_char_idx": 24510, "end_char_idx": 24661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d5b32da-3f51-457c-8238-c8d6825b3cf0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "995c5e9f-5ef4-4d3c-aa75-daf4806b3af3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Therefore, if this number is close to one, few data are isolated\n\n---\n\u00b9The datasets can be downloaded from http://odds.cs.stonybrook.edu/\n\u00b2https://scikit-learn.org/stable/\n\u00b3https://github.com/sahandha/eif\n\u2074http://perso.tesa.prd.fr/jlesouple/codes.html\n\n---\n\n[Image: A horizontal bar chart comparing forest creation time.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig. ", "original_text": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "08733aa864bb98fa6940ddf71ac8e34698bbce12d1554408427dacfa7e5953bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b014c7a-e0cb-4c42-b0c8-b42b6bd517a1", "node_type": "1", "metadata": {"window": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). ", "original_text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. "}, "hash": "463d72bca6f20ee3ff68f5d2de792040024b818300d16ad097f243c0cbbba9d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth. ", "mimetype": "text/plain", "start_char_idx": 24661, "end_char_idx": 24749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b014c7a-e0cb-4c42-b0c8-b42b6bd517a1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). ", "original_text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d5b32da-3f51-457c-8238-c8d6825b3cf0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean time in seconds and a 95% confidence interval.  The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "original_text": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth. "}, "hash": "f504274f4d18053659f682e54ffba1b8597c996f7f96893018fee2bfe6815ba8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7df4072-6d77-40f5-a537-cedd931338aa", "node_type": "1", "metadata": {"window": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. ", "original_text": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars. "}, "hash": "ac598d7a34dac7fe94570a4d138a6f4e3096393815311ae4a42703c702d8aa56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. ", "mimetype": "text/plain", "start_char_idx": 24749, "end_char_idx": 24880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7df4072-6d77-40f5-a537-cedd931338aa", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. ", "original_text": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b014c7a-e0cb-4c42-b0c8-b42b6bd517a1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The \"Generalized\" bars are consistently shorter than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). ", "original_text": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval. "}, "hash": "7b939b2e60c1946bd07f3a3439d685d3490ce4f5d8f1e3fb9a700200ea8f23c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a73cb8d-785f-44fe-af5f-3a92b9c9a91c", "node_type": "1", "metadata": {"window": "**Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n"}, "hash": "32c57696aa7b08bb990d9c0f668415458b57b3b6f1924df41bbf28afb1109d78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars. ", "mimetype": "text/plain", "start_char_idx": 24880, "end_char_idx": 24951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a73cb8d-785f-44fe-af5f-3a92b9c9a91c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7df4072-6d77-40f5-a537-cedd931338aa", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"Forest creation time [s]\".]\n\n **Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. ", "original_text": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars. "}, "hash": "b02c796566ba05de94f20ec1ad71b363255d4cd8ce0ddfe538c63fc6b40e6b17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b993d070-0561-4b80-8cf6-89b37fe56a43", "node_type": "1", "metadata": {"window": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n", "original_text": "**Fig. "}, "hash": "9a776abadfbfca454038b68953f75be81d1148b6f302ef3785c575c6cba076b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n", "mimetype": "text/plain", "start_char_idx": 24951, "end_char_idx": 25048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b993d070-0561-4b80-8cf6-89b37fe56a43", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a73cb8d-785f-44fe-af5f-3a92b9c9a91c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig. ", "original_text": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n"}, "hash": "e38d71b505dde10e56e770090c243a69576e6a76c67d047444ea6bd8903db295", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa56b9e4-7fd3-4cb4-bcd3-360c94ebcfbd", "node_type": "1", "metadata": {"window": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3. ", "original_text": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "438b2471eee9e54a7ea1febcb478bd65ef0a0868b6d22d507d6559717e60de48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 25048, "end_char_idx": 25055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fa56b9e4-7fd3-4cb4-bcd3-360c94ebcfbd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3. ", "original_text": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b993d070-0561-4b80-8cf6-89b37fe56a43", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "8:** Comparison of EIF and GIF computation times for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n [Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n", "original_text": "**Fig. "}, "hash": "36781be896f061aff927123dcd2dd5bbbb1a3efdb09a6e82183286ffd7f73ef6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1aec315-51b5-4286-9c9b-05f79e9f02fe", "node_type": "1", "metadata": {"window": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. ", "original_text": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). "}, "hash": "1d8c671a0870548726b9d6dfadeae97a8446f047068b9f8d104ad922d5715412", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n", "mimetype": "text/plain", "start_char_idx": 25055, "end_char_idx": 25219, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b1aec315-51b5-4286-9c9b-05f79e9f02fe", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. ", "original_text": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa56b9e4-7fd3-4cb4-bcd3-360c94ebcfbd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "[Image: A horizontal bar chart comparing the proportion of external nodes at max depth.  For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3. ", "original_text": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n"}, "hash": "e2501090b3301039210e3808fcd064db062bd18040ed3670bd3ec2ff4bdbb9c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ace95df4-9e60-4433-b8de-6166e841b58b", "node_type": "1", "metadata": {"window": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al. ", "original_text": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. "}, "hash": "b3cf9a84a8761750ce97f316ccd7d323ac72d653578cc4a5bf668931105c0ec2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). ", "mimetype": "text/plain", "start_char_idx": 25219, "end_char_idx": 25325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ace95df4-9e60-4433-b8de-6166e841b58b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al. ", "original_text": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1aec315-51b5-4286-9c9b-05f79e9f02fe", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each dataset, there are two horizontal bars (Extended, Generalized) showing the mean proportion and a 95% confidence interval.  The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. ", "original_text": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method). "}, "hash": "876012dd5d1177276e7155a44d9ff2a07a92f0bcb9e8de60fac249610f2b270d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cabb7dd5-55f8-48b1-be41-4395b35afd74", "node_type": "1", "metadata": {"window": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. ", "original_text": "As one can see in Fig. "}, "hash": "e1fcd1c952641a9d844807927de58d11206fbe6e80470394d7066df93dfb869a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. ", "mimetype": "text/plain", "start_char_idx": 25325, "end_char_idx": 25425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cabb7dd5-55f8-48b1-be41-4395b35afd74", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. ", "original_text": "As one can see in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ace95df4-9e60-4433-b8de-6166e841b58b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The \"Generalized\" bars are generally smaller than the \"Extended\" bars.  The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al. ", "original_text": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible. "}, "hash": "4f33821c9f24ff99e74012c18589554e21e09c2b7ac846393d6cac6f79ba8e08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbb71f8f-5dd2-4179-947b-1b1b6ce4a1ba", "node_type": "1", "metadata": {"window": "**Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. ", "original_text": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n"}, "hash": "874220b79a301399524845aa34be840a7a32abf7ca614bd9be4f5d8028b2db14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As one can see in Fig. ", "mimetype": "text/plain", "start_char_idx": 25425, "end_char_idx": 25448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbb71f8f-5dd2-4179-947b-1b1b6ce4a1ba", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. ", "original_text": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cabb7dd5-55f8-48b1-be41-4395b35afd74", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The y-axis lists dataset names, and the x-axis is \"Proportion of external nodes at max depth\".]\n\n **Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. ", "original_text": "As one can see in Fig. "}, "hash": "44aab97154b9b1e16ae891eef04e0247f1057f9596cbb4806d7c88930140cae4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12a24441-04ea-48db-8c04-b088b20634c4", "node_type": "1", "metadata": {"window": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. ", "original_text": "#### 3.3. "}, "hash": "b3d4537cc970a447adf1b430f92d2d268e45aa1ecebfc6e4151a18649efa66d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n", "mimetype": "text/plain", "start_char_idx": 25448, "end_char_idx": 25559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "12a24441-04ea-48db-8c04-b088b20634c4", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. ", "original_text": "#### 3.3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbb71f8f-5dd2-4179-947b-1b1b6ce4a1ba", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. ", "original_text": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n"}, "hash": "2cc82fd246f54f0b17f15e578e1bd9797e87e277d672d88456b6391910ed3521", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fd1a598-40b0-485c-a891-bc313148cbe6", "node_type": "1", "metadata": {"window": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10. ", "original_text": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. "}, "hash": "1df6ea360b7da806a3fe0c4b2b48828386869d94de09c3d2f0caaf832d6d64ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 3.3. ", "mimetype": "text/plain", "start_char_idx": 25559, "end_char_idx": 25569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5fd1a598-40b0-485c-a891-bc313148cbe6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10. ", "original_text": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12a24441-04ea-48db-8c04-b088b20634c4", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "9:** Comparison of external nodes at maximum depth proportion for several datasets (a line represents a 95% confidence interval and a dot the corresponding mean).\n\n (and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. ", "original_text": "#### 3.3. "}, "hash": "5baddf757a8aac3f179bfe5ee1d232fe0bb7afecece675ce19ac98b0de0c0de8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57fc175e-57a5-4d01-83ea-fcb21c04d058", "node_type": "1", "metadata": {"window": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. ", "original_text": "1, authors in Hariri et al. "}, "hash": "38d6204cd5ee4ab7f4907802a7320be4a13bb09ff559f1d41a30f7dd61e38957", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. ", "mimetype": "text/plain", "start_char_idx": 25569, "end_char_idx": 25656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57fc175e-57a5-4d01-83ea-fcb21c04d058", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. ", "original_text": "1, authors in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fd1a598-40b0-485c-a891-bc313148cbe6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(and conversely, the lower the mean proportion of limit nodes, the more data are isolated by the method).  As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10. ", "original_text": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig. "}, "hash": "d92113c34e23034327ae940df96d3a6a4054b062e79634086629681c49eca64e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db067ee6-c3cd-4a86-9ab7-e40b75c8ad3e", "node_type": "1", "metadata": {"window": "As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. ", "original_text": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. "}, "hash": "b358dbc0f0ca0949f0f94fbf465fa67df67d7975227dfcbafb8e1290235ad2cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, authors in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 25656, "end_char_idx": 25684, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db067ee6-c3cd-4a86-9ab7-e40b75c8ad3e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. ", "original_text": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57fc175e-57a5-4d01-83ea-fcb21c04d058", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As the purpose of IF methods is precisely to isolate data, this ratio should be as low as possible.  As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. ", "original_text": "1, authors in Hariri et al. "}, "hash": "3f8f0838a26579201b068e9d15cb9c5df866efef247840cfc9698a0e090e5adf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "212420c8-9cc6-497c-911b-688747364b40", "node_type": "1", "metadata": {"window": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n", "original_text": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. "}, "hash": "7398a9e94adc5d952b77cd49ba6852210c3cd30ecfa2ea46c424a0d78c5a37c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. ", "mimetype": "text/plain", "start_char_idx": 25684, "end_char_idx": 25788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "212420c8-9cc6-497c-911b-688747364b40", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n", "original_text": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db067ee6-c3cd-4a86-9ab7-e40b75c8ad3e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see in Fig.  9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. ", "original_text": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data. "}, "hash": "e6e27b53663f8d4fa6d10feba018beee08717d2df431aec07748e3ecdd5c6f8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc334825-158b-4f3a-a712-85996f6bcc8a", "node_type": "1", "metadata": {"window": "#### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig. ", "original_text": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. "}, "hash": "466ee14de9cc2b14d58ffcaebf0351aaf091efb1cade749c24c2d998474d37b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. ", "mimetype": "text/plain", "start_char_idx": 25788, "end_char_idx": 25912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc334825-158b-4f3a-a712-85996f6bcc8a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "#### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig. ", "original_text": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "212420c8-9cc6-497c-911b-688747364b40", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "9, GIF leads to smaller proportions of this ratio than EIF (except for the Aloi dataset), which was expected.\n\n #### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n", "original_text": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean. "}, "hash": "6bf2021e5451434cd1d94384cca302859d9684d8bd5d23735d8a7cf177687742", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b92a2bef-4fa1-49fa-9147-39305aadd63d", "node_type": "1", "metadata": {"window": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. ", "original_text": "10. "}, "hash": "e14b2695b7871226dd538468c45fbfab1062a902c5d4ee746a224dbf6a07bf93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 25912, "end_char_idx": 26108, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b92a2bef-4fa1-49fa-9147-39305aadd63d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. ", "original_text": "10. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc334825-158b-4f3a-a712-85996f6bcc8a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "#### 3.3.  Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig. ", "original_text": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig. "}, "hash": "7dae0489f7c5122a732049bc3d14965863f237c8544d792b97081582c6cb59c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a11b40d5-a2bd-4475-be31-31d0de0f6bc7", "node_type": "1", "metadata": {"window": "1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n", "original_text": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. "}, "hash": "4423dc33b8129cecdd2200773f77c83bd5d0e94a2bbee6b63520d4f5d62038d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10. ", "mimetype": "text/plain", "start_char_idx": 26108, "end_char_idx": 26112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a11b40d5-a2bd-4475-be31-31d0de0f6bc7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n", "original_text": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b92a2bef-4fa1-49fa-9147-39305aadd63d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomaly scores\n\nTo assess the robustness of the EIF to the artefacts presented in Fig.  1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. ", "original_text": "10. "}, "hash": "fe94a21fb72e72b097f01bdecdc2cbd931c17903ca9b8471b824847207d16952", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c546c8eb-50a1-41d3-ab77-984bb2a9ae2b", "node_type": "1", "metadata": {"window": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. ", "original_text": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. "}, "hash": "39a668e6ed6742905a16f2d8c633a7d101f2022f9006ad8b14690b389321c733", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. ", "mimetype": "text/plain", "start_char_idx": 26112, "end_char_idx": 26246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c546c8eb-50a1-41d3-ab77-984bb2a9ae2b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. ", "original_text": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a11b40d5-a2bd-4475-be31-31d0de0f6bc7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1, authors in Hariri et al.  (2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n", "original_text": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center. "}, "hash": "b1dbe0589ecfe49e725ab0809fdcd4e45759dae77bd6d9cf208eb39fce30e85f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51892291-e5e0-465c-8e99-96890af08a8f", "node_type": "1", "metadata": {"window": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n", "original_text": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n"}, "hash": "4834f03c2c8a593718a8bb0ed7abb58cda00219b9a6d4a298e373def0471ec58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. ", "mimetype": "text/plain", "start_char_idx": 26246, "end_char_idx": 26333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51892291-e5e0-465c-8e99-96890af08a8f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n", "original_text": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c546c8eb-50a1-41d3-ab77-984bb2a9ae2b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) proposed to analyze the anomaly scores of the algorithm when applied to isotropic Gaussian data.  Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. ", "original_text": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles. "}, "hash": "51db989cf28752d28c0708f4081c7f1161b79f46b36bc060dbe203ef77c2a6db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6469a8b-ab28-4ef8-8cc1-94d469542e78", "node_type": "1", "metadata": {"window": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. ", "original_text": "**Fig. "}, "hash": "922779b40d6031f6f133c1eea82051888f6a13d49af402b591f5fb9470aec819", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n", "mimetype": "text/plain", "start_char_idx": 26333, "end_char_idx": 26424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e6469a8b-ab28-4ef8-8cc1-94d469542e78", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. ", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51892291-e5e0-465c-8e99-96890af08a8f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Indeed, for such data, the anomaly score should remain almost the same for data located at the same distance from the mean.  To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n", "original_text": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n"}, "hash": "ea855077cb67bbe91401cf459656a6579cebe7c0a18cda9149f772ed8a525f59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c7984d0-0e82-4cb9-86a3-1c3ffb9d01c8", "node_type": "1", "metadata": {"window": "10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. ", "original_text": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. "}, "hash": "3c3ca0f5dcaed4f1528e9b84f75a25ce4e102718f7e964b53307e31403f76ddc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 26424, "end_char_idx": 26431, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c7984d0-0e82-4cb9-86a3-1c3ffb9d01c8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. ", "original_text": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6469a8b-ab28-4ef8-8cc1-94d469542e78", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "To extend these experiments to the proposed method, IF, EIF and GIF were trained on the 2D single blob synthetic dataset, and testing points were generated around constant radii, as shown in Fig.  10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. ", "original_text": "**Fig. "}, "hash": "3c0c7f9c899b4d971152aca3ce018f20838ab51c568eb67258fb8f4fe4f27fd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c98e122d-8ee6-4267-af60-f76ca3fa761e", "node_type": "1", "metadata": {"window": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al. ", "original_text": "The red circles represent 1, 2 and 3 data standard deviations.\n\n"}, "hash": "6050702aaf3d570a8bf85c9c43fa69e6410106fd15343d822d360d4e4ad3c330", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. ", "mimetype": "text/plain", "start_char_idx": 26431, "end_char_idx": 26557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c98e122d-8ee6-4267-af60-f76ca3fa761e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al. ", "original_text": "The red circles represent 1, 2 and 3 data standard deviations.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c7984d0-0e82-4cb9-86a3-1c3ffb9d01c8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "10.  The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. ", "original_text": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center. "}, "hash": "b2ac2c8acfdbf44e0ac74f311e132d0e048e7e3644ff8cfc63266531094c6d50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94073e5e-a2b2-4d28-8234-f198fc22a133", "node_type": "1", "metadata": {"window": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. ", "original_text": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. "}, "hash": "821c99d4ada240cfb03d8ce596c1951e90130f4240c77b335e858817d35e0519", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The red circles represent 1, 2 and 3 data standard deviations.\n\n", "mimetype": "text/plain", "start_char_idx": 26557, "end_char_idx": 26621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94073e5e-a2b2-4d28-8234-f198fc22a133", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. ", "original_text": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c98e122d-8ee6-4267-af60-f76ca3fa761e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The mean scores versus con-\n\n[Image: A 2D scatter plot showing a dense circular cluster of blue \"Training Data\" points at the center.  Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al. ", "original_text": "The red circles represent 1, 2 and 3 data standard deviations.\n\n"}, "hash": "b0a00d767b88453de7ebfe507fe1d00f80799ffc8f767812e4f1b960b56fe90d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "912723f0-5e0f-4481-a169-b651d6adcfaa", "node_type": "1", "metadata": {"window": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. ", "original_text": "11a.\n\n"}, "hash": "03e16ea3dbe9f262498cc51d68e692c956d682271c099287c354dbf45bc16faf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. ", "mimetype": "text/plain", "start_char_idx": 26621, "end_char_idx": 26723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "912723f0-5e0f-4481-a169-b651d6adcfaa", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. ", "original_text": "11a.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94073e5e-a2b2-4d28-8234-f198fc22a133", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Surrounding the cluster are gray \"Testing data\" points arranged in concentric circles.  Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. ", "original_text": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig. "}, "hash": "527e563de522525cb7ffbbee9c08f7457cd4c2bae8acd1875f96a567fbd162b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23468c44-a529-4353-ab27-3d82b6f5c0b7", "node_type": "1", "metadata": {"window": "**Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n", "original_text": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. "}, "hash": "f15ac72c3c55c0163a4bf11cb730489732bb3034f4af1ae42bba53b8072737a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11a.\n\n", "mimetype": "text/plain", "start_char_idx": 26723, "end_char_idx": 26729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23468c44-a529-4353-ab27-3d82b6f5c0b7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n", "original_text": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "912723f0-5e0f-4481-a169-b651d6adcfaa", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Three red circles are drawn to indicate 1, 2, and 3 standard deviations from the center.]\n\n **Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. ", "original_text": "11a.\n\n"}, "hash": "067772ecf29c74b005c7af8d489b43db59041c09fd80fc73a253adb995ef0f15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27d2e239-52c8-4ac3-a6fe-53f9a64d4ff3", "node_type": "1", "metadata": {"window": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al. ", "original_text": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. "}, "hash": "9881c7dfbc05ea828d050dbc698aad53bee0f4b54dc8c3c0297702a903c63901", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. ", "mimetype": "text/plain", "start_char_idx": 26729, "end_char_idx": 26933, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "27d2e239-52c8-4ac3-a6fe-53f9a64d4ff3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al. ", "original_text": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23468c44-a529-4353-ab27-3d82b6f5c0b7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n", "original_text": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards. "}, "hash": "e2bd2cff31c9ef2953d758fedad21d79939d5b6bd4937cb9dcd82842839fd431", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "918153e0-332d-428c-bd43-c494c2a218f8", "node_type": "1", "metadata": {"window": "The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019). ", "original_text": "These results were already shown in Hariri et al. "}, "hash": "2deb487a405aad2971b12a6b10f2b97370fe25de040485c87f7efd7b52397131", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. ", "mimetype": "text/plain", "start_char_idx": 26933, "end_char_idx": 27120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "918153e0-332d-428c-bd43-c494c2a218f8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019). ", "original_text": "These results were already shown in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27d2e239-52c8-4ac3-a6fe-53f9a64d4ff3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "10:** Single blob with additional concentric testing data to compute mean statistics for a given distance to the blob center.  The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al. ", "original_text": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset. "}, "hash": "e92a7c341738001745b69bcd2c17f91225a156face21afb599fb146da81155dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7c8b5b2-173c-4439-929b-af79fd32d951", "node_type": "1", "metadata": {"window": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. ", "original_text": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. "}, "hash": "b6371be424b4af537d01714a3278399305ddfb0a19dd33d273aa4ac575ca5e56", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These results were already shown in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 27120, "end_char_idx": 27170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7c8b5b2-173c-4439-929b-af79fd32d951", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. ", "original_text": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "918153e0-332d-428c-bd43-c494c2a218f8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The red circles represent 1, 2 and 3 data standard deviations.\n\n stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019). ", "original_text": "These results were already shown in Hariri et al. "}, "hash": "0ecbf65ae1fa101265a5bb10f221e5be62809cb5cca2a023fa287fac26c4ce71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7063baa-f5cc-4d22-9584-f9f79c311266", "node_type": "1", "metadata": {"window": "11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs. ", "original_text": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. "}, "hash": "6ec31b574688c8ff199617df1711794a62159334642b65800868784165eac9ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. ", "mimetype": "text/plain", "start_char_idx": 27170, "end_char_idx": 27346, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7063baa-f5cc-4d22-9584-f9f79c311266", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs. ", "original_text": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7c8b5b2-173c-4439-929b-af79fd32d951", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "stant radius and the corresponding standard deviations are plotted for the various algorithms in Fig.  11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. ", "original_text": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees. "}, "hash": "347e546d3da7f8d58a516ca8bd228fa4328af1b7a7185a6e0db0332d5359bad7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a01d5b9-1fa1-45c0-89b3-6b6800c1b65e", "node_type": "1", "metadata": {"window": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. ", "original_text": "11b and 11c leading to the same conclusions.\n\n"}, "hash": "fe46a3458cea8523aaa3d9f2ca0d2427953d4ad9f00463d353e17ed5715305de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. ", "mimetype": "text/plain", "start_char_idx": 27346, "end_char_idx": 27422, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a01d5b9-1fa1-45c0-89b3-6b6800c1b65e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. ", "original_text": "11b and 11c leading to the same conclusions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7063baa-f5cc-4d22-9584-f9f79c311266", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "11a.\n\n As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs. ", "original_text": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs. "}, "hash": "d36404c922df751a6918db23b3e88469637a54420535f0260068ba4e23d23eea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72d13484-efda-4de5-8e9c-c75744568095", "node_type": "1", "metadata": {"window": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. ", "original_text": "The convergence of the mean anomaly scores was also studied, as in Hariri et al. "}, "hash": "dea228528a79bf44d626a2616a2ec2a20a0106d14c378eac5c3922e80f975f42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11b and 11c leading to the same conclusions.\n\n", "mimetype": "text/plain", "start_char_idx": 27422, "end_char_idx": 27468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72d13484-efda-4de5-8e9c-c75744568095", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. ", "original_text": "The convergence of the mean anomaly scores was also studied, as in Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a01d5b9-1fa1-45c0-89b3-6b6800c1b65e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, the anomaly scores are equivalent for all the algorithms: there is a fast increase from zero to a value in the interval (2, 3) when the radius increases, and slower variations afterwards.  One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. ", "original_text": "11b and 11c leading to the same conclusions.\n\n"}, "hash": "590552c51f9c8405eccc0a1cde2d9bdbb5a3caf22b84e4ff3d7706775e654c1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71283f72-13b0-4516-b37a-989002996cc4", "node_type": "1", "metadata": {"window": "These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n", "original_text": "(2019). "}, "hash": "187495f83165c22a8f3b203b4b2125c6a0766f431d706b8598424ca5ecff3418", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The convergence of the mean anomaly scores was also studied, as in Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 27468, "end_char_idx": 27549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71283f72-13b0-4516-b37a-989002996cc4", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n", "original_text": "(2019). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72d13484-efda-4de5-8e9c-c75744568095", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "One can observe that the standard deviations of the scores are significantly larger for IF than for EIF and GIF, which is explained by the absence of the \u201ccross\u201d effect for this dataset.  These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. ", "original_text": "The convergence of the mean anomaly scores was also studied, as in Hariri et al. "}, "hash": "9312dc58da5b7de3177ee3877e73e491c252be72d794fff72130974f9f347dec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19cc7d8d-8ea1-42c0-ae3f-414b63fe412b", "node_type": "1", "metadata": {"window": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4. ", "original_text": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. "}, "hash": "ae2b9dd09034ea2fdad76a8919498cb11602dbe3978a18c25070e2118b283caf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019). ", "mimetype": "text/plain", "start_char_idx": 27549, "end_char_idx": 27557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19cc7d8d-8ea1-42c0-ae3f-414b63fe412b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4. ", "original_text": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71283f72-13b0-4516-b37a-989002996cc4", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "These results were already shown in Hariri et al.  (2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n", "original_text": "(2019). "}, "hash": "e61b877e2197d14c186d7f0c53488098aa46e697bbd0d1611c13d799fdab8540", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7adc0b7-8a50-4c62-8f1c-1ee867553773", "node_type": "1", "metadata": {"window": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. ", "original_text": "The results are depicted in Figs. "}, "hash": "38997012dac4b50acf445072ec88a788ec052ea0c38bcc415db273e1bac2e49f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. ", "mimetype": "text/plain", "start_char_idx": 27557, "end_char_idx": 27740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7adc0b7-8a50-4c62-8f1c-1ee867553773", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. ", "original_text": "The results are depicted in Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19cc7d8d-8ea1-42c0-ae3f-414b63fe412b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019) and are repeated here to show that the proposed GIF performs similarly to EIF, with the advantage of being faster, thanks to the absence of empty branches in the trees.  The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4. ", "original_text": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest. "}, "hash": "6c6a2bdd66a00ba008db51eccd95ca69e63f4b3e6286ec4116c85db9452eb159", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8b63164-6ff1-4ce9-8bd3-57f073c290bd", "node_type": "1", "metadata": {"window": "11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. ", "original_text": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. "}, "hash": "f05937f4c2119125427243c3f480794adbab5e3ff38d4f08abdc7e709d2ea255", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results are depicted in Figs. ", "mimetype": "text/plain", "start_char_idx": 27740, "end_char_idx": 27774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8b63164-6ff1-4ce9-8bd3-57f073c290bd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. ", "original_text": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7adc0b7-8a50-4c62-8f1c-1ee867553773", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The same experiments were run on a 3D blob and a 4D blob, as shown in Figs.  11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. ", "original_text": "The results are depicted in Figs. "}, "hash": "fe7c15a08f7eb8fb68dcef93e625b7d4e7e66723e943388a7fd0e9c7ed6d2b30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b932e5d-8c47-415e-913f-a72344470b0d", "node_type": "1", "metadata": {"window": "The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. ", "original_text": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. "}, "hash": "122f358a18ec7af0c0c695871bf30b10481d38c97761423a969b7507e4dc8679", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. ", "mimetype": "text/plain", "start_char_idx": 27774, "end_char_idx": 27833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b932e5d-8c47-415e-913f-a72344470b0d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. ", "original_text": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8b63164-6ff1-4ce9-8bd3-57f073c290bd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "11b and 11c leading to the same conclusions.\n\n The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. ", "original_text": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively. "}, "hash": "876e84d399af5be4523db0c42a07eeb146dcabbfc06642db1f5db590ed561682", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4c9d41e-9d94-458a-9b6c-c416748b7033", "node_type": "1", "metadata": {"window": "(2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs. ", "original_text": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n"}, "hash": "463eabdb992efe179612f652bb45742e497cd4d50590477f533fa34f7b162601", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. ", "mimetype": "text/plain", "start_char_idx": 27833, "end_char_idx": 27961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4c9d41e-9d94-458a-9b6c-c416748b7033", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs. ", "original_text": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b932e5d-8c47-415e-913f-a72344470b0d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The convergence of the mean anomaly scores was also studied, as in Hariri et al.  (2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. ", "original_text": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm. "}, "hash": "874b09d8760aade4d6c0f6e95c16501eb1bccccf57d0c9f86298c3c90c6803d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acadd02c-fdcf-42c8-8b0e-0eeb958ef902", "node_type": "1", "metadata": {"window": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs. ", "original_text": "### 4. "}, "hash": "303f7b34e254c6634d19cef71c5093ff0f6bb97f1a9f873a352b004ee437e553", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n", "mimetype": "text/plain", "start_char_idx": 27961, "end_char_idx": 28120, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "acadd02c-fdcf-42c8-8b0e-0eeb958ef902", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs. ", "original_text": "### 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4c9d41e-9d94-458a-9b6c-c416748b7033", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "(2019).  The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs. ", "original_text": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n"}, "hash": "d7011236f58a3a56676de608b5edabc93571ce20794c234e984071a74c0ff7eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f6a2739-53fb-4c95-9473-8616c5306b03", "node_type": "1", "metadata": {"window": "The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\". ", "original_text": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. "}, "hash": "a7a9e716304dcbb7752b8a72ec47f8ea82ab29388120545e48a512c0358e569d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4. ", "mimetype": "text/plain", "start_char_idx": 28120, "end_char_idx": 28127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f6a2739-53fb-4c95-9473-8616c5306b03", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\". ", "original_text": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acadd02c-fdcf-42c8-8b0e-0eeb958ef902", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The average anomaly scores for the inner an outer shell of each blob and the corresponding standard deviations were computed for each blob for various numbers of trees in the forest.  The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs. ", "original_text": "### 4. "}, "hash": "46cf651da3a671336145d4eaea5b1fe9bfbc9878262cfcc15ae1f933a72d49bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58aba838-0a33-4502-8848-9a25fd8ca131", "node_type": "1", "metadata": {"window": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. ", "original_text": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. "}, "hash": "727712bcfd4f17828666796dc4197b0870ee4e3d2b38864879de2be1decf0284", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 28127, "end_char_idx": 28259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58aba838-0a33-4502-8848-9a25fd8ca131", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. ", "original_text": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f6a2739-53fb-4c95-9473-8616c5306b03", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The results are depicted in Figs.  12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\". ", "original_text": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection. "}, "hash": "73187d21c4c7d09a063f926904a28467a691d0d44ea7d4096eae2ca99fcbd4a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e2f210a-fe04-4d01-a532-a7dbc50294df", "node_type": "1", "metadata": {"window": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend. ", "original_text": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. "}, "hash": "4091f7c3de2170d67d164e46a07d37621a918d83bb4113449338ad874e2ceccd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. ", "mimetype": "text/plain", "start_char_idx": 28259, "end_char_idx": 28444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e2f210a-fe04-4d01-a532-a7dbc50294df", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend. ", "original_text": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58aba838-0a33-4502-8848-9a25fd8ca131", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "12a, 12b and 12c for the 2D, 3D and 4D blobs respectively.  As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. ", "original_text": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm. "}, "hash": "d4452b5480a7c2d8716420d4e06984c50725623365333b828d586fbb86777d3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad9f80a7-83cb-43a6-a448-33be3f6025ad", "node_type": "1", "metadata": {"window": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n", "original_text": "For each blob dimension, the left chart plots \"Score Mean\" vs. "}, "hash": "f3a2a9ac7203c7c2657b4f865a5eb9ccebe938180355535c4a64e977a14a3999", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. ", "mimetype": "text/plain", "start_char_idx": 28444, "end_char_idx": 28643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad9f80a7-83cb-43a6-a448-33be3f6025ad", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n", "original_text": "For each blob dimension, the left chart plots \"Score Mean\" vs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e2f210a-fe04-4d01-a532-a7dbc50294df", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "As one can see, EIF and GIF provide similar results, with lower standard deviations when compared to the standard IF algorithm.  Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend. ", "original_text": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob. "}, "hash": "1cb06c2276f0991f9e010da416170f1300f2d091118c7b80b8921bf062085dda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbc489d4-d7ce-48e9-be48-1fa3203a75ad", "node_type": "1", "metadata": {"window": "### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig. ", "original_text": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs. "}, "hash": "c32c7cef44e9c3e2a12d7d8cd34e7a8583d7e1fcbd3436ec9e62472473348fe8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each blob dimension, the left chart plots \"Score Mean\" vs. ", "mimetype": "text/plain", "start_char_idx": 28643, "end_char_idx": 28706, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fbc489d4-d7ce-48e9-be48-1fa3203a75ad", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig. ", "original_text": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad9f80a7-83cb-43a6-a448-33be3f6025ad", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Moreover, the anomaly scores for EIF and GIF seem to converge to a constant value using a relatively small number of trees (around 100 trees in each forest).\n\n ### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n", "original_text": "For each blob dimension, the left chart plots \"Score Mean\" vs. "}, "hash": "de88407dd07511fba1e9eb6f8a071426bbc6c6cc29a0eb42955fba2663c5b0f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "865b966c-e1c5-4eb5-bbc0-00fe924dbc7b", "node_type": "1", "metadata": {"window": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. ", "original_text": "\"Radius\". "}, "hash": "113f707d2cc082fb54131472f813acc46e9bba4a8f282314d6588a030b7bbeb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs. ", "mimetype": "text/plain", "start_char_idx": 28706, "end_char_idx": 28773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "865b966c-e1c5-4eb5-bbc0-00fe924dbc7b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. ", "original_text": "\"Radius\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbc489d4-d7ce-48e9-be48-1fa3203a75ad", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### 4.  Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig. ", "original_text": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs. "}, "hash": "7febab0733c77591cd792a7dbcfc9a17e89b6a07f93ebb4464e0c74db9168e25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7852e73-58b3-4a80-b234-f5294c821129", "node_type": "1", "metadata": {"window": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n", "original_text": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. "}, "hash": "4b450785db450f8df75a068700cb8d1dec0c81c6261bf4e8d29196824b2269d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Radius\". ", "mimetype": "text/plain", "start_char_idx": 28773, "end_char_idx": 28783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7852e73-58b3-4a80-b234-f5294c821129", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n", "original_text": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "865b966c-e1c5-4eb5-bbc0-00fe924dbc7b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conclusion\n\nThis letter studied a new isolation forest algorithm referred to as generalized isolation forest for anomaly detection.  This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. ", "original_text": "\"Radius\". "}, "hash": "d37d1acd65468f5200e781c2eb9326cd3892f084450d2f80d950a31f9c8da4b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54f822e1-ad28-4c27-995a-03de5fce357f", "node_type": "1", "metadata": {"window": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. ", "original_text": "In the mean score plots, all three lines follow a similar increasing trend. "}, "hash": "7775db47ff175ffc6904613b575e454ea6f1b7227b18a4eee87bddc7346cb12c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. ", "mimetype": "text/plain", "start_char_idx": 28783, "end_char_idx": 28870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54f822e1-ad28-4c27-995a-03de5fce357f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. ", "original_text": "In the mean score plots, all three lines follow a similar increasing trend. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7852e73-58b3-4a80-b234-f5294c821129", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "This algorithm allows some artefacts of isolation forest to be bypassed and produces trees without empty branches, which is a drawback of the extended isolation forest (EIF) algorithm.  Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n", "original_text": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms. "}, "hash": "556c9f9c9eb5544db076790c3daa73ead774fe2f1a303fa217dd1bef395bee60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d58990db-3915-4407-8b3b-4746ced6a24f", "node_type": "1", "metadata": {"window": "For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). ", "original_text": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n"}, "hash": "abf3e73105c90b0db3e08e8ff461b55b691bb2b315de1c430f2323a7b66ce69c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the mean score plots, all three lines follow a similar increasing trend. ", "mimetype": "text/plain", "start_char_idx": 28870, "end_char_idx": 28946, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d58990db-3915-4407-8b3b-4746ced6a24f", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). ", "original_text": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54f822e1-ad28-4c27-995a-03de5fce357f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Experimentations on both synthetic and benchmark datasets allowed\n\n---\n[Image: A set of six line charts arranged in three rows, two charts per row, labeled (a) 2D blob, (b) 3D blob, and (c) 4D blob.  For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. ", "original_text": "In the mean score plots, all three lines follow a similar increasing trend. "}, "hash": "0b00e91cac1a307f848b8791072087a5d30b3da2a5ea0fa3fa7e832213cffa51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e281b008-887d-483c-952d-924ed198f7da", "node_type": "1", "metadata": {"window": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n", "original_text": "**Fig. "}, "hash": "fba135a74b9a8e26f50dfe65d1bb8c4ab69064aaf0d6811d0d631eb2c84bfd0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n", "mimetype": "text/plain", "start_char_idx": 28946, "end_char_idx": 29122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e281b008-887d-483c-952d-924ed198f7da", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d58990db-3915-4407-8b3b-4746ced6a24f", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "For each blob dimension, the left chart plots \"Score Mean\" vs.  \"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). ", "original_text": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n"}, "hash": "cf4a95c21bebe4e19bcd2eba27b11155ab6432b750564ad44e098dfd396f592c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de20353f-25fe-4b56-a28c-2720a44899c3", "node_type": "1", "metadata": {"window": "\"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. ", "original_text": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. "}, "hash": "2ce924c9f0ff9246347d5d8fb19afef75eafa2b97782e4e02a778efe8262698c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 29122, "end_char_idx": 29129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de20353f-25fe-4b56-a28c-2720a44899c3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. ", "original_text": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e281b008-887d-483c-952d-924ed198f7da", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\"Radius\", and the right chart plots \"Score Standard Deviation\" vs.  \"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n", "original_text": "**Fig. "}, "hash": "a81e53f9107d4647d71d431825facfe1d7302239891bec0224205b8b3bb11d06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d13fc6df-c5b4-4d03-a647-69dfb7f616d6", "node_type": "1", "metadata": {"window": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc. ", "original_text": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n"}, "hash": "66de23bf0116a64450c2273f9077a2bd64f7f6f61f89ce936a3c90aaa9e9c660", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. ", "mimetype": "text/plain", "start_char_idx": 29129, "end_char_idx": 29254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d13fc6df-c5b4-4d03-a647-69dfb7f616d6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc. ", "original_text": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de20353f-25fe-4b56-a28c-2720a44899c3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\"Radius\".  Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. ", "original_text": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius. "}, "hash": "21b3e5ecc5ed892b3e384fd501958f60a261f348b7212ecc9a4877d16b091963", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ff2f242-822c-483a-969a-5ce60adc4ee3", "node_type": "1", "metadata": {"window": "In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int. ", "original_text": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. "}, "hash": "9cfa224e728914ee9c2cba307fa383e97cd3f617974538d654705b43fccb494e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n", "mimetype": "text/plain", "start_char_idx": 29254, "end_char_idx": 29327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ff2f242-822c-483a-969a-5ce60adc4ee3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int. ", "original_text": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d13fc6df-c5b4-4d03-a647-69dfb7f616d6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Each chart shows three lines for \"Standard\", \"Extended\", and \"Generalized\" algorithms.  In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc. ", "original_text": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n"}, "hash": "a49c0e762519bb76fb4772e780801ef25898417f1e4cd0b1c4a6683e1d1233ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85eb83cb-a18c-421d-89e0-1bb450d6c058", "node_type": "1", "metadata": {"window": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf. ", "original_text": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). "}, "hash": "4f4d20d11a6d8a9dc89b7a5a96f4d606798c6f19134fdf8b1a21620961642f6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. ", "mimetype": "text/plain", "start_char_idx": 29327, "end_char_idx": 29426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85eb83cb-a18c-421d-89e0-1bb450d6c058", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf. ", "original_text": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ff2f242-822c-483a-969a-5ce60adc4ee3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In the mean score plots, all three lines follow a similar increasing trend.  In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int. ", "original_text": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF. "}, "hash": "d970c29d6735562967fae02fad3d1c7409525f3cfcc14063ba92d6f6ead7b1a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "235434de-574d-4b38-9646-76231217bc85", "node_type": "1", "metadata": {"window": "**Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp. ", "original_text": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n"}, "hash": "cd2420849d4c495bb24f75eaec7da4497a960f20ac070cf853b9e2702bbb2541", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). ", "mimetype": "text/plain", "start_char_idx": 29426, "end_char_idx": 29645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "235434de-574d-4b38-9646-76231217bc85", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp. ", "original_text": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85eb83cb-a18c-421d-89e0-1bb450d6c058", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "In the standard deviation plots, the \"Standard\" (IF) line is consistently higher than the \"Extended\" (EIF) and \"Generalized\" (GIF) lines, which are very close to each other.]\n\n **Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf. ", "original_text": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF). "}, "hash": "3fd1c49ba32bbbe4de96d09efac3fa3aebe2a8eb48671c282e1926b90aee9989", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0de77242-7731-4649-99a2-e7e95505df0d", "node_type": "1", "metadata": {"window": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n", "original_text": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. "}, "hash": "992ad44c821b323dc89dfaa35b44e9bbd677fefc890f953596b6989cdb012785", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n", "mimetype": "text/plain", "start_char_idx": 29645, "end_char_idx": 29812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0de77242-7731-4649-99a2-e7e95505df0d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n", "original_text": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "235434de-574d-4b38-9646-76231217bc85", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "**Fig.  11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp. ", "original_text": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n"}, "hash": "84fdb0d5cfbac1dbd667b13e8e1d4128aaaae993aad48b56cda039c2d8098f31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f06a3f11-4816-4fa7-ad46-d9db5a6aecc7", "node_type": "1", "metadata": {"window": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. ", "original_text": "Neural Data Mining for Credit Card Fraud Detection, in: Proc. "}, "hash": "9e3b57894f35a10bd8f189d4e9c2d1913c236afb1c4ccb2f6e51cb5fe2731f7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. ", "mimetype": "text/plain", "start_char_idx": 29812, "end_char_idx": 29871, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f06a3f11-4816-4fa7-ad46-d9db5a6aecc7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. ", "original_text": "Neural Data Mining for Credit Card Fraud Detection, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0de77242-7731-4649-99a2-e7e95505df0d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "11:** Mean anomaly scores (left) and corresponding standard deviations (right) for the various algorithms versus the radius.  The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n", "original_text": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999. "}, "hash": "74eb28ee1934c7870b5d4a15f3b5c5280ee419371c088b8c6108eed44c00dd78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82588a72-bf59-44c2-a9fa-4cdaccefd816", "node_type": "1", "metadata": {"window": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc. ", "original_text": "Int. "}, "hash": "adcc46725b5146e50cc9fe87cde9953a0b0043d98a757784bba92f35ed7447d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Neural Data Mining for Credit Card Fraud Detection, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 29871, "end_char_idx": 29933, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82588a72-bf59-44c2-a9fa-4cdaccefd816", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc. ", "original_text": "Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f06a3f11-4816-4fa7-ad46-d9db5a6aecc7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "The vertical blue lines represented the 1, 2 and 3 standard deviations.\n\n us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. ", "original_text": "Neural Data Mining for Credit Card Fraud Detection, in: Proc. "}, "hash": "581dccdc0926a757f11272617cfda902a46243ee2333592cc709345cbe13f83e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "000ab9af-1d1d-4c4d-815a-fa6314ccec32", "node_type": "1", "metadata": {"window": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int. ", "original_text": "Conf. "}, "hash": "ad8d7d166dc92af6bb47f67eff41817cc239675e6360efabe6302fa466673c4f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Int. ", "mimetype": "text/plain", "start_char_idx": 29933, "end_char_idx": 29938, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "000ab9af-1d1d-4c4d-815a-fa6314ccec32", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82588a72-bf59-44c2-a9fa-4cdaccefd816", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "us to evaluate the performance of the proposed method, which is similar to that obtained with EIF.  However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc. ", "original_text": "Int. "}, "hash": "ebcfd0267ab5963017f7de4a900b0a4505c4f0e2909c51dfe8fb3548e03d9cb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5c71f73-06a5-4e9e-a787-bc3f7c35c521", "node_type": "1", "metadata": {"window": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf. ", "original_text": "on Tools with Artificial Intelligence, pp. "}, "hash": "2d544ec7629789902ece07c8d4f4b670c732bda7e4da3f418dfbf303b0ac42a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 29938, "end_char_idx": 29944, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5c71f73-06a5-4e9e-a787-bc3f7c35c521", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf. ", "original_text": "on Tools with Artificial Intelligence, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "000ab9af-1d1d-4c4d-815a-fa6314ccec32", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "However, the proposed algorithm has a significantly reduced execution time when compared to EIF, and requires few parameters to store (a threshold at each node for GIF versus an intercept vector for each node for EIF).  Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int. ", "original_text": "Conf. "}, "hash": "b24aaf30d2d9373fe6ae18ae41c823dcdfa67700d3a1468a88e978d7adfa1e8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "474208f2-ff38-4091-81ca-386bec3dcc66", "node_type": "1", "metadata": {"window": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx. ", "original_text": "103\u2013106.\n\n"}, "hash": "e5bcda0883680e712aa001947cda542be142b58abd939a25cb2a9e3c9da306aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Tools with Artificial Intelligence, pp. ", "mimetype": "text/plain", "start_char_idx": 29944, "end_char_idx": 29987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "474208f2-ff38-4091-81ca-386bec3dcc66", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx. ", "original_text": "103\u2013106.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5c71f73-06a5-4e9e-a787-bc3f7c35c521", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Future work will consider active learning and the injection of user feedback into the anomaly detectors to reduce the false alarm rate and improve anomaly detection.\n\n ### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf. ", "original_text": "on Tools with Artificial Intelligence, pp. "}, "hash": "b81cd3cbc2098f50addcc00129c4a807c7ce9cae2fc0c087a42aa035052d2f3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abcf5b23-51a5-4006-9697-55f73f434b9e", "node_type": "1", "metadata": {"window": "Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp. ", "original_text": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. "}, "hash": "f21442618919080c5a4b7174e8cb001c78ced225031c3b5d5ade08dbfc40af79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "103\u2013106.\n\n", "mimetype": "text/plain", "start_char_idx": 29987, "end_char_idx": 29997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "abcf5b23-51a5-4006-9697-55f73f434b9e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp. ", "original_text": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "474208f2-ff38-4091-81ca-386bec3dcc66", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "### References\n\nBrause, R., Langsdorf, T., Hepp, M., 1999.  Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx. ", "original_text": "103\u2013106.\n\n"}, "hash": "d1f8536e05b97f0e3b0e5c0aff9aca6f60e38527d3ddafa108c2edaa3a3a2690", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11cc7d7e-ad31-4370-a8d7-2e1586fed3cf", "node_type": "1", "metadata": {"window": "Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n", "original_text": "LOF: Identifying Density-Based Local Outliers, in: Proc. "}, "hash": "8c6c63153a9a272135934227bdb7c1534971b694f47773736e1463d091455ad9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. ", "mimetype": "text/plain", "start_char_idx": 29997, "end_char_idx": 30055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "11cc7d7e-ad31-4370-a8d7-2e1586fed3cf", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n", "original_text": "LOF: Identifying Density-Based Local Outliers, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abcf5b23-51a5-4006-9697-55f73f434b9e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Neural Data Mining for Credit Card Fraud Detection, in: Proc.  Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp. ", "original_text": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000. "}, "hash": "abde0f9e87f31ead505553277dae4bcae20ccf72a5fdeeccc6a237d6be3024ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b9f422f-6b42-42f1-b7f7-9c39c8f749a8", "node_type": "1", "metadata": {"window": "Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009. ", "original_text": "Int. "}, "hash": "882246d9dd10318b8e0d45b2a52e68a42452678ea81f9e7ec2744ebf8d1772c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LOF: Identifying Density-Based Local Outliers, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 30055, "end_char_idx": 30112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b9f422f-6b42-42f1-b7f7-9c39c8f749a8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009. ", "original_text": "Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11cc7d7e-ad31-4370-a8d7-2e1586fed3cf", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n", "original_text": "LOF: Identifying Density-Based Local Outliers, in: Proc. "}, "hash": "fbadb1aa459842c76f83c9a045ce77342e44fa74db7158b28f4e97ca211769bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10ce6dbb-21e4-461b-9291-40a981328b19", "node_type": "1", "metadata": {"window": "on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey. ", "original_text": "Conf. "}, "hash": "a63bc6e5746bcc0ed65ae86d60e0184cf5495f52d6cc203288b88244ea87e20f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Int. ", "mimetype": "text/plain", "start_char_idx": 30112, "end_char_idx": 30117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10ce6dbb-21e4-461b-9291-40a981328b19", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b9f422f-6b42-42f1-b7f7-9c39c8f749a8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009. ", "original_text": "Int. "}, "hash": "17ad783568dfe243a67abdbf2708448b508cc14199a782c203f230d571b22cb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f2a1d52-5002-499f-95f8-fefb6c988b56", "node_type": "1", "metadata": {"window": "103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n", "original_text": "on Management of Data (SIGMOD), Dallas, Tx. "}, "hash": "1b7b0079ba28e7c4fb1538806ac3deab31a84504d00f9bd26b99d374be042682", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 30117, "end_char_idx": 30123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f2a1d52-5002-499f-95f8-fefb6c988b56", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n", "original_text": "on Management of Data (SIGMOD), Dallas, Tx. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10ce6dbb-21e4-461b-9291-40a981328b19", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Tools with Artificial Intelligence, pp.  103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey. ", "original_text": "Conf. "}, "hash": "6d1fa17024abbb45b627d9d9e5e93012b60b1ac887be511a44c8b535a636b621", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7aa22fc0-444e-476a-a4f4-5538122a8f70", "node_type": "1", "metadata": {"window": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019. ", "original_text": "pp. "}, "hash": "b874f7b3d3cdcd2f47e21672ef7961d0dac0f8fa14abc41ad10ff01e57e59bba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Management of Data (SIGMOD), Dallas, Tx. ", "mimetype": "text/plain", "start_char_idx": 30123, "end_char_idx": 30167, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7aa22fc0-444e-476a-a4f4-5538122a8f70", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019. ", "original_text": "pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f2a1d52-5002-499f-95f8-fefb6c988b56", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "103\u2013106.\n\n Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n", "original_text": "on Management of Data (SIGMOD), Dallas, Tx. "}, "hash": "72f74200baa717c202cef79f49298435639197b9eb0d53ce377afe38cffe05d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec02e213-afce-494a-a6c9-b7fd8870835c", "node_type": "1", "metadata": {"window": "LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. ", "original_text": "93\u2013104.\n\n"}, "hash": "5c4e42897fb03c7d3d41e8baa56f6ef1b0b0f610549107604c409792f0fdc62e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pp. ", "mimetype": "text/plain", "start_char_idx": 30167, "end_char_idx": 30171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ec02e213-afce-494a-a6c9-b7fd8870835c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. ", "original_text": "93\u2013104.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7aa22fc0-444e-476a-a4f4-5538122a8f70", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J., 2000.  LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019. ", "original_text": "pp. "}, "hash": "a0416d153a369bb410cb5d42fba6090913326f5292778b3b0efc203e7eba4afa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "307a322e-de61-439b-97b2-09adc235f9da", "node_type": "1", "metadata": {"window": "Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n", "original_text": "Chandola, V., Banerjee, A., Kumar, V., 2009. "}, "hash": "aac7a40c0fb3783c1f1f3593f0ac5bd4773538a3eaf07d55d3778181630b4059", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "93\u2013104.\n\n", "mimetype": "text/plain", "start_char_idx": 30171, "end_char_idx": 30180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "307a322e-de61-439b-97b2-09adc235f9da", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n", "original_text": "Chandola, V., Banerjee, A., Kumar, V., 2009. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec02e213-afce-494a-a6c9-b7fd8870835c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "LOF: Identifying Density-Based Local Outliers, in: Proc.  Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. ", "original_text": "93\u2013104.\n\n"}, "hash": "6f5b7a55cdc62f51582a62f6cced8824db8643907e5d66d9ad7e4b29c422331b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "687c68ea-14f9-4d90-955e-9d5c04d49625", "node_type": "1", "metadata": {"window": "Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015. ", "original_text": "Anomaly Detection: A Survey. "}, "hash": "620c7c8527067b22cd7cf20a547ac2c92d005cf3c7cdd8d5b69e4a77bc97d6c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chandola, V., Banerjee, A., Kumar, V., 2009. ", "mimetype": "text/plain", "start_char_idx": 30180, "end_char_idx": 30225, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "687c68ea-14f9-4d90-955e-9d5c04d49625", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015. ", "original_text": "Anomaly Detection: A Survey. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "307a322e-de61-439b-97b2-09adc235f9da", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n", "original_text": "Chandola, V., Banerjee, A., Kumar, V., 2009. "}, "hash": "8eed04c6316484ced07947cdd43e55030e3a4f7da14089b9184e4cbd969e1439", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd7a6ec5-ca9b-4356-97ab-1363f7e96407", "node_type": "1", "metadata": {"window": "on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark. ", "original_text": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n"}, "hash": "a7ba57355d5257026c7044f0d0a6a11e469631bfe7761157cbd0e95417870efb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly Detection: A Survey. ", "mimetype": "text/plain", "start_char_idx": 30225, "end_char_idx": 30254, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd7a6ec5-ca9b-4356-97ab-1363f7e96407", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark. ", "original_text": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "687c68ea-14f9-4d90-955e-9d5c04d49625", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015. ", "original_text": "Anomaly Detection: A Survey. "}, "hash": "59e870df8e56eb9a5a8d9ba3334ad5c0dcb07512ed6ca1404d3646af2bdcd6fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea0a42e5-4f38-401c-98f7-12e981181867", "node_type": "1", "metadata": {"window": "pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n", "original_text": "Dutta, J.K., Banerjee, B., 2019. "}, "hash": "73cfdc2e85e4afe4057544fe2a197c332ccd9ca6664d7d3e2aab384558cb9346", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n", "mimetype": "text/plain", "start_char_idx": 30254, "end_char_idx": 30293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea0a42e5-4f38-401c-98f7-12e981181867", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n", "original_text": "Dutta, J.K., Banerjee, B., 2019. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd7a6ec5-ca9b-4356-97ab-1363f7e96407", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Management of Data (SIGMOD), Dallas, Tx.  pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark. ", "original_text": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n"}, "hash": "18c00009fa0f8106a60e40ffa03eca06aa5763eb730ffd81af356d12091067b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05ec4534-1a2d-48c0-a38c-8564420c3403", "node_type": "1", "metadata": {"window": "93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019. ", "original_text": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. "}, "hash": "0032afe4a78c9b3e8b4fa3dfc6e920bdd1fef2b9ff7c39729a263e6d5f3ef17b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dutta, J.K., Banerjee, B., 2019. ", "mimetype": "text/plain", "start_char_idx": 30293, "end_char_idx": 30326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05ec4534-1a2d-48c0-a38c-8564420c3403", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019. ", "original_text": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea0a42e5-4f38-401c-98f7-12e981181867", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "pp.  93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n", "original_text": "Dutta, J.K., Banerjee, B., 2019. "}, "hash": "91f86101a9de8ff184b074d21e51f0ed90e4cb1c0898545c1e4814b1f642d089", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a6988b6-768d-4b9c-a953-eb4360db37f7", "node_type": "1", "metadata": {"window": "Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest. ", "original_text": "Pattern Recognition Letters 122, 99\u2013105.\n\n"}, "hash": "1701c2d17164e8141224bd44ea34f90ff72622472530179d046351b3d770f6ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. ", "mimetype": "text/plain", "start_char_idx": 30326, "end_char_idx": 30406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a6988b6-768d-4b9c-a953-eb4360db37f7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest. ", "original_text": "Pattern Recognition Letters 122, 99\u2013105.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05ec4534-1a2d-48c0-a38c-8564420c3403", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "93\u2013104.\n\n Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019. ", "original_text": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods. "}, "hash": "d85945158e8336e64de9af0f1dc8efd657d332dd71ec31972a0df034dddc92e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfc10111-5f7d-4cdd-a836-c1582f075a3a", "node_type": "1", "metadata": {"window": "Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans. ", "original_text": "Goldstein, M., 2015. "}, "hash": "1d1523fefae2a9cf56e8bddddd95ad752b313c0fb46c5836c790072e8f026343", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern Recognition Letters 122, 99\u2013105.\n\n", "mimetype": "text/plain", "start_char_idx": 30406, "end_char_idx": 30448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cfc10111-5f7d-4cdd-a836-c1582f075a3a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans. ", "original_text": "Goldstein, M., 2015. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a6988b6-768d-4b9c-a953-eb4360db37f7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Chandola, V., Banerjee, A., Kumar, V., 2009.  Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest. ", "original_text": "Pattern Recognition Letters 122, 99\u2013105.\n\n"}, "hash": "c4b63902e1bd6779d851645a06a1e13ac63758533a997778c1bd774eb934901c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9a6c4d5-b1c9-4e0a-a3d6-7f31ebcd6dbc", "node_type": "1", "metadata": {"window": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl. ", "original_text": "Unsupervised Anomaly Detection Benchmark. "}, "hash": "1859be0923f66263b6e68ba5df2a44badfd4e8a285fc61f5da780254d8589e46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Goldstein, M., 2015. ", "mimetype": "text/plain", "start_char_idx": 30448, "end_char_idx": 30469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9a6c4d5-b1c9-4e0a-a3d6-7f31ebcd6dbc", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl. ", "original_text": "Unsupervised Anomaly Detection Benchmark. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc10111-5f7d-4cdd-a836-c1582f075a3a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomaly Detection: A Survey.  ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans. ", "original_text": "Goldstein, M., 2015. "}, "hash": "9f8bcb408afbc119445b25132d2b485048fe0cbc7ddf31d3b39f27c3a5698064", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cec3f937-f694-47a0-b08d-5735ed756915", "node_type": "1", "metadata": {"window": "Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n", "original_text": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n"}, "hash": "5e9f339a40ded607d375625336df5bce51b37cd2d4b87d24cd4f1c14f722267c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unsupervised Anomaly Detection Benchmark. ", "mimetype": "text/plain", "start_char_idx": 30469, "end_char_idx": 30511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cec3f937-f694-47a0-b08d-5735ed756915", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n", "original_text": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9a6c4d5-b1c9-4e0a-a3d6-7f31ebcd6dbc", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "ACM Computing Surveys 41, 15:1\u201315:58.\n\n Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl. ", "original_text": "Unsupervised Anomaly Detection Benchmark. "}, "hash": "1d938f918c310aab569b2aad45e723d23b12b44d858c0f2399a3b2828bd77a5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3ea3186-e860-463c-a908-00460fcc313d", "node_type": "1", "metadata": {"window": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. ", "original_text": "Hariri, S., Kind, M.C., Brunner, R.J., 2019. "}, "hash": "589cdce4da96d0e01c205177794d952322008b3e752e31017144312b4e1e3df6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n", "mimetype": "text/plain", "start_char_idx": 30511, "end_char_idx": 30577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3ea3186-e860-463c-a908-00460fcc313d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. ", "original_text": "Hariri, S., Kind, M.C., Brunner, R.J., 2019. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cec3f937-f694-47a0-b08d-5735ed756915", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Dutta, J.K., Banerjee, B., 2019.  Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n", "original_text": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n"}, "hash": "1bffdea9f29c31a623f95a51bc5577ebb79f5cf265183e3eab188d0617e39176", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bb9a359-3160-4e2f-bd68-2cc53795781c", "node_type": "1", "metadata": {"window": "Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. ", "original_text": "Extended Isolation Forest. "}, "hash": "9bb6b5e7bcc09fa3243cb415e8f0ea3c662be5350c43640f61cbbccc133e9475", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hariri, S., Kind, M.C., Brunner, R.J., 2019. ", "mimetype": "text/plain", "start_char_idx": 30577, "end_char_idx": 30622, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bb9a359-3160-4e2f-bd68-2cc53795781c", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. ", "original_text": "Extended Isolation Forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3ea3186-e860-463c-a908-00460fcc313d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Comparison of Sparse Coding-based versus Traditional Outlier Detection Methods.  Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. ", "original_text": "Hariri, S., Kind, M.C., Brunner, R.J., 2019. "}, "hash": "315bd0d92c9b7dd3ef1cfee600f02e3099419096477d9fc7bfa9214484dfd54f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48f30896-2412-40aa-9b82-d4f59817d281", "node_type": "1", "metadata": {"window": "Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n", "original_text": "IEEE Trans. "}, "hash": "f43a8cf8089e0b7576929134efda6dde63dd1e009cfb16fb39fd70574fe97817", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extended Isolation Forest. ", "mimetype": "text/plain", "start_char_idx": 30622, "end_char_idx": 30649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48f30896-2412-40aa-9b82-d4f59817d281", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n", "original_text": "IEEE Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bb9a359-3160-4e2f-bd68-2cc53795781c", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pattern Recognition Letters 122, 99\u2013105.\n\n Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. ", "original_text": "Extended Isolation Forest. "}, "hash": "575f786f9dfee5816cbb25cf52e2b51c816a1cb27025b8337aa8066a68d5247d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f73f9006-b07d-43f1-94fa-9ff50e6c7f24", "node_type": "1", "metadata": {"window": "Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. ", "original_text": "Knowl. "}, "hash": "c19fc2bbbbb1af19308c1386fbf62253040501973df0fe9674633587fbd0c928", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Trans. ", "mimetype": "text/plain", "start_char_idx": 30649, "end_char_idx": 30661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f73f9006-b07d-43f1-94fa-9ff50e6c7f24", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. ", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48f30896-2412-40aa-9b82-d4f59817d281", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Goldstein, M., 2015.  Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n", "original_text": "IEEE Trans. "}, "hash": "b3f8f5e8e599f2cf3b29ef1709a2fdde9767babfe2f2b4e87d0429f5bc372f29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e77d3f10-09d5-4f2c-a559-f4850412fa72", "node_type": "1", "metadata": {"window": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc. ", "original_text": "Data Eng., 1\u20131.\n\n"}, "hash": "26c6e36b66d635b7048f60f1a6cb27dfdd2eec648af1b11d1e2c2f2dede0bc0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 30661, "end_char_idx": 30668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e77d3f10-09d5-4f2c-a559-f4850412fa72", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc. ", "original_text": "Data Eng., 1\u20131.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f73f9006-b07d-43f1-94fa-9ff50e6c7f24", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Unsupervised Anomaly Detection Benchmark.  URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. ", "original_text": "Knowl. "}, "hash": "b99bd31260983aed03d7ec5961146599fb079b8b0472b4594d010a78b7ef536d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e32ec419-4e4c-4983-9549-fc3f547052f3", "node_type": "1", "metadata": {"window": "Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int. ", "original_text": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. "}, "hash": "94cfaaab00ac45fd4d0dde9cce0f6eec04c99fbd02c3f60beb578f8ec934bcad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Eng., 1\u20131.\n\n", "mimetype": "text/plain", "start_char_idx": 30668, "end_char_idx": 30685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e32ec419-4e4c-4983-9549-fc3f547052f3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int. ", "original_text": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e77d3f10-09d5-4f2c-a559-f4850412fa72", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "URL: https://doi.org/10.7910/DVN/OPQMVF, doi:10.7910/DVN/OPQMVF.\n\n Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc. ", "original_text": "Data Eng., 1\u20131.\n\n"}, "hash": "8547e718f57a4df73ef860beda309d351691e38ebaf32ad00b426963ea0a1cd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6ce2fad-89d3-40f1-a3f7-ceb1af41441d", "node_type": "1", "metadata": {"window": "Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf. ", "original_text": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. "}, "hash": "4427ceeba8b9953020148506f19eec94aace9d2f74099109cc7799537ae67c5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. ", "mimetype": "text/plain", "start_char_idx": 30685, "end_char_idx": 30734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6ce2fad-89d3-40f1-a3f7-ceb1af41441d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf. ", "original_text": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e32ec419-4e4c-4983-9549-fc3f547052f3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Hariri, S., Kind, M.C., Brunner, R.J., 2019.  Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int. ", "original_text": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015. "}, "hash": "44663af8613171d09f455b4b422c253c4a86dcdae46344b8019da9e907bb30bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "913dd08d-5ba9-4271-9efd-f2afa78c6559", "node_type": "1", "metadata": {"window": "IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China. ", "original_text": "Pattern Recognition Letters 52, 17\u201324.\n\n"}, "hash": "0338c359c6f0b96ec37425bb6758e3b45e83554293c7460714ed428859c0bf17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. ", "mimetype": "text/plain", "start_char_idx": 30734, "end_char_idx": 30818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "913dd08d-5ba9-4271-9efd-f2afa78c6559", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China. ", "original_text": "Pattern Recognition Letters 52, 17\u201324.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6ce2fad-89d3-40f1-a3f7-ceb1af41441d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Extended Isolation Forest.  IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf. ", "original_text": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity. "}, "hash": "94835329f078dc55be5390e97e864dfbf588746b413cbfa8cd3336375a3de995", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dfc62ac-bf61-458f-9176-05edfe071f69", "node_type": "1", "metadata": {"window": "Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp. ", "original_text": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. "}, "hash": "2c03b39ce352d6d79f73971337e67b348358c60621d5860b6edd3c8d594a7cb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern Recognition Letters 52, 17\u201324.\n\n", "mimetype": "text/plain", "start_char_idx": 30818, "end_char_idx": 30858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0dfc62ac-bf61-458f-9176-05edfe071f69", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp. ", "original_text": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "913dd08d-5ba9-4271-9efd-f2afa78c6559", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "IEEE Trans.  Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China. ", "original_text": "Pattern Recognition Letters 52, 17\u201324.\n\n"}, "hash": "21e0918e0732e420f7c37cde53afc9ae6eb968c2ce06e25eb26f1b87c9cf46b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c617c52-20a5-4010-bfa6-c1136843cf23", "node_type": "1", "metadata": {"window": "Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n", "original_text": "LoOP: Local Outlier Probabilities, in: Proc. "}, "hash": "f042ea9f365ec47e51fbcd2080e3d8dcf24a7e18459670c123f199332d15f72f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. ", "mimetype": "text/plain", "start_char_idx": 30858, "end_char_idx": 30916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c617c52-20a5-4010-bfa6-c1136843cf23", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n", "original_text": "LoOP: Local Outlier Probabilities, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dfc62ac-bf61-458f-9176-05edfe071f69", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Knowl.  Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp. ", "original_text": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009. "}, "hash": "9b44e431307c73a3080cd65492e29c4dca830ec9dd4725f683bf352ea8cbef85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74fbc614-ec65-4c42-bdf5-c499fdd3ce3a", "node_type": "1", "metadata": {"window": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. ", "original_text": "Int. "}, "hash": "5d56df00fbd93632a30d37a703f7675591aa8cc75ae48a5567f1ea75a0ad7952", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LoOP: Local Outlier Probabilities, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 30916, "end_char_idx": 30961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74fbc614-ec65-4c42-bdf5-c499fdd3ce3a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. ", "original_text": "Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c617c52-20a5-4010-bfa6-c1136843cf23", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Data Eng., 1\u20131.\n\n \u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n", "original_text": "LoOP: Local Outlier Probabilities, in: Proc. "}, "hash": "3e874e2e58382e1bd11d61c456108700a49a441cbe3f9a16a096b72a996dfb84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b49de26-94f3-4620-8f95-7cd08e660ed7", "node_type": "1", "metadata": {"window": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes. ", "original_text": "Conf. "}, "hash": "1db0c75e2e2f1d6bf7f0d972296b38a9fbf32d080d286e118e16ba04aa792e9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Int. ", "mimetype": "text/plain", "start_char_idx": 30961, "end_char_idx": 30966, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b49de26-94f3-4620-8f95-7cd08e660ed7", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74fbc614-ec65-4c42-bdf5-c499fdd3ce3a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "\u0130nkaya, T., Kayalgil, S., \u00d6zdemirel, N.E., 2015.  An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. ", "original_text": "Int. "}, "hash": "c9e40e78e5ecf46828cdaaae6149ee36dd3079a1eac7a90a4543ebc99d61be14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90631b9a-f8e2-4119-a605-5b58e42d2819", "node_type": "1", "metadata": {"window": "Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n", "original_text": "on Information and Knowledge Management (CIKM), Hong-Kong, China. "}, "hash": "11968beb5cfdcdbf1b16009732cf2b1070d1277799345283e7941d1abf53328d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 30966, "end_char_idx": 30972, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90631b9a-f8e2-4119-a605-5b58e42d2819", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n", "original_text": "on Information and Knowledge Management (CIKM), Hong-Kong, China. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b49de26-94f3-4620-8f95-7cd08e660ed7", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "An Adaptive Neighbourhood Construction Algorithm Based on Density and Connectivity.  Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes. ", "original_text": "Conf. "}, "hash": "7e092c6f0b720bc02b4614ae9b29a72adf202f4de6a3c4fd39bbaa62c2ab75d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa3b1146-6e68-4afd-85ed-666c4238f6bb", "node_type": "1", "metadata": {"window": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. ", "original_text": "pp. "}, "hash": "ec7ea9494c284a6e18906a92224a2c59e7c7ee4b75ccb7f68e31124290b7aedb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Information and Knowledge Management (CIKM), Hong-Kong, China. ", "mimetype": "text/plain", "start_char_idx": 30972, "end_char_idx": 31038, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aa3b1146-6e68-4afd-85ed-666c4238f6bb", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. ", "original_text": "pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90631b9a-f8e2-4119-a605-5b58e42d2819", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pattern Recognition Letters 52, 17\u201324.\n\n Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n", "original_text": "on Information and Knowledge Management (CIKM), Hong-Kong, China. "}, "hash": "36a810fa72b1e4ad99d0512c73c513741641d14cf2917d4723006be0d3d2eddf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2affd2e-0139-4f9f-beac-656393beca2e", "node_type": "1", "metadata": {"window": "LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc. ", "original_text": "1649\u20131652.\n\n"}, "hash": "c21835876f4993121f2975e17bbb96587d2ee68ec9680efa029c80920feb4e17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pp. ", "mimetype": "text/plain", "start_char_idx": 31038, "end_char_idx": 31042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2affd2e-0139-4f9f-beac-656393beca2e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc. ", "original_text": "1649\u20131652.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa3b1146-6e68-4afd-85ed-666c4238f6bb", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Kriegel, H.P., Kr\u00f6ger, P., Schubert, E., Zimek, A., 2009.  LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. ", "original_text": "pp. "}, "hash": "13a899531ac7379be1c1a3f462798225d6697b578ddb50f1288680dce1620fcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "140363e9-aad7-4a08-b716-958062385cb0", "node_type": "1", "metadata": {"window": "Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int. ", "original_text": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. "}, "hash": "d0a9bd9886602640ac51f288670f74c464eb7de383e784c91c7dabf7ad46137f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1649\u20131652.\n\n", "mimetype": "text/plain", "start_char_idx": 31042, "end_char_idx": 31054, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "140363e9-aad7-4a08-b716-958062385cb0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int. ", "original_text": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2affd2e-0139-4f9f-beac-656393beca2e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "LoOP: Local Outlier Probabilities, in: Proc.  Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc. ", "original_text": "1649\u20131652.\n\n"}, "hash": "449e0a9eb140b220fc66265b368632ff8701184bff787e89b1481dc53bcddb10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a030c511-615a-4e2f-8da4-0324193beb37", "node_type": "1", "metadata": {"window": "Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf. ", "original_text": "Contextual Anomaly Detection in Crowded Surveillance Scenes. "}, "hash": "11b83910e358088d1556976d90e778b1ce9d783f2130121b1590c7ccc4860d6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. ", "mimetype": "text/plain", "start_char_idx": 31054, "end_char_idx": 31106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a030c511-615a-4e2f-8da4-0324193beb37", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf. ", "original_text": "Contextual Anomaly Detection in Crowded Surveillance Scenes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "140363e9-aad7-4a08-b716-958062385cb0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int. ", "original_text": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014. "}, "hash": "a6c1219fb013b3026c789284df37df7321969cf1d4c7592279a2e73d36003b4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ea26dab-c8cf-45fa-9e4b-31fdb245dd8e", "node_type": "1", "metadata": {"window": "on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy. ", "original_text": "Pattern Recognition Letters 44, 71\u201379.\n\n"}, "hash": "a342b80a80da20f491941e9264ba9d23c8400974f46ee6a5fde9467df8b6612f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Contextual Anomaly Detection in Crowded Surveillance Scenes. ", "mimetype": "text/plain", "start_char_idx": 31106, "end_char_idx": 31167, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ea26dab-c8cf-45fa-9e4b-31fdb245dd8e", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy. ", "original_text": "Pattern Recognition Letters 44, 71\u201379.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a030c511-615a-4e2f-8da4-0324193beb37", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf. ", "original_text": "Contextual Anomaly Detection in Crowded Surveillance Scenes. "}, "hash": "996f2dbc419512229d8a506db3da4ebc270cbb0c50c04cb8677b699a4b89406f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "473ca1d9-66b0-46d9-828a-a66d6ca5230a", "node_type": "1", "metadata": {"window": "pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp. ", "original_text": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. "}, "hash": "ae570a148995231cef1b9d1054245e6e6d07e8fbd9419e733df8d92d27d471a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pattern Recognition Letters 44, 71\u201379.\n\n", "mimetype": "text/plain", "start_char_idx": 31167, "end_char_idx": 31207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "473ca1d9-66b0-46d9-828a-a66d6ca5230a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp. ", "original_text": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ea26dab-c8cf-45fa-9e4b-31fdb245dd8e", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Information and Knowledge Management (CIKM), Hong-Kong, China.  pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy. ", "original_text": "Pattern Recognition Letters 44, 71\u201379.\n\n"}, "hash": "a7a2589c03576f8b05d59134964b2f8493d6169ae7a45d9b55f0211fe13b97f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df2baca8-5b5f-4a6d-a008-76659b89cbc0", "node_type": "1", "metadata": {"window": "1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n", "original_text": "Isolation Forest, in: Proc. "}, "hash": "185915bbfc30e192d848bdc230d8c40063e537ba2bea006bde965da3a76b3975", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. ", "mimetype": "text/plain", "start_char_idx": 31207, "end_char_idx": 31248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df2baca8-5b5f-4a6d-a008-76659b89cbc0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n", "original_text": "Isolation Forest, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "473ca1d9-66b0-46d9-828a-a66d6ca5230a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "pp.  1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp. ", "original_text": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008. "}, "hash": "4a8ffeace3dde40882b570cf3a76cec9858282ba1c14d68581db7ee9fbced374", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8b360da-fe06-4aaf-ac15-8d6c4e423a86", "node_type": "1", "metadata": {"window": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959. ", "original_text": "Int. "}, "hash": "622a2c6b23dbb04ebbe2604a6c7d9ab7c650be112de2bb7d226d450628c67e7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation Forest, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 31248, "end_char_idx": 31276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8b360da-fe06-4aaf-ac15-8d6c4e423a86", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959. ", "original_text": "Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df2baca8-5b5f-4a6d-a008-76659b89cbc0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "1649\u20131652.\n\n Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n", "original_text": "Isolation Forest, in: Proc. "}, "hash": "33220565d48a4c818f6862c13e97369e75bad6078796299d42906378233e5b08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c37d2e49-f3d1-4820-914e-933703b00594", "node_type": "1", "metadata": {"window": "Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. ", "original_text": "Conf. "}, "hash": "e771367cb5eb37714ed8fd99cd6be0c719c84088fe3477900125e08ce0ee27f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Int. ", "mimetype": "text/plain", "start_char_idx": 31276, "end_char_idx": 31281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c37d2e49-f3d1-4820-914e-933703b00594", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8b360da-fe06-4aaf-ac15-8d6c4e423a86", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Leach, M.J.V., Sparks, E.P., Robertson, N.M., 2014.  Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959. ", "original_text": "Int. "}, "hash": "6b1f99af56abed40683a182090a8ee337ddb79f24ce5c8974d4706da4db517cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30df2bf5-3236-431a-8115-cb5d10cbbd82", "node_type": "1", "metadata": {"window": "Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun. ", "original_text": "on Data Mining (ICDM), Pisa, Italy. "}, "hash": "6958a59b94ccdb7a1c4a19faab0d3106700db0b3da1fb47fc72632cb5f5db3a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 31281, "end_char_idx": 31287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30df2bf5-3236-431a-8115-cb5d10cbbd82", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun. ", "original_text": "on Data Mining (ICDM), Pisa, Italy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c37d2e49-f3d1-4820-914e-933703b00594", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Contextual Anomaly Detection in Crowded Surveillance Scenes.  Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. ", "original_text": "Conf. "}, "hash": "d9a80276152f88ef6d8ff30f94dd5353a5f7616d297f5d8fcd99f420f2d978a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "970026de-2eaa-4a93-8927-99203c6afc5d", "node_type": "1", "metadata": {"window": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n", "original_text": "pp. "}, "hash": "dd6521b3a1281d1a0df46d40fcdd000c971e795cb8eb6cde59e4174d5fea4767", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Data Mining (ICDM), Pisa, Italy. ", "mimetype": "text/plain", "start_char_idx": 31287, "end_char_idx": 31323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "970026de-2eaa-4a93-8927-99203c6afc5d", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n", "original_text": "pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30df2bf5-3236-431a-8115-cb5d10cbbd82", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pattern Recognition Letters 44, 71\u201379.\n\n Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun. ", "original_text": "on Data Mining (ICDM), Pisa, Italy. "}, "hash": "708ef2edc158da232b902cf99c13e790739fd3803a90dbad1d0cd6b090fbb95f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e776d01b-0d7c-4c28-b969-96f6787ff460", "node_type": "1", "metadata": {"window": "Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. ", "original_text": "413\u2013422.\n\n"}, "hash": "5a38e06395db2083b95f9312ca505fbb05976cdb08c0932a3bd1ce2610679e62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "pp. ", "mimetype": "text/plain", "start_char_idx": 31323, "end_char_idx": 31327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e776d01b-0d7c-4c28-b969-96f6787ff460", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. ", "original_text": "413\u2013422.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "970026de-2eaa-4a93-8927-99203c6afc5d", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Liu, F.T., Ting, K.M., Zhou, Z.H., 2008.  Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n", "original_text": "pp. "}, "hash": "8f92ec55e3fe2a8bee713ac367229054b38f010b5630822aad7161d7746df5ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b86e3792-b8ee-4f5c-8b40-c01a9377e2c9", "node_type": "1", "metadata": {"window": "Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. ", "original_text": "Muller, M.E., 1959. "}, "hash": "98def91778295d9dcf8f7c3f35bfb6a9f4575cc5460ef03359b24304376d1f19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "413\u2013422.\n\n", "mimetype": "text/plain", "start_char_idx": 31327, "end_char_idx": 31337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b86e3792-b8ee-4f5c-8b40-c01a9377e2c9", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. ", "original_text": "Muller, M.E., 1959. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e776d01b-0d7c-4c28-b969-96f6787ff460", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Isolation Forest, in: Proc.  Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. ", "original_text": "413\u2013422.\n\n"}, "hash": "ba3d518387e8bbbc643611724eb511f2d488c18542c3447ce379d8e84eb93d1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7292937-e072-43c1-8e61-210acc0ad0e8", "node_type": "1", "metadata": {"window": "Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n", "original_text": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. "}, "hash": "7f513db361ee5502ab589e9c896e479544b63fe147d40c4b17efbda9824e090e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Muller, M.E., 1959. ", "mimetype": "text/plain", "start_char_idx": 31337, "end_char_idx": 31357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7292937-e072-43c1-8e61-210acc0ad0e8", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n", "original_text": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b86e3792-b8ee-4f5c-8b40-c01a9377e2c9", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Int.  Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. ", "original_text": "Muller, M.E., 1959. "}, "hash": "66b0e9fadf112a58a5a7e7e12a4388c6fb66c691c7f80c7c09dd04e0e2fff55b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18b7ad9f-ab36-4cf3-a68d-b2b78fbcdd04", "node_type": "1", "metadata": {"window": "on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. ", "original_text": "Commun. "}, "hash": "05c0479d6e70cf6283a8907ed66b5b675e58d07eec5141f02fadbbf16c53eb7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. ", "mimetype": "text/plain", "start_char_idx": 31357, "end_char_idx": 31434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18b7ad9f-ab36-4cf3-a68d-b2b78fbcdd04", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. ", "original_text": "Commun. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7292937-e072-43c1-8e61-210acc0ad0e8", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Conf.  on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n", "original_text": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres. "}, "hash": "99117688463e5171ced3e5b20bec907c77b62dda219e668b34e5a24bb93054ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c2baee0-2b9f-4fe5-8253-4ccee5c94734", "node_type": "1", "metadata": {"window": "pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution. ", "original_text": "ACM 2, 19\u201320.\n\n"}, "hash": "1a1168bf211151dc31361270cb42cf62a87ad1ea93c4fa643b4077b9d5ee7f25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Commun. ", "mimetype": "text/plain", "start_char_idx": 31434, "end_char_idx": 31442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c2baee0-2b9f-4fe5-8253-4ccee5c94734", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution. ", "original_text": "ACM 2, 19\u201320.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18b7ad9f-ab36-4cf3-a68d-b2b78fbcdd04", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "on Data Mining (ICDM), Pisa, Italy.  pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. ", "original_text": "Commun. "}, "hash": "047b782c97a97324e99053fbd52f1e4eee7c5cc05fe979e70c5dacc6de991007", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d261ada0-5541-4a14-996d-dd36479f4e33", "node_type": "1", "metadata": {"window": "413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n", "original_text": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. "}, "hash": "4f3479366d170b72ff224504c385179b57196fb459f7347d6d0822d606cd9320", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ACM 2, 19\u201320.\n\n", "mimetype": "text/plain", "start_char_idx": 31442, "end_char_idx": 31457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d261ada0-5541-4a14-996d-dd36479f4e33", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n", "original_text": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c2baee0-2b9f-4fe5-8253-4ccee5c94734", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "pp.  413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution. ", "original_text": "ACM 2, 19\u201320.\n\n"}, "hash": "509b5bfcfbbf943e279e6d81336dfc005d5207dc14c490ab6c1f21f3321c2a5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c745d15-9e5b-452b-a3b6-72ed2bc980d9", "node_type": "1", "metadata": {"window": "Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004. ", "original_text": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. "}, "hash": "8de821dea6467fd26c50c25fe403d964e0578a54d3dbec4a2c0320b8be1f0148", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. ", "mimetype": "text/plain", "start_char_idx": 31457, "end_char_idx": 31524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c745d15-9e5b-452b-a3b6-72ed2bc980d9", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004. ", "original_text": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d261ada0-5541-4a14-996d-dd36479f4e33", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "413\u2013422.\n\n Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n", "original_text": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020. "}, "hash": "a3c50ff439c295ba5d41277903e9724c6f4fbf2c29d287e83a2f4c7de6a8c981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e566f174-e4f6-4715-a79c-1c23b2865030", "node_type": "1", "metadata": {"window": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description. ", "original_text": "Signal Processing 168, 107474.\n\n"}, "hash": "7306c99488305c8f69beb12287ebe6c3fca81baa880e327449f2300de86cf53a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. ", "mimetype": "text/plain", "start_char_idx": 31524, "end_char_idx": 31621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e566f174-e4f6-4715-a79c-1c23b2865030", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description. ", "original_text": "Signal Processing 168, 107474.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c745d15-9e5b-452b-a3b6-72ed2bc980d9", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Muller, M.E., 1959.  A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004. ", "original_text": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning. "}, "hash": "3b04e045a2d962131c65beda268da79d7ec3710db46416301eb460c71fa5b446", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ed604c4-a85c-45db-951a-c8f95cb8b584", "node_type": "1", "metadata": {"window": "Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n", "original_text": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. "}, "hash": "3f13adea6a0b758a536a866a58c3c66e2ade833d05417351295c77011d8d59c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Signal Processing 168, 107474.\n\n", "mimetype": "text/plain", "start_char_idx": 31621, "end_char_idx": 31653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ed604c4-a85c-45db-951a-c8f95cb8b584", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n", "original_text": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e566f174-e4f6-4715-a79c-1c23b2865030", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres.  Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description. ", "original_text": "Signal Processing 168, 107474.\n\n"}, "hash": "b04631da695f5161845fa035a5ecf83d4abc80ad6d982df758fb9c23790ee85d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45a6a060-e444-477d-aea8-c9d2ffac3605", "node_type": "1", "metadata": {"window": "ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. ", "original_text": "Estimating the Support of a High-Dimensional Distribution. "}, "hash": "6eaecaa1ff80b965e41e36bf40128c7902df42e251df23ad2b4a0a6213b2aef4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. ", "mimetype": "text/plain", "start_char_idx": 31653, "end_char_idx": 31738, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45a6a060-e444-477d-aea8-c9d2ffac3605", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. ", "original_text": "Estimating the Support of a High-Dimensional Distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ed604c4-a85c-45db-951a-c8f95cb8b584", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Commun.  ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n", "original_text": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001. "}, "hash": "07a0428bfb85c6ba0b5d49df9f39a12cd73eed8022aed6d36cbed3e4a766d42a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c7d759e-6a9b-49ad-9fb4-e7b78a07f263", "node_type": "1", "metadata": {"window": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. ", "original_text": "Neural Computation 13, 1443\u20131471.\n\n"}, "hash": "592e9c6a9b415f167cd38910061a9f08823a04010b994d87e9970cbb941ef439", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Estimating the Support of a High-Dimensional Distribution. ", "mimetype": "text/plain", "start_char_idx": 31738, "end_char_idx": 31797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c7d759e-6a9b-49ad-9fb4-e7b78a07f263", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. ", "original_text": "Neural Computation 13, 1443\u20131471.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45a6a060-e444-477d-aea8-c9d2ffac3605", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "ACM 2, 19\u201320.\n\n Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. ", "original_text": "Estimating the Support of a High-Dimensional Distribution. "}, "hash": "6d7476c0109d8f0eec6f025ca78af3560d744c8c4cef64943e0f18e33315064a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbbbab04-36be-4618-8d2b-8408557fa748", "node_type": "1", "metadata": {"window": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans. ", "original_text": "Tax, D.M., Duin, R.P., 2004. "}, "hash": "51fd5c33352ae833107b1a31c54b739ba23296010db6e3600ddc3614f7f73779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Neural Computation 13, 1443\u20131471.\n\n", "mimetype": "text/plain", "start_char_idx": 31797, "end_char_idx": 31832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbbbab04-36be-4618-8d2b-8408557fa748", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans. ", "original_text": "Tax, D.M., Duin, R.P., 2004. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c7d759e-6a9b-49ad-9fb4-e7b78a07f263", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Pilastre, B., Boussouf, L., d\u2019Escrivan, S., Tourneret, J.Y., 2020.  Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. ", "original_text": "Neural Computation 13, 1443\u20131471.\n\n"}, "hash": "2614bccb37ba24694144fa8551735aeeb5180f91a8fa0513673eb19f733e558d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21f55589-f97e-4bb0-99ed-00e38f85f69a", "node_type": "1", "metadata": {"window": "Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp. ", "original_text": "Support Vector Data Description. "}, "hash": "942ab7f2f8d40dd8916aecdcddde0a75e2891cbccedcaa0bdbf131acde73cf72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tax, D.M., Duin, R.P., 2004. ", "mimetype": "text/plain", "start_char_idx": 31832, "end_char_idx": 31861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21f55589-f97e-4bb0-99ed-00e38f85f69a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp. ", "original_text": "Support Vector Data Description. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbbbab04-36be-4618-8d2b-8408557fa748", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Anomaly Detection in Mixed Telemetry Data Using a Sparse Representation and Dictionary Learning.  Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans. ", "original_text": "Tax, D.M., Duin, R.P., 2004. "}, "hash": "39c595a0c0a1debcfbeb2f8c8824756f719d29126b09d9edaf36a427300e64b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4fde1d4-e279-449a-b7ad-11c2f2f68e13", "node_type": "1", "metadata": {"window": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron. ", "original_text": "Machine Learning, 45\u201366.\n\n"}, "hash": "2175a999479dbd525cf9927bfdcf505206c0e2894fe1f791ffe3754ab8521beb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Support Vector Data Description. ", "mimetype": "text/plain", "start_char_idx": 31861, "end_char_idx": 31894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4fde1d4-e279-449a-b7ad-11c2f2f68e13", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron. ", "original_text": "Machine Learning, 45\u201366.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21f55589-f97e-4bb0-99ed-00e38f85f69a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Signal Processing 168, 107474.\n\n Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp. ", "original_text": "Support Vector Data Description. "}, "hash": "da0c4f60302c92ddca2f7508a4058b1d7ce96699fbfb5e9cb34c09c94f995718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfae1987-1550-43a7-bac6-bd86745182f3", "node_type": "1", "metadata": {"window": "Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst. ", "original_text": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. "}, "hash": "d4e8cc304ae70468b4bdc83a76fc596f2ba6304a6733bccb0d444dc2f818834b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Machine Learning, 45\u201366.\n\n", "mimetype": "text/plain", "start_char_idx": 31894, "end_char_idx": 31920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfae1987-1550-43a7-bac6-bd86745182f3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst. ", "original_text": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4fde1d4-e279-449a-b7ad-11c2f2f68e13", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Sch\u00f6lkopf, B., Platt, J.C., Shawe-Taylor, J.C., Smola, A.J., Williamson, R.C., 2001.  Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron. ", "original_text": "Machine Learning, 45\u201366.\n\n"}, "hash": "8cbf43d4b61c268da3cabd7080c3f17b15b20b3cf970ba747cc37d090f5e51c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93065a83-7a7d-4cab-9edf-f9f6c0f057eb", "node_type": "1", "metadata": {"window": "Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n", "original_text": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. "}, "hash": "b219f332b490572b7b7e9df65e5703ad0214b39cd99a17e0910f8e02c080f47f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. ", "mimetype": "text/plain", "start_char_idx": 31920, "end_char_idx": 32001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93065a83-7a7d-4cab-9edf-f9f6c0f057eb", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n", "original_text": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfae1987-1550-43a7-bac6-bd86745182f3", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Estimating the Support of a High-Dimensional Distribution.  Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst. ", "original_text": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017. "}, "hash": "cb9cc9e60bf51ade4701beca6b3c851f8dd2d7fb8c297167b65f1dedba3cf1c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22022308-5243-4598-8b84-0cd625b79c6b", "node_type": "1", "metadata": {"window": "Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts. ", "original_text": "IEEE Trans. "}, "hash": "3e97b40c854a39705f76d4707372e95e7b9bae3cb57c0db23f857214293785c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. ", "mimetype": "text/plain", "start_char_idx": 32001, "end_char_idx": 32136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22022308-5243-4598-8b84-0cd625b79c6b", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts. ", "original_text": "IEEE Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93065a83-7a7d-4cab-9edf-f9f6c0f057eb", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Neural Computation 13, 1443\u20131471.\n\n Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n", "original_text": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction. "}, "hash": "3c53b18e037a3fae8b4f6f605f4d80160b855a24e63756917be93394cdc1efab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "330d3bd4-8368-49e1-99a1-028cd8c8be36", "node_type": "1", "metadata": {"window": "Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right). ", "original_text": "Aerosp. "}, "hash": "31b8c1407eb8284a3946ba5e88bfba972c5b1d099ea83683d8ae6ece3ae94c85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Trans. ", "mimetype": "text/plain", "start_char_idx": 32136, "end_char_idx": 32148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "330d3bd4-8368-49e1-99a1-028cd8c8be36", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right). ", "original_text": "Aerosp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22022308-5243-4598-8b84-0cd625b79c6b", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Tax, D.M., Duin, R.P., 2004.  Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts. ", "original_text": "IEEE Trans. "}, "hash": "8d403b13e8777c97124c38713661435f5546d11f24ea70a7b5a9b59831daefe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb7052f1-5c76-4ccd-a871-0f30433f07be", "node_type": "1", "metadata": {"window": "Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). ", "original_text": "Electron. "}, "hash": "ae259b5f6a72b37964676b83d203028b47e770ebc6d9ab80528faa77230659af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Aerosp. ", "mimetype": "text/plain", "start_char_idx": 32148, "end_char_idx": 32156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb7052f1-5c76-4ccd-a871-0f30433f07be", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). ", "original_text": "Electron. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "330d3bd4-8368-49e1-99a1-028cd8c8be36", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Support Vector Data Description.  Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right). ", "original_text": "Aerosp. "}, "hash": "0b32d0dd076a8fc89609f3bce8aa2d393eb2996e7efaacbe677c1a6441a271a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebf8646d-3eff-42f9-832f-8a2d360967d5", "node_type": "1", "metadata": {"window": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. ", "original_text": "Syst. "}, "hash": "617aa01b17a3ee16f5d206d2116989fd9684ebabd49f7c182e0701b760c36f6a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Electron. ", "mimetype": "text/plain", "start_char_idx": 32156, "end_char_idx": 32166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebf8646d-3eff-42f9-832f-8a2d360967d5", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. ", "original_text": "Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb7052f1-5c76-4ccd-a871-0f30433f07be", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Machine Learning, 45\u201366.\n\n Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). ", "original_text": "Electron. "}, "hash": "815edb161e88d2bdbc4fce986b25cfe853e73789588c59cac85dad7493f0289e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98e2690b-bd39-4111-9ed3-12855493805a", "node_type": "1", "metadata": {"window": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". ", "original_text": "53, 1384\u20131401.\n\n"}, "hash": "cc3ca7e3846a4c83d4a3147881744b1e7467c20f7844c9a76345d28f9a2f788b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Syst. ", "mimetype": "text/plain", "start_char_idx": 32166, "end_char_idx": 32172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98e2690b-bd39-4111-9ed3-12855493805a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". ", "original_text": "53, 1384\u20131401.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebf8646d-3eff-42f9-832f-8a2d360967d5", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Yairi, T., Takeishi, N., Oda, T., Nakajima, Y., Nishimura, N., Takata, N., 2017.  A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. ", "original_text": "Syst. "}, "hash": "348babdaed156ca6271a99fc7fef1a1cb7d2f35bc9cb5840b5359a3d782870da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f469470-7b9d-4254-bd9a-25df8f92d569", "node_type": "1", "metadata": {"window": "IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. ", "original_text": "---\n[Image: A large 3x3 grid of line charts. "}, "hash": "9f24d6d5064701acc4959785925dfe78d6a785d515647a769a2867cad4327353", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "53, 1384\u20131401.\n\n", "mimetype": "text/plain", "start_char_idx": 32172, "end_char_idx": 32188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f469470-7b9d-4254-bd9a-25df8f92d569", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. ", "original_text": "---\n[Image: A large 3x3 grid of line charts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98e2690b-bd39-4111-9ed3-12855493805a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "A Data Driven Health Monitoring Method for Satellite Housekeeping Data Based on Probabilistic Clustering and Dimensionality Reduction.  IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". ", "original_text": "53, 1384\u20131401.\n\n"}, "hash": "58f733ea96da4c12469a42ac6432c788a1abb650bab1208e6aa07d461d0ec8ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d226170-119a-4474-8f9b-07cc1bb6276a", "node_type": "1", "metadata": {"window": "Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n", "original_text": "Each column represents an algorithm (IF left, EIF center, GIF right). "}, "hash": "9bf2f580d162ba9044e571292f0d7aae4b350609b1c90ee8278c7c9bebd3118a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n[Image: A large 3x3 grid of line charts. ", "mimetype": "text/plain", "start_char_idx": 32188, "end_char_idx": 32233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d226170-119a-4474-8f9b-07cc1bb6276a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n", "original_text": "Each column represents an algorithm (IF left, EIF center, GIF right). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f469470-7b9d-4254-bd9a-25df8f92d569", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "IEEE Trans.  Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. ", "original_text": "---\n[Image: A large 3x3 grid of line charts. "}, "hash": "c23840e7552bfae6a26f0f3b3c7b96bc16bb6e58cfe4ecfd45ea883ff833e7fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28f65a5f-f9d8-4c04-9ac2-5cb6ea2cd4f0", "node_type": "1", "metadata": {"window": "Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig. ", "original_text": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). "}, "hash": "7d6b5ac1d17cc303ad1a2780baf39f528867de004a20f63eaf862b9e207789fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each column represents an algorithm (IF left, EIF center, GIF right). ", "mimetype": "text/plain", "start_char_idx": 32233, "end_char_idx": 32303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28f65a5f-f9d8-4c04-9ac2-5cb6ea2cd4f0", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig. ", "original_text": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d226170-119a-4474-8f9b-07cc1bb6276a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Aerosp.  Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n", "original_text": "Each column represents an algorithm (IF left, EIF center, GIF right). "}, "hash": "49698dcaa61127a5985a424b3e080d5259f7fcdbdba4accd745ae6373f5b9347", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e704d944-9977-47d9-8cac-6b8ba7a28792", "node_type": "1", "metadata": {"window": "Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. "}, "hash": "8db42ee009141f3056c3af6f306754f4fee0094dd1d8b2ba67fac5af7dd99796", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). ", "mimetype": "text/plain", "start_char_idx": 32303, "end_char_idx": 32384, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e704d944-9977-47d9-8cac-6b8ba7a28792", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28f65a5f-f9d8-4c04-9ac2-5cb6ea2cd4f0", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Electron.  Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig. ", "original_text": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob). "}, "hash": "155a4c6d4bdc7e81900b3ea78e17ca5b7e535adb84a71922057169a0c2fc77bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c076aa5e-4aa3-420b-99cd-5d85d782adfd", "node_type": "1", "metadata": {"window": "53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". "}, "hash": "4e5b6b07aadf536e34581d0d2bf8b087a5d159b56ad443ca33060fe46888507e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. ", "mimetype": "text/plain", "start_char_idx": 32384, "end_char_idx": 32521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c076aa5e-4aa3-420b-99cd-5d85d782adfd", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e704d944-9977-47d9-8cac-6b8ba7a28792", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Syst.  53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob. "}, "hash": "31b48b06381b2e11e296ffa26947d1fde0325f896652f7b19a799af29656869c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a34a56dd-3e98-4745-a474-f9db39c04e7a", "node_type": "1", "metadata": {"window": "---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. "}, "hash": "54118c37937d06b94603fbf6a4ae8e4d1236fb539eb2edb040101680b05cb173", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". ", "mimetype": "text/plain", "start_char_idx": 32521, "end_char_idx": 32588, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a34a56dd-3e98-4745-a474-f9db39c04e7a", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c076aa5e-4aa3-420b-99cd-5d85d782adfd", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "53, 1384\u20131401.\n\n ---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\". "}, "hash": "bf338700eec12295c3cc82927dc3c85ab70da2298d937454816d2dfb9ece484b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f903e8f7-e07b-4399-97e4-7b0f88b682a1", "node_type": "1", "metadata": {"window": "Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "All plots show the scores converging as the number of trees approaches 100.]\n\n"}, "hash": "f4317478d43e19dee78458ddd32ba96369bbaf4ef4cc4a4fa2b3fdcbea0c1617", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. ", "mimetype": "text/plain", "start_char_idx": 32588, "end_char_idx": 32691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f903e8f7-e07b-4399-97e4-7b0f88b682a1", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "All plots show the scores converging as the number of trees approaches 100.]\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a34a56dd-3e98-4745-a474-f9db39c04e7a", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "---\n[Image: A large 3x3 grid of line charts.  Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "The plots show the mean score and standard deviation (vertical bars) as the number of trees increases. "}, "hash": "f9922181b540378472aa38c58bcc1eea3c5bcf62e44ec74c6511432ae69c9f30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17fd8f70-539c-4817-a667-e09ec192c6c6", "node_type": "1", "metadata": {"window": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "**Fig. "}, "hash": "a340f3bb04cd41cdb91af063e4ab3302111f7e7d62d53071300faeaa7147346c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All plots show the scores converging as the number of trees approaches 100.]\n\n", "mimetype": "text/plain", "start_char_idx": 32691, "end_char_idx": 32769, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17fd8f70-539c-4817-a667-e09ec192c6c6", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f903e8f7-e07b-4399-97e4-7b0f88b682a1", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Each column represents an algorithm (IF left, EIF center, GIF right).  Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "All plots show the scores converging as the number of trees approaches 100.]\n\n"}, "hash": "015c62333cc4de16772d8471affb4033bfc376ce627abd04a70d77d84349aa52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec24be88-d2f4-434d-8ece-561f3f180ca3", "node_type": "1", "metadata": {"window": "Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right)."}, "hash": "e8b810f96c026fc510a8d3c7d193b8010c809e776b9e034bc8901e5be94e3678", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Fig. ", "mimetype": "text/plain", "start_char_idx": 32769, "end_char_idx": 32776, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ec24be88-d2f4-434d-8ece-561f3f180ca3", "embedding": null, "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "06d8f0af-18f9-45c2-9afa-f4de154c8206", "node_type": "4", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf"}, "hash": "b8d3eae38a44f6af7497c27f652df0aae689a60b82bc38011f506ac5ffb79db8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17fd8f70-539c-4817-a667-e09ec192c6c6", "node_type": "1", "metadata": {"title": "Generalized isolation forest for anomaly detection", "authors": "Lesouple et al.", "year": 2021, "file_path": "ad-papers-pdf/generalized_isolation_forest.pdf", "window": "Each row represents a dataset dimension ((a) 2D blob, (b) 3D blob, (c) 4D blob).  Within each cell, two plots are overlaid: one for the inner shell (bottom line) and one for the outer shell (top line) of the data blob.  The x-axis is \"Number of Trees\" and the y-axis is \"Anomaly Score\".  The plots show the mean score and standard deviation (vertical bars) as the number of trees increases.  All plots show the scores converging as the number of trees approaches 100.]\n\n **Fig.  12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "original_text": "**Fig. "}, "hash": "7322975c286d9f7774e6144c54e44e7cf65faabdd7dab5b8bf5c65ed5c8a31d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12:** Mean score for inner (bottom) and outer (top) shells versus the number of trees in the forest with corresponding standard deviations (vertical lines) for IF (left), EIF (center) and GIF (right).", "mimetype": "text/plain", "start_char_idx": 32776, "end_char_idx": 32976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]